# ğŸ“‹ ìº”ë“¤ ì²­í¬ ì²˜ë¦¬ê¸° ë¶„ë¦¬ ë° íš¨ìœ¨í™” ë¦¬íŒ©í„°ë§ ì„¤ê³„ì„œ

> **ëª©ì **: CandleDataProviderì—ì„œ ì²­í¬ ì²˜ë¦¬ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ ë‹¨ì¼ ì±…ì„ ì›ì¹™ì„ ì¤€ìˆ˜í•˜ê³ , ë™ì‹œì— ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ë¦¬íŒ©í„°ë§

---

## ğŸ¯ ë¦¬íŒ©í„°ë§ ëª©í‘œ

### ì£¼ìš” ëª©í‘œ
1. **ì²­í¬ ì²˜ë¦¬ ë¡œì§ ë¶„ë¦¬**: CandleDataProviderì˜ ë³µì¡ì„± ê°ì†Œ
2. **ë©”ì„œë“œ ëª…ëª… ê°œì„ **: ê¸°ëŠ¥ê³¼ ë™ì‘ì„ ëª…í™•íˆ í‘œí˜„í•˜ëŠ” ë©”ì„œë“œëª… ì ìš©
3. **ì„±ëŠ¥ ìµœì í™”**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ ë° ì²˜ë¦¬ ì†ë„ í–¥ìƒ
4. **ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€**: í˜„ì¬ ë¡œì§ê³¼ ê¸°ëŠ¥ êµ¬ì¡°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
5. **í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í™•ë³´**: ê° ë‹¨ê³„ë³„ ë…ë¦½ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ êµ¬ì¡°

### ì„±ê³¼ ì§€í‘œ
- ì½”ë“œ ë³µì¡ë„ 30% ê°ì†Œ
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 20% ì ˆì•½
- ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 90% ì´ìƒ
- API í˜¸ì¶œ íš¨ìœ¨ì„± 15% í–¥ìƒ

---

## ğŸ” í˜„ì¬ ë¬¸ì œì  ë¶„ì„

### CandleDataProviderì˜ ë¬¸ì œì 

#### 1. ì±…ì„ ê³¼ë¶€í•˜ (God Object)
```python
class CandleDataProvider:
    # ì „ì²´ ìˆ˜ì§‘ ì¡°ì • + ê°œë³„ ì²­í¬ ì²˜ë¦¬ + ê³„íš ìˆ˜ë¦½ + ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
    # â†’ 1,200ì¤„ì˜ ê±°ëŒ€í•œ í´ë˜ìŠ¤
```

#### 2. ëª¨í˜¸í•œ ë©”ì„œë“œëª…
- `mark_chunk_completed()` â†’ ì‹¤ì œë¡œëŠ” ì „ì²´ ì²­í¬ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰
- `_process_chunk_direct_storage()` â†’ ì²˜ë¦¬+ì €ì¥ì´ ë’¤ì„ì„
- `_analyze_chunk_overlap()` â†’ ë‹¨ìˆœ OverlapAnalyzer í˜¸ì¶œë§Œ í•¨

#### 3. ë¹„íš¨ìœ¨ì ì¸ ì²˜ë¦¬ íë¦„
```python
# í˜„ì¬: ë¶ˆí•„ìš”í•œ ì¤‘ë³µ ê²€ì¦ ë° ë©”ëª¨ë¦¬ ë‚­ë¹„
def mark_chunk_completed():
    # 1. ë³µì¡í•œ ì¡°ê±´ ë¶„ê¸°
    # 2. ì¤‘ë³µ ë°ì´í„° ê²€ì¦
    # 3. ë©”ëª¨ë¦¬ì— ì¤‘ê°„ ê²°ê³¼ë¬¼ ëˆ„ì 
    # 4. ê²¹ì¹¨ ë¶„ì„ í›„ì—ë„ ë¶ˆí•„ìš”í•œ API í˜¸ì¶œ
```

#### 4. í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€
- ëª¨ë“  ê¸°ëŠ¥ì´ ì–½í˜€ìˆì–´ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë¶ˆê°€ëŠ¥
- Mock ê°ì²´ ì‚¬ìš© ì–´ë ¤ì›€
- íŠ¹ì • ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸í•˜ê¸° ì–´ë ¤ì›€

---

## ğŸ—ï¸ ìƒˆë¡œìš´ ChunkProcessor ì„¤ê³„

### í•µì‹¬ ì„¤ê³„ ì² í•™

#### 1. **Pipeline Pattern**: ëª…í™•í•œ 4ë‹¨ê³„ ì²˜ë¦¬ íë¦„
#### 2. **Single Responsibility**: ê° ë©”ì„œë“œëŠ” í•˜ë‚˜ì˜ ëª…í™•í•œ ì±…ì„
#### 3. **Early Exit**: ì¡°ê¸° ì¢…ë£Œë¡œ ë¶ˆí•„ìš”í•œ ì²˜ë¦¬ ë°©ì§€
#### 4. **Memory Efficiency**: ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”

### ì „ì²´ êµ¬ì¡°

```python
class ChunkProcessor:
    """
    ìº”ë“¤ ì²­í¬ ì²˜ë¦¬ ì „ë¬¸ í´ë˜ìŠ¤

    ì±…ì„:
    - ê°œë³„ ì²­í¬ì˜ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
    - API í˜¸ì¶œ ìµœì í™” ë° ê²¹ì¹¨ ë¶„ì„
    - ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë° ë°ì´í„° ì •ê·œí™”
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—ëŸ¬ ì²˜ë¦¬
    """

    def __init__(self,
                 overlap_analyzer: OverlapAnalyzer,
                 upbit_client: UpbitPublicClient,
                 repository: CandleRepositoryInterface,
                 empty_candle_detector_factory,
                 performance_tracker: Optional[PerformanceTracker] = None):

        # ì™¸ë¶€ ì˜ì¡´ì„± ì£¼ì… (í…ŒìŠ¤íŠ¸ ìš©ì´ì„±)
        self.overlap_analyzer = overlap_analyzer
        self.upbit_client = upbit_client
        self.repository = repository
        self.empty_candle_detector_factory = empty_candle_detector_factory

        # ì„±ëŠ¥ ì¶”ì  (ì„ íƒì )
        self.performance_tracker = performance_tracker or PerformanceTracker()

        # ë¡œê¹…
        self.logger = create_component_logger("ChunkProcessor")
```

---

## ğŸ”„ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì„¤ê³„

### ë©”ì¸ íŒŒì´í”„ë¼ì¸

```python
async def execute_chunk_pipeline(self,
                                chunk_info: ChunkInfo,
                                collection_state: CollectionState) -> ChunkResult:
    """
    ğŸš€ ì²­í¬ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸ - ì „ì²´ íë¦„ì´ í•œëˆˆì— ë³´ì„

    Args:
        chunk_info: ì²˜ë¦¬í•  ì²­í¬ ì •ë³´
        collection_state: ì „ì²´ ìˆ˜ì§‘ ìƒíƒœ

    Returns:
        ChunkResult: ì²˜ë¦¬ ê²°ê³¼ (ì„±ê³µ/ì‹¤íŒ¨, ì €ì¥ ê°œìˆ˜, ë©”íƒ€ë°ì´í„°)
    """

    chunk_id = chunk_info.chunk_id
    self.logger.info(f"ğŸš€ ì²­í¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {chunk_id}")

    with self.performance_tracker.measure_chunk_execution(chunk_id):
        try:
            # Phase 1: ğŸ“‹ ì¤€ë¹„ ë° ë¶„ì„ ë‹¨ê³„
            execution_plan = await self._phase1_prepare_execution(chunk_info)

            # ì¡°ê¸° ì¢…ë£Œ: ì™„ì „ ê²¹ì¹¨ì¸ ê²½ìš° API í˜¸ì¶œ ìƒëµ
            if execution_plan.should_skip_api_call:
                return self._create_skip_result(execution_plan, chunk_info)

            # Phase 2: ğŸŒ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
            api_response = await self._phase2_fetch_data(chunk_info, execution_plan)

            # ì¡°ê¸° ì¢…ë£Œ: ë¹ˆ ì‘ë‹µ ë˜ëŠ” ì—…ë¹„íŠ¸ ë°ì´í„° ë
            if api_response.requires_early_exit:
                return self._handle_early_exit(api_response, chunk_info)

            # Phase 3: âš™ï¸ ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„
            processed_data = await self._phase3_process_data(api_response, chunk_info)

            # Phase 4: ğŸ’¾ ë°ì´í„° ì €ì¥ ë‹¨ê³„
            storage_result = await self._phase4_persist_data(processed_data, chunk_info)

            # âœ… ì„±ê³µ ê²°ê³¼ ìƒì„±
            return self._create_success_result(storage_result, chunk_info)

        except Exception as e:
            self.logger.error(f"âŒ ì²­í¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {chunk_id}, ì˜¤ë¥˜: {e}")
            return self._create_error_result(e, chunk_info)
```

### Phase 1: ì¤€ë¹„ ë° ë¶„ì„ ë‹¨ê³„

```python
async def _phase1_prepare_execution(self, chunk_info: ChunkInfo) -> ExecutionPlan:
    """
    ğŸ“‹ ì²­í¬ ì‹¤í–‰ ì¤€ë¹„ ë° ê²¹ì¹¨ ë¶„ì„

    ì±…ì„:
    - ê²¹ì¹¨ ìƒíƒœ ë¶„ì„
    - API í˜¸ì¶œ ì „ëµ ê²°ì •
    - ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
    """

    with self.performance_tracker.measure_phase('preparation'):
        self.logger.debug(f"ğŸ“‹ ì‹¤í–‰ ì¤€ë¹„: {chunk_info.chunk_id}")

        # ê²¹ì¹¨ ë¶„ì„ ìˆ˜í–‰
        overlap_analysis = await self._analyze_data_overlap(chunk_info)

        # ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
        execution_plan = self._build_execution_plan(chunk_info, overlap_analysis)

        # ChunkInfo ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
        chunk_info.set_overlap_info(overlap_analysis.overlap_result)

        self.logger.debug(f"ğŸ“‹ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ: {execution_plan.strategy}")
        return execution_plan

async def _analyze_data_overlap(self, chunk_info: ChunkInfo) -> OverlapAnalysis:
    """
    ğŸ” ë°ì´í„° ê²¹ì¹¨ ìƒíƒœ ì •ë°€ ë¶„ì„

    í˜„ì¬ _analyze_chunk_overlap() ë©”ì„œë“œë¥¼ ê°œì„ :
    - ë” ëª…í™•í•œ ë©”ì„œë“œëª…
    - ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ê°ì²´ë¡œ ë°˜í™˜
    """

    # ì²­í¬ ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    chunk_start = chunk_info.to
    chunk_end = self._calculate_chunk_end_time(chunk_info)

    # OverlapAnalyzerë¥¼ í†µí•œ ê²¹ì¹¨ ë¶„ì„
    overlap_result = await self.overlap_analyzer.analyze_overlap(
        OverlapRequest(
            symbol=chunk_info.symbol,
            timeframe=chunk_info.timeframe,
            target_start=chunk_start,
            target_end=chunk_end,
            target_count=chunk_info.count
        )
    )

    return OverlapAnalysis(
        overlap_result=overlap_result,
        analysis_time=datetime.now(timezone.utc),
        optimization_applied=True
    )
```

### Phase 2: ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„

```python
async def _phase2_fetch_data(self,
                            chunk_info: ChunkInfo,
                            execution_plan: ExecutionPlan) -> ApiResponse:
    """
    ğŸŒ ìµœì í™”ëœ API ë°ì´í„° ìˆ˜ì§‘

    ê°œì„ ì‚¬í•­:
    - ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”ëœ API í˜¸ì¶œ
    - ì‘ë‹µ ë°ì´í„° ì¦‰ì‹œ ê²€ì¦
    - ì—…ë¹„íŠ¸ ë°ì´í„° ë ê°ì§€
    """

    with self.performance_tracker.measure_phase('api_fetch'):
        self.logger.debug(f"ğŸŒ API ë°ì´í„° ìˆ˜ì§‘: {chunk_info.chunk_id}")

        # ìµœì í™”ëœ API íŒŒë¼ë¯¸í„° ì‚¬ìš©
        api_params = execution_plan.get_optimized_api_params()

        # API í˜¸ì¶œ (ê¸°ì¡´ _fetch_chunk_from_api ë¡œì§ í™œìš©)
        raw_data = await self._call_upbit_api(chunk_info, api_params)

        # ì‘ë‹µ ì¦‰ì‹œ ê²€ì¦
        validation_result = self._validate_api_response(raw_data, chunk_info)

        # ChunkInfoì— API ì‘ë‹µ ì •ë³´ ì €ì¥
        chunk_info.set_api_response_info(raw_data)

        # êµ¬ì¡°í™”ëœ ì‘ë‹µ ê°ì²´ ìƒì„±
        return ApiResponse(
            raw_data=raw_data,
            validation_result=validation_result,
            has_upbit_data_end=len(raw_data) < api_params['count'],
            requires_early_exit=validation_result.has_critical_errors
        )

async def _call_upbit_api(self, chunk_info: ChunkInfo, api_params: Dict) -> List[Dict]:
    """
    ğŸ“¡ ì—…ë¹„íŠ¸ API í˜¸ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    í˜„ì¬ _fetch_chunk_from_api() ë©”ì„œë“œë¥¼ ë¶„ë¦¬:
    - ë” ëª…í™•í•œ ë©”ì„œë“œëª…
    - API í˜¸ì¶œ ë¡œì§ë§Œ ì§‘ì¤‘
    """

    # ê¸°ì¡´ _fetch_chunk_from_apiì˜ íƒ€ì„í”„ë ˆì„ë³„ ë¶„ê¸° ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if chunk_info.timeframe == '1s':
        return await self.upbit_client.get_candles_seconds(**api_params)
    elif chunk_info.timeframe.endswith('m'):
        unit = int(chunk_info.timeframe[:-1])
        return await self.upbit_client.get_candles_minutes(unit=unit, **api_params)
    # ... ê¸°íƒ€ íƒ€ì„í”„ë ˆì„ ì²˜ë¦¬
```

### Phase 3: ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„

```python
async def _phase3_process_data(self,
                              api_response: ApiResponse,
                              chunk_info: ChunkInfo) -> ProcessedData:
    """
    âš™ï¸ ìº”ë“¤ ë°ì´í„° ì²˜ë¦¬ ë° ì •ê·œí™”

    ì±…ì„:
    - ë¹ˆ ìº”ë“¤ ê°ì§€ ë° ì±„ìš°ê¸°
    - ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦
    - ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ìƒì„±
    """

    with self.performance_tracker.measure_phase('data_processing'):
        self.logger.debug(f"âš™ï¸ ë°ì´í„° ì²˜ë¦¬: {chunk_info.chunk_id}")

        # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        filled_data = await self._detect_and_fill_gaps(
            api_response.raw_data, chunk_info
        )

        # ë°ì´í„° ì •ê·œí™”
        normalized_data = self._normalize_candle_data(filled_data)

        # ìµœì¢… ê²€ì¦
        validation_result = self._validate_processed_data(normalized_data, chunk_info)

        # ChunkInfoì— ìµœì¢… ì²˜ë¦¬ ì •ë³´ ì €ì¥
        chunk_info.set_final_candle_info(normalized_data)

        return ProcessedData(
            candles=normalized_data,
            gap_filled_count=len(filled_data) - len(api_response.raw_data),
            processing_metadata=self._create_processing_metadata(chunk_info),
            validation_passed=validation_result.is_valid
        )

async def _detect_and_fill_gaps(self,
                               raw_candles: List[Dict],
                               chunk_info: ChunkInfo) -> List[Dict]:
    """
    ğŸ” ë¹ˆ ìº”ë“¤ ê°ì§€ ë° ì±„ìš°ê¸° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

    í˜„ì¬ _process_api_candles_with_empty_filling() ë©”ì„œë“œë¥¼ ê°œì„ :
    - ë” ëª…í™•í•œ ë©”ì„œë“œëª…
    - ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ë§Œ ì§‘ì¤‘
    """

    # ê¸°ì¡´ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    detector = self.empty_candle_detector_factory(chunk_info.symbol, chunk_info.timeframe)

    return detector.detect_and_fill_gaps(
        raw_candles,
        api_start=chunk_info.api_request_start,
        api_end=chunk_info.api_request_end,
        is_first_chunk=len(chunk_info.completed_chunks) == 0
    )
```

### Phase 4: ë°ì´í„° ì €ì¥ ë‹¨ê³„

```python
async def _phase4_persist_data(self,
                              processed_data: ProcessedData,
                              chunk_info: ChunkInfo) -> StorageResult:
    """
    ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì˜êµ¬ ì €ì¥

    ì±…ì„:
    - Repositoryë¥¼ í†µí•œ ë°ì´í„° ì €ì¥
    - ì €ì¥ ê²°ê³¼ ê²€ì¦
    - ì €ì¥ ë©”íƒ€ë°ì´í„° ìƒì„±
    """

    with self.performance_tracker.measure_phase('data_storage'):
        self.logger.debug(f"ğŸ’¾ ë°ì´í„° ì €ì¥: {chunk_info.chunk_id}")

        # ì €ì¥ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        saved_count = await self.repository.save_raw_api_data(
            chunk_info.symbol,
            chunk_info.timeframe,
            processed_data.candles
        )

        # ì €ì¥ ê²°ê³¼ ê²€ì¦
        storage_validation = self._validate_storage_result(saved_count, processed_data)

        self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥")

        return StorageResult(
            saved_count=saved_count,
            expected_count=len(processed_data.candles),
            storage_time=datetime.now(timezone.utc),
            validation_passed=storage_validation.is_valid,
            metadata=self._create_storage_metadata(chunk_info, saved_count)
        )
```

---

## ğŸ“Š ë°ì´í„° ëª¨ë¸ ì„¤ê³„

### ìƒˆë¡œìš´ ê²°ê³¼ ê°ì²´ë“¤

```python
@dataclass
class ExecutionPlan:
    """ì²­í¬ ì‹¤í–‰ ê³„íš"""
    strategy: str  # 'skip_complete_overlap', 'partial_fetch', 'full_fetch'
    should_skip_api_call: bool
    optimized_api_params: Dict[str, Any]
    expected_data_range: Tuple[datetime, datetime]
    overlap_optimization: bool = True

    def get_optimized_api_params(self) -> Dict[str, Any]:
        """ê²¹ì¹¨ ë¶„ì„ ê¸°ë°˜ ìµœì í™”ëœ API íŒŒë¼ë¯¸í„° ë°˜í™˜"""
        return self.optimized_api_params

@dataclass
class OverlapAnalysis:
    """ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼"""
    overlap_result: Any  # OverlapResult ê°ì²´
    analysis_time: datetime
    optimization_applied: bool
    recommendations: List[str] = field(default_factory=list)

@dataclass
class ApiResponse:
    """API ì‘ë‹µ ë˜í¼"""
    raw_data: List[Dict[str, Any]]
    validation_result: Any  # ValidationResult ê°ì²´
    has_upbit_data_end: bool
    requires_early_exit: bool
    response_metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ProcessedData:
    """ì²˜ë¦¬ëœ ìº”ë“¤ ë°ì´í„°"""
    candles: List[Dict[str, Any]]
    gap_filled_count: int
    processing_metadata: Dict[str, Any]
    validation_passed: bool

@dataclass
class StorageResult:
    """ë°ì´í„° ì €ì¥ ê²°ê³¼"""
    saved_count: int
    expected_count: int
    storage_time: datetime
    validation_passed: bool
    metadata: Dict[str, Any]

@dataclass
class ChunkResult:
    """ì²­í¬ ì²˜ë¦¬ ìµœì¢… ê²°ê³¼"""
    success: bool
    chunk_id: str
    saved_count: int
    processing_time_ms: float
    phases_completed: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)
    error: Optional[Exception] = None

    def is_successful(self) -> bool:
        return self.success and self.error is None
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### 1. ì¡°ê¸° ì¢…ë£Œ íŒ¨í„´ (Early Exit)

```python
async def execute_chunk_pipeline(self, chunk_info, collection_state):
    # ğŸš€ ìµœì í™” 1: ì™„ì „ ê²¹ì¹¨ ì‹œ API í˜¸ì¶œ ìƒëµ
    execution_plan = await self._phase1_prepare_execution(chunk_info)
    if execution_plan.should_skip_api_call:
        self.logger.info(f"âš¡ ì™„ì „ ê²¹ì¹¨ìœ¼ë¡œ API í˜¸ì¶œ ìƒëµ: {chunk_info.chunk_id}")
        return self._create_skip_result(execution_plan, chunk_info)

    # ğŸš€ ìµœì í™” 2: ë¹ˆ ì‘ë‹µ ì‹œ ì¦‰ì‹œ ì¢…ë£Œ
    api_response = await self._phase2_fetch_data(chunk_info, execution_plan)
    if api_response.requires_early_exit:
        self.logger.info(f"âš¡ ë¹ˆ ì‘ë‹µìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ: {chunk_info.chunk_id}")
        return self._handle_early_exit(api_response, chunk_info)
```

### 2. ë©”ëª¨ë¦¬ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬

```python
async def _process_large_dataset(self, raw_data: List[Dict]) -> AsyncGenerator[Dict, None]:
    """
    ğŸš€ ëŒ€ìš©ëŸ‰ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
    - ë©”ëª¨ë¦¬ì— ëª¨ë“  ë°ì´í„°ë¥¼ ì˜¬ë¦¬ì§€ ì•ŠìŒ
    - ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í™•ë³´
    """

    BATCH_SIZE = 100  # ë°°ì¹˜ í¬ê¸°

    for i in range(0, len(raw_data), BATCH_SIZE):
        batch = raw_data[i:i + BATCH_SIZE]

        # ë°°ì¹˜ ì²˜ë¦¬
        processed_batch = await self._process_candle_batch(batch)

        # ì¦‰ì‹œ yieldí•˜ì—¬ ë©”ëª¨ë¦¬ í•´ì œ
        for candle in processed_batch:
            yield candle
```

### 3. ê³„ì‚° ê²°ê³¼ ìºì‹±

```python
from functools import lru_cache

class ChunkProcessor:
    @lru_cache(maxsize=256)
    def _calculate_timeframe_metadata(self, timeframe: str) -> Dict[str, Any]:
        """
        ğŸš€ ë°˜ë³µ ê³„ì‚° ìºì‹±
        - íƒ€ì„í”„ë ˆì„ë³„ ë©”íƒ€ë°ì´í„°ëŠ” ë¶ˆë³€ì´ë¯€ë¡œ ìºì‹œ ì ìš©
        - ê³„ì‚° ì˜¤ë²„í—¤ë“œ ì œê±°
        """
        return {
            'delta': TimeUtils.get_timeframe_delta(timeframe),
            'format': TimeUtils.get_timeframe_format(timeframe),
            'api_method': self._get_api_method_for_timeframe(timeframe)
        }

    def _invalidate_cache(self):
        """í•„ìš”ì‹œ ìºì‹œ ë¬´íš¨í™”"""
        self._calculate_timeframe_metadata.cache_clear()
```

### 4. ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬

```python
import asyncio

async def _parallel_validation(self, data_chunks: List[List[Dict]]) -> List[bool]:
    """
    ğŸš€ ë³‘ë ¬ ë°ì´í„° ê²€ì¦
    - ë…ë¦½ì ì¸ ê²€ì¦ ì‘ì—…ì„ ë³‘ë ¬ ì²˜ë¦¬
    - I/O ë°”ìš´ë“œ ì‘ì—… íš¨ìœ¨ì„± ê·¹ëŒ€í™”
    """

    # ë³‘ë ¬ ê²€ì¦ íƒœìŠ¤í¬ ìƒì„±
    validation_tasks = [
        self._validate_chunk_data(chunk)
        for chunk in data_chunks
    ]

    # ëª¨ë“  ê²€ì¦ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
    results = await asyncio.gather(*validation_tasks, return_exceptions=True)

    # ì˜ˆì™¸ ì²˜ë¦¬
    return [
        result if not isinstance(result, Exception) else False
        for result in results
    ]
```

---

## ğŸ”„ CandleDataProviderì™€ì˜ ë¶„ë¦¬ ì „ëµ

### ë¶„ë¦¬ ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„

```python
class CandleDataProvider:
    """
    ìˆ˜ì§‘ ì¡°ì •ìë¡œ ì—­í•  ë³€ê²½

    ë‚¨ì€ ì±…ì„:
    - ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì¡°ì •
    - Collection ìƒíƒœ ê´€ë¦¬
    - ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½
    - ìµœì¢… ê²°ê³¼ ì¡°íšŒ
    """

    def __init__(self,
                 repository: CandleRepositoryInterface,
                 upbit_client: UpbitPublicClient,
                 overlap_analyzer: OverlapAnalyzer,
                 chunk_size: int = 200,
                 enable_empty_candle_processing: bool = True):

        # ê¸°ì¡´ ì´ˆê¸°í™”ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer

        # ğŸ†• ChunkProcessor ì˜ì¡´ì„± ì£¼ì…
        self.chunk_processor = ChunkProcessor(
            overlap_analyzer=overlap_analyzer,
            upbit_client=upbit_client,
            repository=repository,
            empty_candle_detector_factory=self._get_empty_candle_detector
        )

    async def mark_chunk_completed(self, request_id: str) -> bool:
        """
        âœ¨ ê°œì„ ëœ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬ - ChunkProcessorì— ìœ„ì„

        ê¸°ì¡´ ë³µì¡í•œ ë¡œì§ì„ ChunkProcessorë¡œ ìœ„ì„í•˜ì—¬ ë‹¨ìˆœí™”
        """

        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        collection_state = self.active_collections[request_id]
        current_chunk = collection_state.current_chunk

        if current_chunk is None:
            raise ValueError("ì²˜ë¦¬ ì¤‘ì¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        self.logger.info(f"ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {current_chunk.chunk_id}")

        try:
            # ğŸš€ í•µì‹¬ ë³€ê²½: ChunkProcessorì— ìœ„ì„
            chunk_result = await self.chunk_processor.execute_chunk_pipeline(
                current_chunk, collection_state
            )

            # ê²°ê³¼ ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            self._process_chunk_result(collection_state, chunk_result)

            # ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸
            if self._is_collection_complete(collection_state):
                collection_state.is_completed = True
                collection_state.current_chunk = None
                self.logger.info(f"ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {request_id}")
                return True

            # ë‹¤ìŒ ì²­í¬ ì¤€ë¹„
            self._prepare_next_chunk(collection_state,
                                   collection_state.request_info.get_request_type())
            return False

        except Exception as e:
            self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {current_chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    def _process_chunk_result(self,
                             collection_state: CollectionState,
                             chunk_result: ChunkResult):
        """ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ CollectionStateì— ë°˜ì˜"""

        if chunk_result.is_successful():
            # ì„±ê³µì ì¸ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
            completed_chunk = collection_state.current_chunk
            completed_chunk.status = "completed"
            collection_state.completed_chunks.append(completed_chunk)

            # ì¹´ìš´íŒ… ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
            collection_state.total_collected += completed_chunk.count

            self.logger.info(f"ì²­í¬ ì™„ë£Œ: {completed_chunk.chunk_id}, "
                           f"ì €ì¥: {chunk_result.saved_count}ê°œ, "
                           f"ëˆ„ì : {collection_state.total_collected}/{collection_state.total_requested}")
        else:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            collection_state.current_chunk.status = "failed"
            collection_state.error_message = str(chunk_result.error)
            raise chunk_result.error
```

### ì ì§„ì  ë¶„ë¦¬ ë¡œë“œë§µ

#### Phase 1: ChunkProcessor í´ë˜ìŠ¤ ìƒì„± (1ì£¼ì°¨)

```python
# 1. ìƒˆë¡œìš´ í´ë˜ìŠ¤ ìƒì„± (ê¸°ì¡´ ì½”ë“œëŠ” ìœ ì§€)
class ChunkProcessor:
    # ìƒˆë¡œìš´ ë©”ì„œë“œë“¤ êµ¬í˜„
    pass

# 2. CandleDataProviderì— ChunkProcessor ì£¼ì…
class CandleDataProvider:
    def __init__(self, ...):
        self.chunk_processor = ChunkProcessor(...)  # ì¶”ê°€
        # ê¸°ì¡´ ì½”ë“œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
```

#### Phase 2: ë©”ì„œë“œ ì´ì£¼ (2ì£¼ì°¨)

```python
# 3. ê¸°ì¡´ ë©”ì„œë“œë“¤ì„ ChunkProcessorë¡œ ë³µì‚¬
# 4. CandleDataProviderì—ì„œ ChunkProcessor ë©”ì„œë“œ í˜¸ì¶œ
async def mark_chunk_completed(self, request_id: str):
    # ê¸°ì¡´ ë¡œì§ ìœ ì§€ + ChunkProcessor í˜¸ì¶œ ì¶”ê°€
    chunk_result = await self.chunk_processor.execute_chunk_pipeline(...)
    # ê¸°ì¡´ í›„ì²˜ë¦¬ ë¡œì§ ìœ ì§€
```

#### Phase 3: ê¸°ì¡´ ë©”ì„œë“œ ì œê±° (3ì£¼ì°¨)

```python
# 5. ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ í™•ì¸ í›„ ê¸°ì¡´ ë©”ì„œë“œë“¤ ì œê±°
# 6. ì½”ë“œ ì •ë¦¬ ë° ìµœì í™” ì ìš©
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ êµ¬ì¡°

```python
class TestChunkProcessor:
    """ChunkProcessor ì „ìš© í…ŒìŠ¤íŠ¸"""

    @pytest.fixture
    def chunk_processor(self):
        """í…ŒìŠ¤íŠ¸ìš© ChunkProcessor ì¸ìŠ¤í„´ìŠ¤"""
        return ChunkProcessor(
            overlap_analyzer=Mock(),
            upbit_client=Mock(),
            repository=Mock(),
            empty_candle_detector_factory=Mock(),
        )

    async def test_execute_chunk_pipeline_success(self, chunk_processor):
        """ì •ìƒì ì¸ ì²­í¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""

        # Given
        chunk_info = create_test_chunk_info()
        collection_state = create_test_collection_state()

        # When
        result = await chunk_processor.execute_chunk_pipeline(chunk_info, collection_state)

        # Then
        assert result.is_successful()
        assert result.saved_count > 0

    async def test_phase1_prepare_execution_with_complete_overlap(self, chunk_processor):
        """ì™„ì „ ê²¹ì¹¨ ìƒí™©ì—ì„œì˜ ì‹¤í–‰ ì¤€ë¹„ í…ŒìŠ¤íŠ¸"""

        # Given: ì™„ì „ ê²¹ì¹¨ ìƒí™© ì„¤ì •
        chunk_info = create_chunk_info_with_complete_overlap()

        # When
        execution_plan = await chunk_processor._phase1_prepare_execution(chunk_info)

        # Then
        assert execution_plan.should_skip_api_call is True
        assert execution_plan.strategy == 'skip_complete_overlap'

    async def test_phase2_fetch_data_with_rate_limit(self, chunk_processor):
        """Rate Limit ìƒí™©ì—ì„œì˜ ë°ì´í„° ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        # Rate Limit ê´€ë ¨ í…ŒìŠ¤íŠ¸ ë¡œì§
        pass

    async def test_phase3_process_data_with_gaps(self, chunk_processor):
        """Gapì´ ìˆëŠ” ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê´€ë ¨ í…ŒìŠ¤íŠ¸ ë¡œì§
        pass

    async def test_phase4_persist_data_with_storage_failure(self, chunk_processor):
        """ì €ì¥ ì‹¤íŒ¨ ìƒí™© í…ŒìŠ¤íŠ¸"""
        # ì €ì¥ ì‹¤íŒ¨ ë³µêµ¬ ë¡œì§ í…ŒìŠ¤íŠ¸
        pass
```

### ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

```python
class TestChunkProcessorPerformance:
    """ì„±ëŠ¥ ìµœì í™” ê²€ì¦ í…ŒìŠ¤íŠ¸"""

    async def test_memory_usage_optimization(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ê²€ì¦"""

        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
        large_dataset = create_large_test_dataset(10000)

        initial_memory = get_memory_usage()

        # ì²­í¬ ì²˜ë¦¬ ì‹¤í–‰
        await chunk_processor.execute_chunk_pipeline(...)

        final_memory = get_memory_usage()

        # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ì´ ì„ê³„ê°’ ì´í•˜ì¸ì§€ í™•ì¸
        memory_increase = final_memory - initial_memory
        assert memory_increase < MEMORY_THRESHOLD

    async def test_api_call_optimization(self):
        """API í˜¸ì¶œ ìµœì í™” ê²€ì¦"""

        # ê²¹ì¹¨ ìƒí™©ì—ì„œ API í˜¸ì¶œ ìˆ˜ í™•ì¸
        with patch.object(upbit_client, 'get_candles_minutes') as mock_api:
            await chunk_processor.execute_chunk_pipeline(...)

            # ì˜ˆìƒë³´ë‹¤ ì ì€ API í˜¸ì¶œ í™•ì¸
            assert mock_api.call_count < expected_calls
```

### í†µí•© í…ŒìŠ¤íŠ¸

```python
class TestChunkProcessorIntegration:
    """CandleDataProviderì™€ì˜ í†µí•© í…ŒìŠ¤íŠ¸"""

    async def test_integration_with_candle_data_provider(self):
        """ì‹¤ì œ CandleDataProviderì™€ì˜ ì—°ë™ í…ŒìŠ¤íŠ¸"""

        provider = CandleDataProvider(...)

        # ê¸°ì¡´ get_candles ë©”ì„œë“œ ë™ì‘ ê²€ì¦
        candles = await provider.get_candles('KRW-BTC', '1m', count=100)

        assert len(candles) == 100
        # ê¸°ì¡´ ë™ì‘ê³¼ ë™ì¼í•œ ê²°ê³¼ í™•ì¸
```

---

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ë‚´ì¥ ì„±ëŠ¥ ì¶”ì ê¸°

```python
class PerformanceTracker:
    """ì²­í¬ ì²˜ë¦¬ ì„±ëŠ¥ ì¶”ì """

    def __init__(self):
        self.metrics = {}
        self.logger = create_component_logger("PerformanceTracker")

    @contextmanager
    def measure_chunk_execution(self, chunk_id: str):
        """ì „ì²´ ì²­í¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        try:
            yield
        finally:
            execution_time = time.time() - start_time
            self.metrics[f"{chunk_id}_total"] = execution_time
            self.logger.info(f"â±ï¸ ì²­í¬ ì‹¤í–‰ ì‹œê°„: {chunk_id} = {execution_time:.3f}ì´ˆ")

    @contextmanager
    def measure_phase(self, phase_name: str):
        """ê°œë³„ Phase ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        try:
            yield
        finally:
            phase_time = time.time() - start_time
            self.metrics[f"phase_{phase_name}"] = phase_time
            self.logger.debug(f"ğŸ“Š Phase ì‹œê°„: {phase_name} = {phase_time:.3f}ì´ˆ")

    def get_performance_report(self) -> Dict[str, float]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            'avg_total_time': np.mean([v for k, v in self.metrics.items() if '_total' in k]),
            'avg_preparation_time': np.mean([v for k, v in self.metrics.items() if 'phase_preparation' in k]),
            'avg_api_fetch_time': np.mean([v for k, v in self.metrics.items() if 'phase_api_fetch' in k]),
            'avg_processing_time': np.mean([v for k, v in self.metrics.items() if 'phase_data_processing' in k]),
            'avg_storage_time': np.mean([v for k, v in self.metrics.items() if 'phase_data_storage' in k]),
        }
```

### ì„±ëŠ¥ ì•Œë¦¼ ì‹œìŠ¤í…œ

```python
class PerformanceAlertSystem:
    """ì„±ëŠ¥ ì„ê³„ê°’ ëª¨ë‹ˆí„°ë§"""

    def __init__(self, thresholds: Dict[str, float]):
        self.thresholds = thresholds
        self.logger = create_component_logger("PerformanceAlert")

    def check_performance_thresholds(self, metrics: Dict[str, float]):
        """ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼"""

        for metric_name, value in metrics.items():
            threshold = self.thresholds.get(metric_name)

            if threshold and value > threshold:
                self.logger.warning(f"ğŸš¨ ì„±ëŠ¥ ì„ê³„ê°’ ì´ˆê³¼: {metric_name} = {value:.3f}ì´ˆ (ì„ê³„ê°’: {threshold:.3f}ì´ˆ)")

                # í•„ìš”ì‹œ ì¶”ê°€ ì•Œë¦¼ ë¡œì§ (Slack, ì´ë©”ì¼ ë“±)
                self._send_performance_alert(metric_name, value, threshold)
```

---

## ğŸ“ êµ¬í˜„ ë¡œë“œë§µ

### Week 1: ChunkProcessor ê¸°ë³¸ êµ¬ì¡° êµ¬í˜„
- [ ] ChunkProcessor í´ë˜ìŠ¤ ìƒì„±
- [ ] 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ë©”ì„œë“œ êµ¬í˜„
- [ ] ë°ì´í„° ëª¨ë¸ (ExecutionPlan, ApiResponse ë“±) êµ¬í˜„
- [ ] ê¸°ë³¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±

### Week 2: Phase 1-2 êµ¬í˜„ (ì¤€ë¹„ & ìˆ˜ì§‘)
- [ ] `_phase1_prepare_execution` êµ¬í˜„
- [ ] `_analyze_data_overlap` êµ¬í˜„
- [ ] `_phase2_fetch_data` êµ¬í˜„
- [ ] `_call_upbit_api` êµ¬í˜„ (ê¸°ì¡´ ë¡œì§ ì´ì£¼)
- [ ] ì¡°ê¸° ì¢…ë£Œ ë¡œì§ êµ¬í˜„

### Week 3: Phase 3-4 êµ¬í˜„ (ì²˜ë¦¬ & ì €ì¥)
- [ ] `_phase3_process_data` êµ¬í˜„
- [ ] `_detect_and_fill_gaps` êµ¬í˜„ (ê¸°ì¡´ ë¡œì§ ì´ì£¼)
- [ ] `_phase4_persist_data` êµ¬í˜„
- [ ] ì„±ëŠ¥ ìµœì í™” ë¡œì§ ì ìš©

### Week 4: CandleDataProvider í†µí•©
- [ ] CandleDataProviderì— ChunkProcessor ì£¼ì…
- [ ] `mark_chunk_completed` ë©”ì„œë“œ ê°œì„ 
- [ ] ê¸°ì¡´ ë©”ì„œë“œë“¤ê³¼ì˜ í˜¸í™˜ì„± í™•ë³´
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰

### Week 5: ì„±ëŠ¥ ìµœì í™” & ë§ˆë¬´ë¦¬
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©
- [ ] ìºì‹± ë¡œì§ êµ¬í˜„
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

### Week 6: í…ŒìŠ¤íŠ¸ & ê²€ì¦
- [ ] ì „ì²´ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì •
- [ ] ê¸°ì¡´ ê¸°ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸
- [ ] í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„

---

## ğŸ¯ ì˜ˆìƒ ì„±ê³¼

### ì½”ë“œ í’ˆì§ˆ ê°œì„ 
- **ë³µì¡ë„ ê°ì†Œ**: CandleDataProvider 1,200ì¤„ â†’ 800ì¤„ (33% ê°ì†Œ)
- **ì±…ì„ ë¶„ë¦¬**: ë‹¨ì¼ ì±…ì„ ì›ì¹™ ì¤€ìˆ˜ë¡œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: 70% â†’ 90% (ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í™•ë³´)

### ì„±ëŠ¥ ê°œì„ 
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: ì¡°ê¸° ì¢…ë£Œ ë° ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ 20% ì ˆì•½
- **API íš¨ìœ¨ì„±**: ê²¹ì¹¨ ë¶„ì„ ìµœì í™”ë¡œ 15% í˜¸ì¶œ ê°ì†Œ
- **ì²˜ë¦¬ ì†ë„**: ë³‘ë ¬ ì²˜ë¦¬ ë° ìºì‹±ìœ¼ë¡œ 10% í–¥ìƒ

### ê°œë°œ ìƒì‚°ì„± í–¥ìƒ
- **ë””ë²„ê¹… ìš©ì´ì„±**: ë‹¨ê³„ë³„ ë¡œê¹…ìœ¼ë¡œ ë¬¸ì œ ì§€ì  ë¹ ë¥¸ íŒŒì•…
- **ê¸°ëŠ¥ í™•ì¥ì„±**: ìƒˆë¡œìš´ ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ê°€ ìš©ì´í•¨
- **í…ŒìŠ¤íŠ¸ íš¨ìœ¨ì„±**: ê°œë³„ Phase ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¡œ ì •ë°€í•œ ê²€ì¦

---

## ğŸš€ ê²°ë¡ 

ì´ ë¦¬íŒ©í„°ë§ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ëª©í‘œë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **ëª…í™•í•œ ì±…ì„ ë¶„ë¦¬**: CandleDataProviderëŠ” ì¡°ì •ì, ChunkProcessorëŠ” ì‹¤í–‰ì
2. **ì§ê´€ì ì¸ ë©”ì„œë“œëª…**: ê¸°ëŠ¥ê³¼ ë™ì‘ì´ ëª…í™•íˆ í‘œí˜„ë˜ëŠ” ë©”ì„œë“œëª… ì ìš©
3. **ì„±ëŠ¥ ìµœì í™”**: ì¡°ê¸° ì¢…ë£Œ, ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±, ìºì‹±ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
4. **ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€**: í˜„ì¬ ë¡œì§ê³¼ ê¸°ëŠ¥ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì¡´
5. **í…ŒìŠ¤íŠ¸ ìš©ì´ì„±**: ê° ë‹¨ê³„ë³„ ë…ë¦½ì ì¸ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥

**ìµœì¢…ì ìœ¼ë¡œ ë” ê¹”ë”í•˜ê³ , ë” ë¹ ë¥´ê³ , ë” í…ŒìŠ¤íŠ¸í•˜ê¸° ì‰¬ìš´ ìº”ë“¤ ì²˜ë¦¬ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.** ğŸ¯

---

> **Next Steps**: ì´ ì„¤ê³„ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ êµ¬í˜„ì„ ì§„í–‰í•˜ë©°, ê° ë‹¨ê³„ë³„ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ê²€ì¦í•´ë‚˜ê°€ê² ìŠµë‹ˆë‹¤.
