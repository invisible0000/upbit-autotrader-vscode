"""
üìã ChunkProcessor v3.0 - candle_business_models.py Í∏∞Î∞ò Îã®ÏàúÌôî Î≤ÑÏ†Ñ

Created: 2025-09-23
Purpose: "ÏÜåÏä§Ïùò ÏõêÏ≤ú" ÏõêÏπôÏóê Îî∞Î•∏ Îã®ÏàúÌôîÎêú Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ ÏóîÏßÑ
Architecture: RequestInfo + List[ChunkInfo] = ÏôÑÏ†ÑÌïú ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅ

ÌïµÏã¨ ÏÑ§Í≥Ñ Î≥ÄÍ≤Ω:
1. ÏÉÅÌÉú Í¥ÄÎ¶¨ Ï†úÍ±∞: InternalCollectionState, CollectionProgress Îì± Ï†úÍ±∞
2. Îã®Ïùº ÏÜåÏä§ ÏõêÏπô: candle_business_models.pyÏùò 4Í∞ú ÌïµÏã¨ Î™®Îç∏Îßå ÏÇ¨Ïö©
3. ÏàúÏàò Ï≤≠ÌÅ¨ Ï≤òÎ¶¨: ChunkProcessorÎäî Í∞úÎ≥Ñ Ï≤≠ÌÅ¨ Ï≤òÎ¶¨Îßå Îã¥Îãπ
4. Î™®ÎãàÌÑ∞ÎßÅ Î∂ÑÎ¶¨: Î≥µÏû°Ìïú ÏÉÅÌÉú Ï∂îÏ†ÅÏùÑ Î≥ÑÎèÑ Í≥ÑÏ∏µÏúºÎ°ú Î∂ÑÎ¶¨

Ï£ºÏöî Î≥ÄÍ≤ΩÏÇ¨Ìï≠:
- execute_full_collection() ‚Üí process_collection() (Îã®ÏàúÌôî)
- InternalCollectionState Ï†úÍ±∞ ‚Üí List[ChunkInfo] ÏßÅÏ†ë ÏÇ¨Ïö©
- Î≥µÏû°Ìïú ÏôÑÎ£å ÌåêÎã® ‚Üí should_complete_collection() Ìï®Ïàò ÏÇ¨Ïö©
- ÏÉÅÌÉú ÎèôÍ∏∞Ìôî Î°úÏßÅ Ï†úÍ±∞ ‚Üí ChunkInfoÏóêÏÑú ÏßÅÏ†ë ÏÉÅÌÉú Í¥ÄÎ¶¨

Í∏∞ÎåÄ Ìö®Í≥º:
- ÏΩîÎìú Î≥µÏû°ÎèÑ 50% Ïù¥ÏÉÅ Í∞êÏÜå
- ÎîîÎ≤ÑÍπÖ Ïö©Ïù¥ÏÑ± Ìñ•ÏÉÅ (ChunkInfo JSON Î°úÍ∑∏)
- ÌôïÏû•ÏÑ± Ìñ•ÏÉÅ (Î™®ÎãàÌÑ∞ÎßÅ ÏöîÍµ¨ÏÇ¨Ìï≠ Î≥ÄÍ≤Ω Ïãú ÌïµÏã¨ Î°úÏßÅ ÏòÅÌñ• ÏóÜÏùå)
"""

import time
import json
from datetime import datetime, timezone
from typing import Optional, Dict, Any, Callable, List

# Infrastructure Î°úÍπÖ
from upbit_auto_trading.infrastructure.logging import create_component_logger

# ÌïµÏã¨ ÎπÑÏ¶àÎãàÏä§ Î™®Îç∏ (candle_business_models.py)
from upbit_auto_trading.infrastructure.market_data.candle.models.candle_business_models import (
    RequestInfo,
    CollectionPlan,
    ChunkInfo,
    OverlapRequest,
    OverlapResult,
    OverlapStatus,
    ChunkStatus,
    RequestType,
    should_complete_collection,
    create_collection_plan
)

# ÏùòÏ°¥ÏÑ± (Infrastructure Í≥ÑÏ∏µ)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)
from upbit_auto_trading.infrastructure.market_data.candle.empty_candle_detector import (
    EmptyCandleDetector
)
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils

# Îã®ÏàúÌôîÎêú Í≤∞Í≥º Î™®Îç∏
from dataclasses import dataclass


@dataclass
class CollectionResult:
    """Îã®ÏàúÌôîÎêú ÏàòÏßë Í≤∞Í≥º - ÏµúÏ¢Ö Í≤∞Í≥ºÎßå"""
    success: bool
    chunks: List[ChunkInfo]
    collected_count: int
    requested_count: int
    processing_time_seconds: float
    error: Optional[Exception] = None


# ÏßÑÌñâ ÏÉÅÌô© ÏΩúÎ∞± ÌÉÄÏûÖ
ProgressCallback = Callable[[int, int], None]  # (completed_chunks, total_chunks)

logger = create_component_logger("ChunkProcessor")


class ChunkProcessor:
    """
    ChunkProcessor v3.0 - Îã®ÏàúÌôîÎêú Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ ÏóîÏßÑ

    ÌïµÏã¨ ÏõêÏπô:
    1. Îã®Ïùº ÏÜåÏä§: candle_business_models.pyÏùò 4Í∞ú ÌïµÏã¨ Î™®Îç∏Îßå ÏÇ¨Ïö©
    2. ÏÉÅÌÉú Í¥ÄÎ¶¨ Ï†úÍ±∞: Î≥µÏû°Ìïú CollectionState, InternalCollectionState Ï†úÍ±∞
    3. ÏàúÏàò Ï≤òÎ¶¨: RequestInfo + List[ChunkInfo] = ÏôÑÏ†ÑÌïú ÎπÑÏ¶àÎãàÏä§ Î°úÏßÅ
    4. Î™®ÎãàÌÑ∞ÎßÅ Î∂ÑÎ¶¨: ÏßÑÌñâ ÏÉÅÌô©ÏùÄ Î≥ÑÎèÑ Í≥ÑÏ∏µÏóêÏÑú ChunkInfo Í∏∞Î∞ò Î∂ÑÏÑù

    Ï£ºÏöî Ïù∏ÌÑ∞ÌéòÏù¥Ïä§:
    - process_collection(): RequestInfo ‚Üí List[ChunkInfo] (Î©îÏù∏ API)
    - process_single_chunk(): Í∞úÎ≥Ñ Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ (CandleDataProvider Ïó∞ÎèôÏö©)
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        empty_candle_detector_factory: Callable[[str, str], EmptyCandleDetector],
        chunk_size: int = 200,
        enable_empty_candle_processing: bool = True,
        dry_run: bool = False
    ):
        """
        ChunkProcessor v3.0 Ï¥àÍ∏∞Ìôî

        Args:
            repository: Ï∫îÎì§ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•ÏÜå
            upbit_client: ÏóÖÎπÑÌä∏ API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
            overlap_analyzer: Í≤πÏπ® Î∂ÑÏÑùÍ∏∞
            empty_candle_detector_factory: Îπà Ï∫îÎì§ Í∞êÏßÄÍ∏∞ Ìå©ÌÜ†Î¶¨
            chunk_size: Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞ (Í∏∞Î≥∏ 200, ÏóÖÎπÑÌä∏ Ï†úÌïú)
            enable_empty_candle_processing: Îπà Ï∫îÎì§ Ï≤òÎ¶¨ ÌôúÏÑ±Ìôî Ïó¨Î∂Ä
            dry_run: Í±¥Ïãù Ïã§Ìñâ (Ïã§Ï†ú Ï†ÄÏû•ÌïòÏßÄ ÏïäÏùå)
        """
        # ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer
        self.empty_candle_detector_factory = empty_candle_detector_factory

        # ÏÑ§Ï†ï
        self.chunk_size = min(chunk_size, 200)  # ÏóÖÎπÑÌä∏ Ï†úÌïú Ï§ÄÏàò
        self.enable_empty_candle_processing = enable_empty_candle_processing
        self.dry_run = dry_run

        # Legacy Ìò∏Ìôò ÏÑ§Ï†ï
        self.api_rate_limit_rps = 10  # 10 RPS Í∏∞Ï§Ä

        logger.info("ChunkProcessor v3.0 Ï¥àÍ∏∞Ìôî ÏôÑÎ£å (Îã®ÏàúÌôî Î≤ÑÏ†Ñ)")
        logger.info(f"Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞: {self.chunk_size}, "
                    f"Îπà Ï∫îÎì§ Ï≤òÎ¶¨: {'ÌôúÏÑ±Ìôî' if enable_empty_candle_processing else 'ÎπÑÌôúÏÑ±Ìôî'}, "
                    f"API Rate Limit: {self.api_rate_limit_rps} RPS, "
                    f"DRY-RUN: {'ÌôúÏÑ±Ìôî' if dry_run else 'ÎπÑÌôúÏÑ±Ìôî'}")

    def _log_chunk_info_debug(
        self,
        chunk_info: ChunkInfo,
        status: str = "unknown",
        processing_time_ms: Optional[float] = None
    ) -> None:
        """ChunkInfo ÏÉÅÌÉúÎ•º JSON ÌòïÏãùÏúºÎ°ú ÎîîÎ≤ÑÍ∑∏ Ï∂úÎ†• - Ïó∞ÏÜç Ï∂îÏ†ÅÏö©"""
        if not chunk_info:
            return

        try:
            effective_end = chunk_info.get_effective_end_time()

            debug_data = {
                "chunk_id": chunk_info.chunk_id,
                "chunk_index": chunk_info.chunk_index,
                "symbol": chunk_info.symbol,
                "timeframe": chunk_info.timeframe,
                "status": status,
                "chunk_status": chunk_info.chunk_status.value,
                "count": chunk_info.count,
                "to": chunk_info.to.isoformat() if chunk_info.to else None,
                "end": chunk_info.end.isoformat() if chunk_info.end else None,

                # API ÏöîÏ≤≠ Ï†ïÎ≥¥
                "api_request_count": getattr(chunk_info, 'api_request_count', None),
                "api_response_count": len(getattr(chunk_info, 'api_response_data', [])),

                # ÏµúÏ¢Ö Ï∫îÎì§ Ï†ïÎ≥¥
                "final_candle_count": getattr(chunk_info, 'final_candle_count', None),
                "final_candle_start": (
                    chunk_info.final_candle_start.isoformat()
                    if chunk_info.final_candle_start else None
                ),
                "final_candle_end": (
                    chunk_info.final_candle_end.isoformat()
                    if chunk_info.final_candle_end else None
                ),

                # Ïú†Ìö® ÏãúÍ∞Ñ (ÌïµÏã¨)
                "effective_end_time": effective_end.isoformat() if effective_end else None,
                "time_source": chunk_info.get_time_source(),

                # Í≤πÏπ® ÏÉÅÌÉú
                "overlap_status": chunk_info.overlap_status.value if chunk_info.overlap_status else None,

                # Ï≤òÎ¶¨ ÏãúÍ∞Ñ
                "processing_time_ms": processing_time_ms
            }

            logger.debug(f"üîç ChunkInfo: {json.dumps(debug_data, ensure_ascii=False)}")

        except Exception as e:
            logger.warning(f"ChunkInfo ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏ ÏÉùÏÑ± Ïã§Ìå®: {e}")

    # =========================================================================
    # üöÄ Î©îÏù∏ API - Îã®ÏàúÌôîÎêú Ï≤≠ÌÅ¨ Ï≤òÎ¶¨
    # =========================================================================

    async def process_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None,
        progress_callback: Optional[ProgressCallback] = None
    ) -> CollectionResult:
        """
        üöÄ Îã®ÏàúÌôîÎêú Ï∫îÎì§ ÏàòÏßë - candle_business_models.py Í∏∞Î∞ò

        Î≥µÏû°Ìïú ÏÉÅÌÉú Í¥ÄÎ¶¨ ÌÅ¥ÎûòÏä§ Ï†úÍ±∞ÌïòÍ≥† RequestInfo + List[ChunkInfo]ÎßåÏúºÎ°ú ÎèôÏûë.
        Ï≤≠ÌÅ¨ Ï≤òÎ¶¨Ïùò ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌùêÎ¶ÑÏùÑ Íµ¨ÌòÑÌïú Î©îÏù∏ API.

        Args:
            symbol: Í±∞Îûò Ïã¨Î≥º (Ïòà: 'KRW-BTC')
            timeframe: ÌÉÄÏûÑÌîÑÎ†àÏûÑ (Ïòà: '1m', '5m', '1d')
            count: ÏàòÏßëÌï† Ï∫îÎì§ Í∞úÏàò
            to: ÏãúÏûë ÏãúÏ†ê (ÏµúÏã† Ï∫îÎì§ Í∏∞Ï§Ä)
            end: Ï¢ÖÎ£å ÏãúÏ†ê (Í≥ºÍ±∞ Ï∫îÎì§ Í∏∞Ï§Ä)
            progress_callback: ÏßÑÌñâ ÏÉÅÌô© ÏΩúÎ∞± Ìï®Ïàò (completed_chunks, total_chunks)

        Returns:
            CollectionResult: ÏàòÏßë Í≤∞Í≥º Î∞è Î™®Îì† Ï≤≠ÌÅ¨ Ï†ïÎ≥¥

        Raises:
            ValueError: ÏûòÎ™ªÎêú ÌååÎùºÎØ∏ÌÑ∞
            Exception: ÏàòÏßë Í≥ºÏ†ï Ïò§Î•ò
        """
        start_time = time.time()

        logger.info(f"Îã®ÏàúÌôîÎêú Ï∫îÎì§ ÏàòÏßë ÏãúÏûë: {symbol} {timeframe}")
        if count:
            logger.info(f"Í∞úÏàò: {count:,}Í∞ú")
        if to:
            logger.info(f"ÏãúÏûë: {to}")
        if end:
            logger.info(f"Ï¢ÖÎ£å: {end}")
        if self.dry_run:
            logger.info("üîÑ DRY-RUN Î™®Îìú: Ïã§Ï†ú Ï†ÄÏû•ÌïòÏßÄ ÏïäÏùå")

        try:
            # 1. RequestInfo ÏÉùÏÑ± (Îã®Ïùº ÏÜåÏä§)
            request_info = RequestInfo(
                symbol=symbol,
                timeframe=timeframe,
                count=count,
                to=TimeUtils.normalize_datetime_to_utc(to) if to else None,
                end=TimeUtils.normalize_datetime_to_utc(end) if end else None
            )

            # 2. ÏàòÏßë Í≥ÑÌöç ÏàòÎ¶Ω (Îã®ÏàúÌôî)
            plan = create_collection_plan(request_info, self.chunk_size, self.api_rate_limit_rps)

            logger.info(f"Í≥ÑÌöç ÏàòÎ¶Ω ÏôÑÎ£å: {plan.total_count:,}Í∞ú Ï∫îÎì§, {plan.estimated_chunks}Ï≤≠ÌÅ¨, "
                        f"ÏòàÏÉÅ ÏÜåÏöîÏãúÍ∞Ñ: {plan.estimated_duration_seconds:.1f}Ï¥à")

            # 3. Ï≤≠ÌÅ¨Î≥Ñ ÏàúÏ∞® Ï≤òÎ¶¨ (Îã®ÏàúÌïú Î¶¨Ïä§Ìä∏ Í¥ÄÎ¶¨)
            chunks: List[ChunkInfo] = []

            for chunk_index in range(plan.estimated_chunks):
                # Ï≤≠ÌÅ¨ ÏÉùÏÑ± (Ïù¥Ï†Ñ Ï≤≠ÌÅ¨ Í≤∞Í≥º Í∏∞Î∞ò Ïó∞ÏÜçÏÑ±)
                chunk = self._create_chunk(chunk_index, request_info, plan, chunks)

                # Í∞úÎ≥Ñ Ï≤≠ÌÅ¨ Ï≤òÎ¶¨
                await self._process_single_chunk(chunk)
                chunks.append(chunk)

                # ÏßÑÌñâÎ•† Î≥¥Í≥†
                if progress_callback:
                    progress_callback(len(chunks), plan.estimated_chunks)

                # ÏôÑÎ£å ÌåêÎã® (Îã®ÏàúÌôî)
                if should_complete_collection(request_info, chunks):
                    logger.info(f"ÏàòÏßë ÏôÑÎ£å Ï°∞Í±¥ Îã¨ÏÑ±: {len(chunks)}Í∞ú Ï≤≠ÌÅ¨ Ï≤òÎ¶¨")
                    break

            # 4. ÏµúÏ¢Ö Í≤∞Í≥º ÏÉùÏÑ±
            processing_time = time.time() - start_time
            return self._create_success_result(chunks, request_info, processing_time)

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"Îã®ÏàúÌôîÎêú Ï∫îÎì§ ÏàòÏßë Ïã§Ìå®: {e}")
            return self._create_error_result(e, chunks if 'chunks' in locals() else [], processing_time)

    # =========================================================================
    # üîó CandleDataProvider Ïó∞ÎèôÏö© API (ÌïòÏúÑ Ìò∏ÌôòÏÑ±)
    # =========================================================================

    async def process_single_chunk(self, chunk: ChunkInfo) -> ChunkInfo:
        """
        Îã®Ïùº Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ - CandleDataProvider Ïó∞ÎèôÏö©

        Í∏∞Ï°¥ execute_single_chunk() Ïù∏ÌÑ∞ÌéòÏù¥Ïä§ ÎåÄÏ≤¥.
        ChunkInfoÎ•º Î∞õÏïÑÏÑú Ï≤òÎ¶¨ÌïòÍ≥† ÎèôÏùºÌïú ChunkInfoÎ•º Î∞òÌôò (ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏Îê®).

        Args:
            chunk: Ï≤òÎ¶¨Ìï† Ï≤≠ÌÅ¨ Ï†ïÎ≥¥

        Returns:
            ChunkInfo: Ï≤òÎ¶¨ ÏôÑÎ£åÎêú ÎèôÏùºÌïú Ï≤≠ÌÅ¨ (ÏÉÅÌÉú ÏóÖÎç∞Ïù¥Ìä∏Îê®)
        """
        logger.debug(f"Îã®Ïùº Ï≤≠ÌÅ¨ Ï≤òÎ¶¨: {chunk.chunk_id}")

        try:
            await self._process_single_chunk(chunk)
            return chunk

        except Exception as e:
            logger.error(f"Îã®Ïùº Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ Ïã§Ìå®: {chunk.chunk_id}, Ïò§Î•ò: {e}")
            chunk.mark_failed()
            raise

    # =========================================================================
    # üèóÔ∏è ÌïµÏã¨ Ï≤òÎ¶¨ Î°úÏßÅ - Legacy Î°úÏßÅ Î≥¥Ï°¥ÌïòÎêò Îã®ÏàúÌôî
    # =========================================================================

    async def _process_single_chunk(self, chunk: ChunkInfo) -> None:
        """
        Í∞úÎ≥Ñ Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ ÌïµÏã¨ Î°úÏßÅ

        Í∏∞Ï°¥ _process_current_chunk() Î°úÏßÅÏùÑ Îã®ÏàúÌôîÌïòÏó¨ Ïù¥Ïãù.
        ÏÉÅÌÉú Í¥ÄÎ¶¨Îäî ChunkInfoÏóêÏÑú ÏßÅÏ†ë Ï≤òÎ¶¨ÌïòÍ≥†, Î≥µÏû°Ìïú Ï§ëÍ∞Ñ ÏÉÅÌÉú Ï†úÍ±∞.
        """
        logger.info(f"Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ ÏãúÏûë: {chunk.chunk_id}")
        chunk.mark_processing()

        try:
            # 1. Í≤πÏπ® Î∂ÑÏÑù (Ï≤´ Ï≤≠ÌÅ¨Îäî Ï°∞Í±¥Î∂Ä Í±¥ÎÑàÎõ∞Í∏∞)
            request_type = self._get_request_type_from_chunk(chunk)
            is_first_chunk = chunk.chunk_index == 0

            if not self._should_skip_overlap_analysis(is_first_chunk, request_type):
                overlap_result = await self._analyze_chunk_overlap(chunk)
                if overlap_result:
                    chunk.set_overlap_info(overlap_result)
                    self._log_chunk_info_debug(chunk, status="overlap_analyzed")

            # 2. Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Î∞è Ï≤òÎ¶¨
            if chunk.needs_api_call():
                # API Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
                api_response = await self._fetch_api_data(chunk)
                chunk.set_api_response_info(api_response)

                # Îπà Ï∫îÎì§ Ï≤òÎ¶¨
                final_candles = await self._process_empty_candles(api_response, chunk, is_first_chunk)
                chunk.set_final_candle_info(final_candles)

                # Ï†ÄÏû•
                if not self.dry_run:
                    await self.repository.save_raw_api_data(
                        chunk.symbol, chunk.timeframe, final_candles
                    )
                else:
                    logger.info(f"üîÑ DRY-RUN: Ï†ÄÏû• ÏãúÎÆ¨Î†àÏù¥ÏÖò {len(final_candles)}Í∞ú")

            else:
                # COMPLETE_OVERLAP: API Ìò∏Ï∂ú ÏóÜÏù¥ ÏôÑÎ£å
                logger.debug("ÏôÑÏ†Ñ Í≤πÏπ® ‚Üí API Ìò∏Ï∂ú ÏóÜÏù¥ ÏôÑÎ£å")
                chunk.set_api_response_info([])
                chunk.set_final_candle_info([])

            # 3. Ï≤≠ÌÅ¨ ÏôÑÎ£å Ï≤òÎ¶¨
            chunk.mark_completed()
            self._log_chunk_info_debug(chunk, status="completed")

            logger.info(f"Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ ÏôÑÎ£å: {chunk.chunk_id}")

        except Exception as e:
            chunk.mark_failed()
            logger.error(f"Ï≤≠ÌÅ¨ Ï≤òÎ¶¨ Ïã§Ìå®: {chunk.chunk_id}, Ïò§Î•ò: {e}")
            raise

    def _create_chunk(
        self,
        chunk_index: int,
        request_info: RequestInfo,
        plan: CollectionPlan,
        completed_chunks: List[ChunkInfo]
    ) -> ChunkInfo:
        """
        Ï≤≠ÌÅ¨ ÏÉùÏÑ± (Ïù¥Ï†Ñ Ï≤≠ÌÅ¨ Í∏∞Î∞ò Ïó∞ÏÜçÏÑ±)

        Í∏∞Ï°¥ _create_next_chunk() Î°úÏßÅÏùÑ Îã®ÏàúÌôîÌïòÏó¨ Ïù¥Ïãù.
        InternalCollectionState ÏóÜÏù¥ List[ChunkInfo]ÎßåÏúºÎ°ú Ïó∞ÏÜçÏÑ± Í¥ÄÎ¶¨.
        """
        # Ï≤≠ÌÅ¨ ÌÅ¨Í∏∞ Í≥ÑÏÇ∞ (ÎÇ®ÏùÄ Í∞úÏàò Í≥†Î†§)
        collected_count = sum(c.final_candle_count or 0 for c in completed_chunks if c.is_completed())
        remaining_count = request_info.expected_count - collected_count
        chunk_count = min(remaining_count, self.chunk_size)

        if chunk_index == 0:
            # Ï≤´ Î≤àÏß∏ Ï≤≠ÌÅ¨: planÏùò first_chunk_params ÏÇ¨Ïö©
            params = plan.first_chunk_params.copy()
            chunk_count = params.get("count", chunk_count)
            to_time = params.get("to", None)

            if isinstance(to_time, str):
                # Î¨∏ÏûêÏó¥Ïù¥Î©¥ datetimeÏúºÎ°ú Î≥ÄÌôò
                to_time = datetime.fromisoformat(to_time)

            # end ÏãúÍ∞Ñ Í≥ÑÏÇ∞
            end_time = None
            if to_time and chunk_count:
                end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

        else:
            # ÌõÑÏÜç Ï≤≠ÌÅ¨: Ïù¥Ï†Ñ Ï≤≠ÌÅ¨Ïùò Ïú†Ìö® ÎÅù ÏãúÍ∞Ñ Í∏∞Î∞ò Ïó∞ÏÜçÏÑ±
            last_chunk = completed_chunks[-1]
            last_effective_time = last_chunk.get_effective_end_time()

            if not last_effective_time:
                raise ValueError(f"Ïù¥Ï†Ñ Ï≤≠ÌÅ¨({last_chunk.chunk_id})ÏóêÏÑú Ïú†Ìö®Ìïú ÎÅù ÏãúÍ∞ÑÏùÑ Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏäµÎãàÎã§")

            # Îã§Ïùå Ï≤≠ÌÅ¨ ÏãúÏûë = Ïù¥Ï†Ñ Ï≤≠ÌÅ¨ ÎÅù - 1Ìã± (Ïó∞ÏÜçÏÑ±)
            to_time = TimeUtils.get_time_by_ticks(last_effective_time, request_info.timeframe, -1)
            end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

        # ChunkInfo ÏÉùÏÑ±
        chunk = ChunkInfo(
            chunk_id=f"{request_info.symbol}_{request_info.timeframe}_{chunk_index:05d}",
            chunk_index=chunk_index,
            symbol=request_info.symbol,
            timeframe=request_info.timeframe,
            count=chunk_count,
            to=to_time,
            end=end_time
        )

        self._log_chunk_info_debug(chunk, status="created")
        return chunk

    def _get_request_type_from_chunk(self, chunk: ChunkInfo) -> RequestType:
        """Ï≤≠ÌÅ¨Î°úÎ∂ÄÌÑ∞ ÏöîÏ≤≠ ÌÉÄÏûÖ Ï∂îÏ†ï (Legacy Ìò∏Ìôò)"""
        if chunk.chunk_index == 0:
            # Ï≤´ Ï≤≠ÌÅ¨: to Ïó¨Î∂ÄÎ°ú ÌåêÎã®
            if chunk.to is None:
                return RequestType.COUNT_ONLY
            else:
                return RequestType.TO_COUNT  # Îã®ÏàúÌôî (TO_ENDÎäî Î≥ÑÎèÑ Ï≤òÎ¶¨ ÌïÑÏöî)
        else:
            # ÌõÑÏÜç Ï≤≠ÌÅ¨: Ìï≠ÏÉÅ TO_COUNT Ìå®ÌÑ¥
            return RequestType.TO_COUNT

    def _should_skip_overlap_analysis(self, is_first_chunk: bool, request_type: RequestType) -> bool:
        """Í≤πÏπ® Î∂ÑÏÑù Í±¥ÎÑàÎõ∞Í∏∞ Ïó¨Î∂Ä ÌåêÎã®"""
        return is_first_chunk and request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]

    async def _analyze_chunk_overlap(self, chunk: ChunkInfo) -> Optional[OverlapResult]:
        """
        Ï≤≠ÌÅ¨ Í≤πÏπ® Î∂ÑÏÑù

        Í∏∞Ï°¥ _analyze_overlap() Î°úÏßÅ Îã®ÏàúÌôî.
        ChunkInfoÏóêÏÑú ÏßÅÏ†ë Ï†ïÎ≥¥ Ï∂îÏ∂úÌïòÏó¨ OverlapAnalyzer Ìò∏Ï∂ú.
        """
        if not chunk.to or not chunk.end:
            logger.debug(f"Í≤πÏπ® Î∂ÑÏÑù Í±¥ÎÑàÎúÄ: {chunk.chunk_id} (ÏãúÍ∞Ñ Ï†ïÎ≥¥ ÏóÜÏùå)")
            return None

        logger.debug(f"Í≤πÏπ® Î∂ÑÏÑù: {chunk.symbol} {chunk.timeframe}")

        try:
            # ÏòàÏÉÅ Ï∫îÎì§ Í∞úÏàò Í≥ÑÏÇ∞
            expected_count = TimeUtils.calculate_expected_count(chunk.to, chunk.end, chunk.timeframe)

            overlap_request = OverlapRequest(
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                target_start=chunk.to,
                target_end=chunk.end,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)
            logger.debug(f"Í≤πÏπ® Î∂ÑÏÑù Í≤∞Í≥º: {overlap_result.status.value}")

            return overlap_result

        except Exception as e:
            logger.warning(f"Í≤πÏπ® Î∂ÑÏÑù Ïã§Ìå®: {chunk.chunk_id}, Ïò§Î•ò: {e}")
            return None

    async def _fetch_api_data(self, chunk: ChunkInfo) -> List[Dict[str, Any]]:
        """
        API Îç∞Ïù¥ÌÑ∞ ÏàòÏßë

        Í∏∞Ï°¥ _fetch_from_api() Î°úÏßÅÏùÑ ChunkInfo Í∏∞Î∞òÏúºÎ°ú Îã®ÏàúÌôî.
        ÌÉÄÏûÑÌîÑÎ†àÏûÑÎ≥Ñ API Î∂ÑÍ∏∞Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌïòÎêò ÏÉÅÌÉú Í¥ÄÎ¶¨ Ï†úÍ±∞.
        """
        logger.debug(f"API Îç∞Ïù¥ÌÑ∞ ÏàòÏßë: {chunk.chunk_id}")

        # ChunkInfoÏóêÏÑú ÏµúÏ†ÅÌôîÎêú API ÌååÎùºÎØ∏ÌÑ∞ Ï∂îÏ∂ú
        api_count, api_to = chunk.get_api_params()

        try:
            # ÌÉÄÏûÑÌîÑÎ†àÏûÑÎ≥Ñ API Î©îÏÑúÎìú ÏÑ†ÌÉù (Legacy Î°úÏßÅ Î≥¥Ï°¥)
            if chunk.timeframe == '1s':
                to_param = self._format_time_param_seconds(api_to)
                candles = await self.upbit_client.get_candles_seconds(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe.endswith('m'):
                unit = int(chunk.timeframe[:-1])
                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî Î∂ÑÎ¥â Îã®ÏúÑ: {unit}")

                to_param = self._format_time_param_minutes(api_to, chunk.timeframe)
                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit, market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1d':
                to_param = self._format_time_param_days(api_to, chunk.timeframe)
                candles = await self.upbit_client.get_candles_days(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1w':
                to_param = self._format_time_param_weeks(api_to, chunk.timeframe)
                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1M':
                to_param = self._format_time_param_months(api_to, chunk.timeframe)
                candles = await self.upbit_client.get_candles_months(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1y':
                to_param = self._format_time_param_years(api_to, chunk.timeframe)
                candles = await self.upbit_client.get_candles_years(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            else:
                raise ValueError(f"ÏßÄÏõêÌïòÏßÄ ÏïäÎäî ÌÉÄÏûÑÌîÑÎ†àÏûÑ: {chunk.timeframe}")

            # API ÏàòÏßë ÏôÑÎ£å Î°úÍπÖ
            overlap_info = f" (overlap: {chunk.overlap_status.value})" if chunk.overlap_status else ""
            logger.info(f"API ÏàòÏßë ÏôÑÎ£å: {chunk.chunk_id}, {len(candles)}Í∞ú{overlap_info}")

            return candles

        except Exception as e:
            logger.error(f"API Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Ïã§Ìå®: {chunk.chunk_id}, Ïò§Î•ò: {e}")
            raise

    async def _process_empty_candles(
        self,
        api_candles: List[Dict[str, Any]],
        chunk: ChunkInfo,
        is_first_chunk: bool
    ) -> List[Dict[str, Any]]:
        """
        Îπà Ï∫îÎì§ Ï≤òÎ¶¨

        Í∏∞Ï°¥ _process_empty_candles() Î°úÏßÅÏùÑ ChunkInfo Í∏∞Î∞òÏúºÎ°ú Îã®ÏàúÌôî.
        Î≥µÏû°Ìïú ÏïàÏ†Ñ Î≤îÏúÑ Í≥ÑÏÇ∞ÏùÄ EmptyCandleDetectorÏóê ÏúÑÏûÑ.
        """
        if not self.enable_empty_candle_processing:
            return api_candles

        logger.debug(f"Îπà Ï∫îÎì§ Ï≤òÎ¶¨: {chunk.chunk_id}")

        try:
            # EmptyCandleDetector ÏÉùÏÑ±
            detector = self.empty_candle_detector_factory(chunk.symbol, chunk.timeframe)

            # Îπà Ï∫îÎì§ Í∞êÏßÄ Î∞è Ï±ÑÏö∞Í∏∞
            processed_candles = detector.detect_and_fill_gaps(
                api_candles,
                api_start=chunk.api_request_start or chunk.to,
                api_end=chunk.api_request_end or chunk.end,
                is_first_chunk=is_first_chunk
            )

            # Í≤∞Í≥º Î°úÍπÖ
            if len(processed_candles) != len(api_candles):
                empty_count = len(processed_candles) - len(api_candles)
                logger.info(f"Îπà Ï∫îÎì§ Ï±ÑÏõÄ: {len(api_candles)}Í∞ú + {empty_count}Í∞ú = {len(processed_candles)}Í∞ú")

            return processed_candles

        except Exception as e:
            logger.warning(f"Îπà Ï∫îÎì§ Ï≤òÎ¶¨ Ïã§Ìå®: {chunk.chunk_id}, Ïò§Î•ò: {e}")
            # Ìè¥Î∞±: ÏõêÎ≥∏ Î∞òÌôò
            return api_candles

    # =========================================================================
    # üõ†Ô∏è Ìó¨Ìçº Î©îÏÑúÎìúÎì§
    # =========================================================================

    def _format_time_param_seconds(self, api_to: Optional[datetime]) -> Optional[str]:
        """Ï¥àÎ¥â API ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Îß∑"""
        if not api_to:
            return None
        timeframe_delta = TimeUtils.get_timeframe_delta("1s")
        fetch_time = api_to + timeframe_delta
        return fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

    def _format_time_param_minutes(self, api_to: Optional[datetime], timeframe: str) -> Optional[str]:
        """Î∂ÑÎ¥â API ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Îß∑"""
        if not api_to:
            return None
        timeframe_delta = TimeUtils.get_timeframe_delta(timeframe)
        fetch_time = api_to + timeframe_delta
        return fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

    def _format_time_param_days(self, api_to: Optional[datetime], timeframe: str) -> Optional[str]:
        """ÏùºÎ¥â API ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Îß∑"""
        if not api_to:
            return None
        fetch_start_time = TimeUtils.get_time_by_ticks(api_to, timeframe, 1)
        return fetch_start_time.strftime("%Y-%m-%d")

    def _format_time_param_weeks(self, api_to: Optional[datetime], timeframe: str) -> Optional[str]:
        """Ï£ºÎ¥â API ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Îß∑"""
        if not api_to:
            return None
        fetch_start_time = TimeUtils.get_time_by_ticks(api_to, timeframe, 1)
        return fetch_start_time.strftime("%Y-%m-%d")

    def _format_time_param_months(self, api_to: Optional[datetime], timeframe: str) -> Optional[str]:
        """ÏõîÎ¥â API ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Îß∑"""
        if not api_to:
            return None
        fetch_start_time = TimeUtils.get_time_by_ticks(api_to, timeframe, 1)
        return fetch_start_time.strftime("%Y-%m")

    def _format_time_param_years(self, api_to: Optional[datetime], timeframe: str) -> Optional[str]:
        """Ïó∞Î¥â API ÏãúÍ∞Ñ ÌååÎùºÎØ∏ÌÑ∞ Ìè¨Îß∑"""
        if not api_to:
            return None
        fetch_start_time = TimeUtils.get_time_by_ticks(api_to, timeframe, 1)
        return fetch_start_time.strftime("%Y")

    def _create_success_result(
        self,
        chunks: List[ChunkInfo],
        request_info: RequestInfo,
        processing_time: float
    ) -> CollectionResult:
        """ÏÑ±Í≥µ Í≤∞Í≥º ÏÉùÏÑ±"""
        collected_count = sum(c.final_candle_count or 0 for c in chunks if c.is_completed())

        return CollectionResult(
            success=True,
            chunks=chunks,
            collected_count=collected_count,
            requested_count=request_info.expected_count,
            processing_time_seconds=processing_time
        )

    def _create_error_result(
        self,
        error: Exception,
        chunks: List[ChunkInfo],
        processing_time: float
    ) -> CollectionResult:
        """Ïò§Î•ò Í≤∞Í≥º ÏÉùÏÑ±"""
        collected_count = sum(c.final_candle_count or 0 for c in chunks if c.is_completed())

        return CollectionResult(
            success=False,
            chunks=chunks,
            collected_count=collected_count,
            requested_count=0,
            processing_time_seconds=processing_time,
            error=error
        )
