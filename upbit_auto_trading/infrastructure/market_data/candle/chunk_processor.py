"""
ğŸ“‹ ChunkProcessor v3.0 - candle_business_models.py ê¸°ë°˜ ë‹¨ìˆœí™” ë²„ì „
Created: 2025-09-23
Purpose: "ì†ŒìŠ¤ì˜ ì›ì²œ" ì›ì¹™ì— ë”°ë¥¸ ë‹¨ìˆœí™”ëœ ì²­í¬ ì²˜ë¦¬ ì—”ì§„
Architecture: RequestInfo + List[ChunkInfo] = ì™„ì „í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§

í•µì‹¬ ì„¤ê³„ ë³€ê²½:
1. ìƒíƒœ ê´€ë¦¬ ì œê±°: InternalCollectionState, CollectionProgress ë“± ì œê±°
2. ë‹¨ì¼ ì†ŒìŠ¤ ì›ì¹™: candle_business_models.pyì˜ 4ê°œ í•µì‹¬ ëª¨ë¸ë§Œ ì‚¬ìš©
3. ìˆœìˆ˜ ì²­í¬ ì²˜ë¦¬: ChunkProcessorëŠ” ê°œë³„ ì²­í¬ ì²˜ë¦¬ë§Œ ë‹´ë‹¹
4. ëª¨ë‹ˆí„°ë§ ë¶„ë¦¬: ë³µì¡í•œ ìƒíƒœ ì¶”ì ì„ ë³„ë„ ê³„ì¸µìœ¼ë¡œ ë¶„ë¦¬

ì£¼ìš” ë³€ê²½ì‚¬í•­:
- execute_full_collection() â†’ process_collection() (ë‹¨ìˆœí™”)
- InternalCollectionState ì œê±° â†’ List[ChunkInfo] ì§ì ‘ ì‚¬ìš©
- ë³µì¡í•œ ì™„ë£Œ íŒë‹¨ â†’ should_complete_collection() í•¨ìˆ˜ ì‚¬ìš©
- ìƒíƒœ ë™ê¸°í™” ë¡œì§ ì œê±° â†’ ChunkInfoì—ì„œ ì§ì ‘ ìƒíƒœ ê´€ë¦¬

ê¸°ëŒ€ íš¨ê³¼:
- ì½”ë“œ ë³µì¡ë„ 50% ì´ìƒ ê°ì†Œ
- ë””ë²„ê¹… ìš©ì´ì„± í–¥ìƒ (ChunkInfo JSON ë¡œê·¸)
- í™•ì¥ì„± í–¥ìƒ (ëª¨ë‹ˆí„°ë§ ìš”êµ¬ì‚¬í•­ ë³€ê²½ ì‹œ í•µì‹¬ ë¡œì§ ì˜í–¥ ì—†ìŒ)
"""

import time
import json
from datetime import datetime
from typing import Optional, Dict, Any, Callable, List
# Infrastructure ë¡œê¹…
from upbit_auto_trading.infrastructure.logging import create_component_logger
# í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ëª¨ë¸ (candle_business_models.py)
from upbit_auto_trading.infrastructure.market_data.candle.models.candle_business_models import (
    RequestInfo,
    CollectionPlan,
    CollectionResult,
    ChunkInfo,
    OverlapRequest,
    OverlapResult,
    RequestType,
    should_complete_collection,
    create_collection_plan
)
# ì˜ì¡´ì„± (Infrastructure ê³„ì¸µ)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)
from upbit_auto_trading.infrastructure.market_data.candle.empty_candle_detector import (
    EmptyCandleDetector
)
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils

# ì§„í–‰ ìƒí™© ì½œë°± íƒ€ì…
ProgressCallback = Callable[[int, int], None]  # (completed_chunks, total_chunks)
logger = create_component_logger("ChunkProcessor")


class ChunkProcessor:
    """
    ChunkProcessor v3.0 - ë‹¨ìˆœí™”ëœ ì²­í¬ ì²˜ë¦¬ ì—”ì§„
    í•µì‹¬ ì›ì¹™:
    1. ë‹¨ì¼ ì†ŒìŠ¤: candle_business_models.pyì˜ 4ê°œ í•µì‹¬ ëª¨ë¸ë§Œ ì‚¬ìš©
    2. ìƒíƒœ ê´€ë¦¬ ì œê±°: ë³µì¡í•œ CollectionState, InternalCollectionState ì œê±°
    3. ìˆœìˆ˜ ì²˜ë¦¬: RequestInfo + List[ChunkInfo] = ì™„ì „í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
    4. ëª¨ë‹ˆí„°ë§ ë¶„ë¦¬: ì§„í–‰ ìƒí™©ì€ ë³„ë„ ê³„ì¸µì—ì„œ ChunkInfo ê¸°ë°˜ ë¶„ì„
    ì£¼ìš” ì¸í„°í˜ì´ìŠ¤:
    - process_collection(): RequestInfo â†’ List[ChunkInfo] (ë©”ì¸ API)
    - process_single_chunk(): ê°œë³„ ì²­í¬ ì²˜ë¦¬ (CandleDataProvider ì—°ë™ìš©)
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        empty_candle_detector_factory: Callable[[str, str], EmptyCandleDetector],
        chunk_size: int = 200,
        enable_empty_candle_processing: bool = True,
        dry_run: bool = False
    ):
        """
        ChunkProcessor v3.0 ì´ˆê¸°í™”
        Args:
            repository: ìº”ë“¤ ë°ì´í„° ì €ì¥ì†Œ
            upbit_client: ì—…ë¹„íŠ¸ API í´ë¼ì´ì–¸íŠ¸
            overlap_analyzer: ê²¹ì¹¨ ë¶„ì„ê¸°
            empty_candle_detector_factory: ë¹ˆ ìº”ë“¤ ê°ì§€ê¸° íŒ©í† ë¦¬
            chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ 200, ì—…ë¹„íŠ¸ ì œí•œ)
            enable_empty_candle_processing: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
            dry_run: ê±´ì‹ ì‹¤í–‰ (ì‹¤ì œ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        """

        # ì˜ì¡´ì„± ì£¼ì…
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer
        self.empty_candle_detector_factory = empty_candle_detector_factory

        # ì„¤ì •
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜
        self.enable_empty_candle_processing = enable_empty_candle_processing
        self.dry_run = dry_run

        # Legacy í˜¸í™˜ ì„¤ì •
        self.api_rate_limit_rps = 10  # 10 RPS ê¸°ì¤€
        logger.info("ChunkProcessor v3.0 ì´ˆê¸°í™” ì™„ë£Œ (ë‹¨ìˆœí™” ë²„ì „)")
        logger.info(f"ì²­í¬ í¬ê¸°: {self.chunk_size}, "
                    f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: {'í™œì„±í™”' if enable_empty_candle_processing else 'ë¹„í™œì„±í™”'}, "
                    f"API Rate Limit: {self.api_rate_limit_rps} RPS, "
                    f"DRY-RUN: {'í™œì„±í™”' if dry_run else 'ë¹„í™œì„±í™”'}")

    def _log_chunk_info_debug(
        self,
        chunk_info: ChunkInfo,
        status: str = "unknown"
    ) -> None:
        """ChunkInfo ìƒíƒœë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë””ë²„ê·¸ ì¶œë ¥ - ëª¨ë“  ChunkInfo ì†ì„± í¬í•¨"""
        if not chunk_info:
            return
        try:
            effective_end = chunk_info.get_effective_end_time()
            debug_data = {
                # === ì²­í¬ ì‹ë³„ ì •ë³´ ===
                "chunk_id": chunk_info.chunk_id,
                "chunk_index": chunk_info.chunk_index,
                "symbol": chunk_info.symbol,
                "timeframe": chunk_info.timeframe,
                "status": status,
                "chunk_status": chunk_info.chunk_status.value,

                # === ì²­í¬ íŒŒë¼ë¯¸í„° ===
                "count": chunk_info.count,
                "to": chunk_info.to.isoformat() if chunk_info.to else None,
                "end": chunk_info.end.isoformat() if chunk_info.end else None,

                # === ê²¹ì¹¨ ë¶„ì„ ì •ë³´ ===
                "overlap_status": chunk_info.overlap_status.value if chunk_info.overlap_status else None,

                # === OverlapResult í†µí•© í•„ë“œ (DB ê¸°ì¡´ ë°ì´í„°) ===
                "db_start": chunk_info.db_start.isoformat() if chunk_info.db_start else None,
                "db_end": chunk_info.db_end.isoformat() if chunk_info.db_end else None,

                # === ìš”ì²­ ë‹¨ê³„ (ì˜¤ë²„ë© ë¶„ì„ ê²°ê³¼) ===
                "api_request_count": chunk_info.api_request_count,
                "api_request_start": (
                    chunk_info.api_request_start.isoformat()
                    if chunk_info.api_request_start else None
                ),
                "api_request_end": (
                    chunk_info.api_request_end.isoformat()
                    if chunk_info.api_request_end else None
                ),

                # === API í˜¸ì¶œ ìºì‹œ íŒŒë¼ë¯¸í„° ===
                "api_fetch_count": chunk_info.api_fetch_count,
                "api_fetch_to": (
                    chunk_info.api_fetch_to.isoformat()
                    if chunk_info.api_fetch_to else None
                ),

                # === ì‘ë‹µ ë‹¨ê³„ (ì‹¤ì œ API ì‘ë‹µ) ===
                "api_response_count": chunk_info.api_response_count,
                "api_response_start": (
                    chunk_info.api_response_start.isoformat()
                    if chunk_info.api_response_start else None
                ),
                "api_response_end": (
                    chunk_info.api_response_end.isoformat()
                    if chunk_info.api_response_end else None
                ),

                # === ìµœì¢… ì²˜ë¦¬ ë‹¨ê³„ (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í›„) ===
                "final_candle_count": chunk_info.final_candle_count,
                "final_candle_start": (
                    chunk_info.final_candle_start.isoformat()
                    if chunk_info.final_candle_start else None
                ),
                "final_candle_end": (
                    chunk_info.final_candle_end.isoformat()
                    if chunk_info.final_candle_end else None
                ),

                # === ê³„ì‚°ëœ ìº”ë“¤ ìˆ˜ ìºì‹œ ===
                "effective_candle_count": chunk_info.effective_candle_count,
                "cumulative_candle_count": chunk_info.cumulative_candle_count,

                # === ì²˜ë¦¬ ìƒíƒœ ì •ë³´ ===
                "created_at": (
                    chunk_info.created_at.isoformat()
                    if chunk_info.created_at else None
                ),
                "processing_started_at": (
                    chunk_info.processing_started_at.isoformat()
                    if chunk_info.processing_started_at else None
                ),
                "completed_at": (
                    chunk_info.completed_at.isoformat()
                    if chunk_info.completed_at else None
                ),
                "processing_duration_seconds": chunk_info.get_processing_duration(),

                # === ê³„ì‚°ëœ/íŒŒìƒ ì •ë³´ (í¸ì˜ í•„ë“œ) ===
                "effective_end_time": (
                    effective_end.isoformat() if effective_end else None
                ),
                "time_source": chunk_info.get_time_source(),
                "processing_time_ms": (
                    chunk_info.get_processing_duration() * 1000
                    if chunk_info.get_processing_duration() else None
                ),
                "has_overlap_info": chunk_info.has_overlap_info(),
                "needs_api_call": chunk_info.needs_api_call(),
                "needs_partial_api_call": chunk_info.needs_partial_api_call(),
                "has_complete_time_info": chunk_info.has_complete_time_info(),
                "calculated_effective_candle_count": (
                    chunk_info.calculate_effective_candle_count()
                ),

                # === API íŒŒë¼ë¯¸í„° (í˜„ì¬ ê³„ì‚°ê°’) ===
                "current_api_params": {
                    "count": chunk_info.get_api_params()[0],
                    "to": (
                        chunk_info.get_api_params()[1].isoformat()
                        if chunk_info.get_api_params()[1] else None
                    )
                }
            }
            logger.debug(
                f"ğŸ” ChunkInfo Complete Dump: "
                f"{json.dumps(debug_data, ensure_ascii=False, indent=2)}"
            )
        except Exception as e:
            logger.warning(f"ChunkInfo ë””ë²„ê·¸ ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {e}")

    def _log_request_info_debug(
        self,
        request_info: RequestInfo,
        status: str = "created"
    ) -> None:
        """RequestInfo ìƒíƒœë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë””ë²„ê·¸ ì¶œë ¥ - ëª¨ë“  RequestInfo ì†ì„± í¬í•¨"""
        if not request_info:
            return
        try:
            debug_data = {
                # === ê¸°ë³¸ íŒŒë¼ë¯¸í„° (ì‚¬ìš©ì ì…ë ¥) ===
                "symbol": request_info.symbol,
                "timeframe": request_info.timeframe,
                "count": request_info.count,
                "to": request_info.to.isoformat() if request_info.to else None,
                "end": request_info.end.isoformat() if request_info.end else None,
                "request_at": (
                    request_info.request_at.isoformat()
                    if request_info.request_at else None
                ),

                # === ì‚¬ì „ ê³„ì‚°ëœ í•„ë“œë“¤ (ì •ê·œí™”/ìµœì í™”ë¨) ===
                "aligned_to": (
                    request_info.aligned_to.isoformat()
                    if request_info.aligned_to else None
                ),
                "aligned_end": (
                    request_info.aligned_end.isoformat()
                    if request_info.aligned_end else None
                ),
                "expected_count": request_info.expected_count,

                # === ê³„ì‚°ëœ/íŒŒìƒ ì •ë³´ (ë¶„ì„ìš©) ===
                "request_type": request_info.get_request_type().value,
                "should_align_time": request_info.should_align_time(),
                "needs_current_time_fallback": request_info.needs_current_time_fallback(),
                "should_skip_overlap_analysis_for_first_chunk": (
                    request_info.should_skip_overlap_analysis_for_first_chunk()
                ),

                # === í¸ì˜ ì ‘ê·¼ì ê²°ê³¼ ===
                "aligned_to_time": (
                    request_info.get_aligned_to_time().isoformat()
                    if request_info.get_aligned_to_time() else None
                ),
                "aligned_end_time": (
                    request_info.get_aligned_end_time().isoformat()
                    if request_info.get_aligned_end_time() else None
                ),
                "expected_count_accessor": request_info.get_expected_count(),

                # === ë¡œê¹… ë¬¸ìì—´ ===
                "log_string": request_info.to_log_string(),
                "internal_log_string": request_info.to_internal_log_string(),

                # === ë””ë²„ê·¸ ìƒíƒœ ===
                "status": status
            }
            logger.debug(
                f"ğŸ” RequestInfo Complete Dump: "
                f"{json.dumps(debug_data, ensure_ascii=False, indent=2)}"
            )
        except Exception as e:
            logger.warning(f"RequestInfo ë””ë²„ê·¸ ë¡œê·¸ ìƒì„± ì‹¤íŒ¨: {e}")

    # ğŸš€ ë©”ì¸ API - ë‹¨ìˆœí™”ëœ ì²­í¬ ì²˜ë¦¬
    async def process_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None,
        progress_callback: Optional[ProgressCallback] = None
    ) -> CollectionResult:
        """
        ğŸš€ ë‹¨ìˆœí™”ëœ ìº”ë“¤ ìˆ˜ì§‘ - candle_business_models.py ê¸°ë°˜
        ë³µì¡í•œ ìƒíƒœ ê´€ë¦¬ í´ë˜ìŠ¤ ì œê±°í•˜ê³  RequestInfo + List[ChunkInfo]ë§Œìœ¼ë¡œ ë™ì‘.
        ì²­í¬ ì²˜ë¦¬ì˜ ìì—°ìŠ¤ëŸ¬ìš´ íë¦„ì„ êµ¬í˜„í•œ ë©”ì¸ API.
        Args:
            symbol: ê±°ë˜ ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m', '5m', '1d')
            count: ìˆ˜ì§‘í•  ìº”ë“¤ ê°œìˆ˜
            to: ì‹œì‘ ì‹œì  (ìµœì‹  ìº”ë“¤ ê¸°ì¤€)
            end: ì¢…ë£Œ ì‹œì  (ê³¼ê±° ìº”ë“¤ ê¸°ì¤€)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜ (completed_chunks, total_chunks)
        Returns:
            CollectionResult: ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì •ë³´
        Raises:
            ValueError: ì˜ëª»ëœ íŒŒë¼ë¯¸í„°
            Exception: ìˆ˜ì§‘ ê³¼ì • ì˜¤ë¥˜
        """
        start_time = time.time()

        logger.info(f"ë‹¨ìˆœí™”ëœ ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘: {symbol} {timeframe}")
        if count:
            logger.info(f"ê°œìˆ˜: {count:,}ê°œ")
        if to:
            logger.info(f"ì‹œì‘: {to}")
        if end:
            logger.info(f"ì¢…ë£Œ: {end}")
        if self.dry_run:
            logger.info("ğŸ”„ DRY-RUN ëª¨ë“œ: ì‹¤ì œ ì €ì¥í•˜ì§€ ì•ŠìŒ")
        try:
            # 1. RequestInfo ìƒì„± (ë‹¨ì¼ ì†ŒìŠ¤)
            request_info = RequestInfo(
                symbol=symbol,
                timeframe=timeframe,
                count=count,
                to=TimeUtils.normalize_datetime_to_utc(to) if to else None,
                end=TimeUtils.normalize_datetime_to_utc(end) if end else None
            )

            # RequestInfo ë””ë²„ê·¸ ì¶œë ¥ (í•œ ë²ˆë§Œ)
            self._log_request_info_debug(request_info, "initialized")

            # 2. ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½ (ë‹¨ìˆœí™”)
            plan = create_collection_plan(request_info, self.chunk_size, self.api_rate_limit_rps)
            logger.info(f"ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {plan.total_count:,}ê°œ ìº”ë“¤, {plan.estimated_chunks}ì²­í¬, "
                        f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {plan.estimated_duration_seconds:.1f}ì´ˆ")

            # 3. ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬ (ë‹¨ìˆœí•œ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬)
            chunks: List[ChunkInfo] = []
            for chunk_index in range(plan.estimated_chunks):

                # ì²­í¬ ìƒì„± (ì´ì „ ì²­í¬ ê²°ê³¼ ê¸°ë°˜ ì—°ì†ì„±)
                chunk = self._create_chunk(chunk_index, request_info, plan, chunks)

                # ê°œë³„ ì²­í¬ ì²˜ë¦¬
                await self._process_single_chunk(chunk)

                previous_total = 0
                if chunks:
                    # ì§€ê¸ˆê¹Œì§€ ì²˜ë¦¬ëœ ì²­í¬ë“¤ ì¤‘ì—ì„œ, ëˆ„ì  ìº”ë“¤ ìˆ˜ê°€ í™•ì •ëœ ê°€ì¥ ìµœê·¼ ì²­í¬ë¥¼ ì°¾ì•„ì„œ ê·¸ ëˆ„ì  countë¥¼ ì‚¬ìš©
                    last_completed = next((c for c in reversed(chunks) if c.cumulative_candle_count is not None), None)
                    if last_completed:
                        previous_total = last_completed.cumulative_candle_count
                    else:
                        # ëª¨ë‘ ì—†ìœ¼ë©´, ëª¨ë“  ì™„ë£Œëœ ì²­í¬ë“¤ì˜ ìœ íš¨ ìº”ë“¤ ìˆ˜ í•©ì‚°
                        previous_total = sum(
                            c.calculate_effective_candle_count()
                            for c in chunks
                            if c.is_completed()
                        )

                chunk.update_cumulative_candle_count(previous_total)
                chunks.append(chunk)

                # ì§„í–‰ë¥  ë³´ê³ 
                if progress_callback:
                    progress_callback(len(chunks), plan.estimated_chunks)

                # ì™„ë£Œ íŒë‹¨ (ë‹¨ìˆœí™”)
                if should_complete_collection(request_info, chunks):
                    logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ ì¡°ê±´ ë‹¬ì„±: {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬")
                    break

            # 4. ìµœì¢… ê²°ê³¼ ìƒì„±
            processing_time = time.time() - start_time
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, ì²˜ë¦¬ ì‹œê°„ {processing_time:.2f}s")
            return self._create_success_result(chunks, request_info)

        except Exception as e:
            logger.error(f"ë‹¨ìˆœí™”ëœ ìº”ë“¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self._create_error_result(e)

    # ğŸ”— CandleDataProvider ì—°ë™ìš© API (í•˜ìœ„ í˜¸í™˜ì„±)
    async def process_single_chunk(self, chunk: ChunkInfo) -> ChunkInfo:
        """
        ### ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬ - CandleDataProvider ì—°ë™ìš©
        - ê¸°ì¡´ execute_single_chunk() ì¸í„°í˜ì´ìŠ¤ ëŒ€ì²´.
        - ChunkInfoë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ê³  ë™ì¼í•œ ChunkInfoë¥¼ ë°˜í™˜ (ìƒíƒœ ì—…ë°ì´íŠ¸ë¨).
        Args:
            chunk: ì²˜ë¦¬í•  ì²­í¬ ì •ë³´
        Returns:
            ChunkInfo: ì²˜ë¦¬ ì™„ë£Œëœ ë™ì¼í•œ ì²­í¬ (ìƒíƒœ ì—…ë°ì´íŠ¸ë¨)
        """
        logger.debug(f"ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬: {chunk.chunk_id}")
        try:
            await self._process_single_chunk(chunk)
            return chunk

        except Exception as e:
            logger.error(f"ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            chunk.mark_failed()
            raise

    # ğŸ—ï¸ í•µì‹¬ ì²˜ë¦¬ ë¡œì§ - Legacy ë¡œì§ ë³´ì¡´í•˜ë˜ ë‹¨ìˆœí™”
    async def _process_single_chunk(self, chunk: ChunkInfo) -> None:
        """
        ### ê°œë³„ ì²­í¬ ì²˜ë¦¬ í•µì‹¬ ë¡œì§
        - ê¸°ì¡´ _process_current_chunk() ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì´ì‹.
        - ìƒíƒœ ê´€ë¦¬ëŠ” ChunkInfoì—ì„œ ì§ì ‘ ì²˜ë¦¬í•˜ê³ , ë³µì¡í•œ ì¤‘ê°„ ìƒíƒœ ì œê±°.
        """
        logger.info(f"ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {chunk.chunk_id}")
        chunk.mark_processing()
        try:
            # 1. ê²¹ì¹¨ ë¶„ì„ (ì²« ì²­í¬ëŠ” ì¡°ê±´ë¶€ ê±´ë„ˆë›°ê¸°)
            request_type = self._get_request_type_from_chunk(chunk)
            is_first_chunk = chunk.chunk_index == 0
            if not self._should_skip_overlap_analysis(is_first_chunk, request_type):
                overlap_result = await self._analyze_chunk_overlap(chunk)
                if overlap_result:
                    chunk.set_overlap_info(overlap_result)
                    self._log_chunk_info_debug(chunk, status="overlap_analyzed")

            # 2. ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
            if chunk.needs_api_call():
                # API ë°ì´í„° ìˆ˜ì§‘
                api_response = await self._fetch_api_data(chunk)
                chunk.set_api_response_info(api_response)
                # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
                final_candles = await self._process_empty_candles(api_response, chunk, is_first_chunk)
                chunk.set_final_candle_info(final_candles)
                # ì €ì¥
                if not self.dry_run:
                    await self.repository.save_raw_api_data(
                        chunk.symbol, chunk.timeframe, final_candles
                    )
                else:
                    logger.info(f"ğŸ”„ DRY-RUN: ì €ì¥ ì‹œë®¬ë ˆì´ì…˜ {len(final_candles)}ê°œ")
            else:
                # COMPLETE_OVERLAP: API í˜¸ì¶œ ì—†ì´ ì™„ë£Œ
                logger.debug("ì™„ì „ ê²¹ì¹¨ â†’ API í˜¸ì¶œ ì—†ì´ ì™„ë£Œ")
                chunk.set_api_response_info([])
                chunk.set_final_candle_info([])
            # 3. ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
            chunk.mark_completed()
            self._log_chunk_info_debug(chunk, status="completed")
            logger.info(f"ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: {chunk.chunk_id}")
        except Exception as e:
            chunk.mark_failed()
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    def _create_chunk(
        self,
        chunk_index: int,
        request_info: RequestInfo,
        plan: CollectionPlan,
        completed_chunks: List[ChunkInfo]
    ) -> ChunkInfo:
        """
        ### ì²­í¬ ìƒì„± (ì´ì „ ì²­í¬ ê¸°ë°˜ ì—°ì†ì„±)
        - ê¸°ì¡´ _create_next_chunk() ë¡œì§ì„ ë‹¨ìˆœí™”í•˜ì—¬ ì´ì‹.
        - InternalCollectionState ì—†ì´ List[ChunkInfo]ë§Œìœ¼ë¡œ ì—°ì†ì„± ê´€ë¦¬.
        """
        # ì²­í¬ í¬ê¸° ê³„ì‚° (ë‚¨ì€ ê°œìˆ˜ ê³ ë ¤)
        collected_count = 0
        if completed_chunks:
            last_completed = next((c for c in reversed(completed_chunks) if c.is_completed()), None)

            if last_completed:
                if last_completed.cumulative_candle_count is not None:
                    collected_count = last_completed.cumulative_candle_count

                else:
                    collected_count = sum(
                        c.calculate_effective_candle_count()
                        for c in completed_chunks
                        if c.is_completed()
                    )

        remaining_count = request_info.expected_count - collected_count
        chunk_count = min(remaining_count, self.chunk_size)

        if chunk_index == 0:
            # ì²« ë²ˆì§¸ ì²­í¬: planì˜ first_chunk_params ì‚¬ìš©
            params = plan.first_chunk_params.copy()
            chunk_count = params.get("count", chunk_count)
            to_time = params.get("to", None)

            if isinstance(to_time, str):
                # ë¬¸ìì—´ì´ë©´ datetimeìœ¼ë¡œ ë³€í™˜
                to_time = datetime.fromisoformat(to_time)

            # end ì‹œê°„ ê³„ì‚°
            end_time = None
            if to_time and chunk_count:
                end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))
        else:
            # í›„ì† ì²­í¬: ì´ì „ ì²­í¬ì˜ ìœ íš¨ ë ì‹œê°„ ê¸°ë°˜ ì—°ì†ì„±
            last_chunk = completed_chunks[-1]
            last_effective_time = last_chunk.get_effective_end_time()

            if not last_effective_time:
                raise ValueError(f"ì´ì „ ì²­í¬({last_chunk.chunk_id})ì—ì„œ ìœ íš¨í•œ ë ì‹œê°„ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ = ì´ì „ ì²­í¬ ë - 1í‹± (ì—°ì†ì„±)
            to_time = TimeUtils.get_time_by_ticks(last_effective_time, request_info.timeframe, -1)
            end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

        # ChunkInfo ìƒì„±
        chunk = ChunkInfo(
            chunk_id=f"{request_info.symbol}_{request_info.timeframe}_{chunk_index:05d}",
            chunk_index=chunk_index,
            symbol=request_info.symbol,
            timeframe=request_info.timeframe,
            count=chunk_count,
            to=to_time,
            end=end_time
        )

        self._log_chunk_info_debug(chunk, status="created")
        return chunk

    def _get_request_type_from_chunk(self, chunk: ChunkInfo) -> RequestType:
        """ì²­í¬ë¡œë¶€í„° ìš”ì²­ íƒ€ì… ì¶”ì • (Legacy í˜¸í™˜)"""
        if chunk.chunk_index == 0:
            # ì²« ì²­í¬: to ì—¬ë¶€ë¡œ íŒë‹¨
            if chunk.to is None:
                return RequestType.COUNT_ONLY
            else:
                return RequestType.TO_COUNT  # ë‹¨ìˆœí™” (TO_ENDëŠ” ë³„ë„ ì²˜ë¦¬ í•„ìš”)
        else:
            # í›„ì† ì²­í¬: í•­ìƒ TO_COUNT íŒ¨í„´
            return RequestType.TO_COUNT

    def _should_skip_overlap_analysis(self, is_first_chunk: bool, request_type: RequestType) -> bool:
        """ê²¹ì¹¨ ë¶„ì„ ê±´ë„ˆë›°ê¸° ì—¬ë¶€ íŒë‹¨"""
        return is_first_chunk and request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]

    async def _analyze_chunk_overlap(self, chunk: ChunkInfo) -> Optional[OverlapResult]:
        """
        ì²­í¬ ê²¹ì¹¨ ë¶„ì„
        ê¸°ì¡´ _analyze_overlap() ë¡œì§ ë‹¨ìˆœí™”.
        ChunkInfoì—ì„œ ì§ì ‘ ì •ë³´ ì¶”ì¶œí•˜ì—¬ OverlapAnalyzer í˜¸ì¶œ.
        """
        if not chunk.to or not chunk.end:
            logger.debug(f"ê²¹ì¹¨ ë¶„ì„ ê±´ë„ˆëœ€: {chunk.chunk_id} (ì‹œê°„ ì •ë³´ ì—†ìŒ)")
            return None
        logger.debug(f"ê²¹ì¹¨ ë¶„ì„: {chunk.symbol} {chunk.timeframe}")
        try:
            # ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
            expected_count = TimeUtils.calculate_expected_count(chunk.to, chunk.end, chunk.timeframe)
            overlap_request = OverlapRequest(
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                target_start=chunk.to,
                target_end=chunk.end,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)
            logger.debug(f"ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼: {overlap_result.status.value}")
            return overlap_result

        except Exception as e:
            logger.warning(f"ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            return None

    async def _fetch_api_data(self, chunk: ChunkInfo) -> List[Dict[str, Any]]:
        """
        API ë°ì´í„° ìˆ˜ì§‘
        ê¸°ì¡´ _fetch_from_api() ë¡œì§ì„ ChunkInfo ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”.
        íƒ€ì„í”„ë ˆì„ë³„ API ë¶„ê¸°ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ ìƒíƒœ ê´€ë¦¬ ì œê±°.
        """
        logger.debug(f"API ë°ì´í„° ìˆ˜ì§‘: {chunk.chunk_id}")
        api_count, api_to = chunk.get_api_params()

        if api_count <= 0:
            logger.debug(f"API í˜¸ì¶œ ê±´ìˆ˜ 0ìœ¼ë¡œ skip: {chunk.chunk_id}")
            return []

        if api_to is None:
            to_param = None
            logger.debug(f"ì²­í¬ {chunk.chunk_id}ëŠ” COUNT_ONLY ë˜ëŠ” END_ONLY â†’ to íŒŒë¼ë¯¸í„° ì—†ìŒ")
        else:
            try:
                # Upbit to exclusive ì´ë¯€ë¡œ ë¯¸ë˜ë¡œ í•œ í‹± ë³´ì •
                fetch_time = TimeUtils.get_time_by_ticks(api_to, chunk.timeframe, 1)
                logger.debug(f"ì§„ì¶œì  ë³´ì • {api_to} â†’ {fetch_time}")
                to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

            except Exception as exc:
                logger.error(f"to íŒŒë¼ë¯¸í„° ê³„ì‚° ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {exc}")
                raise
        try:
            if chunk.timeframe == '1s':
                candles = await self.upbit_client.get_candles_seconds(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe.endswith('m'):

                unit = int(chunk.timeframe[:-1])

                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ë´‰ ë‹¨ìœ„: {unit}")

                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit, market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1d':
                candles = await self.upbit_client.get_candles_days(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1w':
                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1M':
                candles = await self.upbit_client.get_candles_months(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            elif chunk.timeframe == '1y':
                candles = await self.upbit_client.get_candles_years(
                    market=chunk.symbol, count=api_count, to=to_param
                )

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk.timeframe}")

            overlap_info = f" (overlap: {chunk.overlap_status.value})" if chunk.overlap_status else ""
            logger.info(f"API ìˆ˜ì§‘ ì™„ë£Œ: {chunk.chunk_id}, {len(candles)}ê°œ{overlap_info}, to={to_param}")
            return candles

        except Exception as e:
            logger.error(f"API ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    async def _process_empty_candles(
        self,
        api_candles: List[Dict[str, Any]],
        chunk: ChunkInfo,
        is_first_chunk: bool
    ) -> List[Dict[str, Any]]:
        """
        ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
        ê¸°ì¡´ _process_empty_candles() ë¡œì§ì„ ChunkInfo ê¸°ë°˜ìœ¼ë¡œ ë‹¨ìˆœí™”.
        ë³µì¡í•œ ì•ˆì „ ë²”ìœ„ ê³„ì‚°ì€ EmptyCandleDetectorì— ìœ„ì„.
        """
        if not self.enable_empty_candle_processing:
            return api_candles
        logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: {chunk.chunk_id}")
        try:
            # EmptyCandleDetector ìƒì„±
            detector = self.empty_candle_detector_factory(chunk.symbol, chunk.timeframe)

            # TEST01: ì•ì´ ì—´ë¦° ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê°•ì œ í…ŒìŠ¤íŠ¸
            # handle_front_open_empty_candle = chunk.to is not None  # toê°€ ìˆìœ¼ë©´ ì²˜ë¦¬ í—ˆìš©
            handle_front_open_empty_candle = False  # ê¸°ë³¸ì ìœ¼ë¡œ ì•ì´ ì—´ë¦° ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì•ˆ í•¨

            # ë¹ˆ ìº”ë“¤ ê°ì§€ ë° ì±„ìš°ê¸°
            processed_candles = detector.detect_and_fill_gaps(
                api_candles,
                api_start=chunk.api_request_start or chunk.to,
                api_end=chunk.api_request_end or chunk.end,
                handle_front_open_empty_candle=handle_front_open_empty_candle,  # TEST01: ì•ì´ ì—´ë¦° ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í—ˆìš©
                is_first_chunk=is_first_chunk
            )

            # ê²°ê³¼ ë¡œê¹…
            if len(processed_candles) != len(api_candles):
                empty_count = len(processed_candles) - len(api_candles)
                logger.info(f"ë¹ˆ ìº”ë“¤ ì±„ì›€: {len(api_candles)}ê°œ + {empty_count}ê°œ = {len(processed_candles)}ê°œ")
            return processed_candles

        except Exception as e:
            logger.warning(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            # í´ë°±: ì›ë³¸ ë°˜í™˜
            return api_candles

    def _create_success_result(
        self,
        chunks: List[ChunkInfo],
        request_info: RequestInfo
    ) -> CollectionResult:
        """ì„±ê³µ ê²°ê³¼ ìƒì„±"""
        request_start, request_end = self._calculate_request_bounds(request_info, chunks)

        return CollectionResult(
            success=True,
            request_start_time=request_start,
            request_end_time=request_end
        )

    def _create_error_result(
        self,
        error: Exception
    ) -> CollectionResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return CollectionResult(
            success=False,
            request_start_time=None,
            request_end_time=None,
            error=error
        )

    def _calculate_request_bounds(
        self,
        request_info: RequestInfo,
        chunks: List[ChunkInfo]
    ) -> tuple[Optional[datetime], Optional[datetime]]:
        """ìš”ì²­ íƒ€ì…ì— ë”°ë¥¸ ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚°"""
        if not chunks:
            return None, None

        request_type = request_info.get_request_type()
        timeframe = request_info.timeframe

        if request_type in (RequestType.COUNT_ONLY, RequestType.END_ONLY):
            first_chunk = chunks[0]
            start_time = first_chunk.api_response_start
            if start_time is None:
                aligned_to = request_info.get_aligned_to_time()
                if aligned_to:
                    start_time = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)
            if start_time is None:
                return None, None

            expected = request_info.get_expected_count()

            if expected <= 1:
                end_time = start_time
            else:
                end_time = TimeUtils.get_time_by_ticks(start_time, timeframe, -(expected - 1))
            return start_time, end_time

        aligned_to = request_info.get_aligned_to_time()
        start_time = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1) if aligned_to else None
        end_time = request_info.get_aligned_end_time()

        return start_time, end_time
