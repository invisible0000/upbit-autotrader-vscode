"""
ğŸ“‹ ChunkProcessor v2.0 - Legacy ë¡œì§ ì™„ì „ ë³´ì¡´ ë²„ì „

Created: 2025-09-23
Purpose: candle_data_provider_original.pyì˜ ê²€ì¦ëœ ë¡œì§ì„ 100% ë³´ì¡´í•˜ë©´ì„œ ë…ë¦½ì  ì‚¬ìš© ì§€ì›
Features: Legacy ë©”ì„œë“œ ì™„ì „ ì´ì‹, ë…ë¦½ì  ì‚¬ìš©, Progress Callback, UI ì—°ë™
Architecture: DDD Infrastructure ê³„ì¸µ, ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´

í•µì‹¬ ì„¤ê³„ ì›ì¹™:
1. Legacy First: ê¸°ì¡´ ì˜ ë™ì‘í•˜ë˜ ë¡œì§ì„ 100% ë³´ì¡´
2. Minimal Change: êµ¬ì¡° ë³€ê²½ë§Œ í•˜ê³  ë¡œì§ì€ ê·¸ëŒ€ë¡œ
3. Single Responsibility: ChunkProcessorëŠ” ì²­í¬ ì²˜ë¦¬ë§Œ ë‹´ë‹¹
4. Clean Interface: ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ê¹”ë”í•œ API ì œê³µ
"""

import time
from datetime import datetime, timezone
from typing import Optional, Dict, Any, Callable, List

# Infrastructure ë¡œê¹…
from upbit_auto_trading.infrastructure.logging import create_component_logger

# ê¸°ì¡´ ëª¨ë¸ë“¤ (ê·¸ëŒ€ë¡œ í™œìš©)
from upbit_auto_trading.infrastructure.market_data.candle.models import (
    ChunkInfo,
    CollectionState,
    RequestInfo,
    RequestType,
    OverlapStatus,
    OverlapRequest,
    OverlapResult,
    # ChunkProcessor ì „ìš© ëª¨ë¸ë“¤
    CollectionResult,
    InternalCollectionState,
    ProgressCallback,
    create_success_collection_result,
    create_error_collection_result,
    create_collection_progress
)

# ì˜ì¡´ì„± (Infrastructure ê³„ì¸µ)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)
from upbit_auto_trading.infrastructure.market_data.candle.empty_candle_detector import (
    EmptyCandleDetector
)
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils

# Legacy í˜¸í™˜ìš© ChunkResult (ì„ì‹œ êµ¬í˜„)
from dataclasses import dataclass
from typing import List


@dataclass
class ChunkResult:
    """ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ (Legacy í˜¸í™˜ìš©)"""
    success: bool
    chunk_id: str
    saved_count: int
    processing_time_ms: float
    phases_completed: List[str] = None
    metadata: Optional[Dict[str, Any]] = None
    error: Optional[Exception] = None


def create_success_result(storage_result, chunk_id: str, processing_time_ms: float) -> ChunkResult:
    """ì„±ê³µ ê²°ê³¼ ìƒì„±"""
    return ChunkResult(
        success=True,
        chunk_id=chunk_id,
        saved_count=getattr(storage_result, 'saved_count', 0),
        processing_time_ms=processing_time_ms,
        phases_completed=["completed"],
        metadata={}
    )


def create_error_result(error: Exception, chunk_id: str, processing_time_ms: float) -> ChunkResult:
    """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
    return ChunkResult(
        success=False,
        chunk_id=chunk_id,
        saved_count=0,
        processing_time_ms=processing_time_ms,
        error=error,
        phases_completed=[],
        metadata={}
    )


logger = create_component_logger("ChunkProcessor")


class ChunkProcessor:
    """
    ChunkProcessor v2.0 - Legacy ë¡œì§ ì™„ì „ ë³´ì¡´ + ë…ë¦½ì  ì‚¬ìš© ì§€ì›

    í•µì‹¬ íŠ¹ì§•:
    1. Legacy ë©”ì„œë“œ 100% ì´ì‹: candle_data_provider_original.py ë¡œì§ ê·¸ëŒ€ë¡œ
    2. ì´ì¤‘ ì¸í„°í˜ì´ìŠ¤:
       - execute_full_collection(): ë…ë¦½ì  ì‚¬ìš© (ì½”ì¸ ìŠ¤í¬ë¦¬ë„ˆ ë“±)
       - execute_single_chunk(): CandleDataProvider ì—°ë™ìš©
    3. Progress Callback: ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© ë³´ê³ 
    4. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: Legacy ìˆ˜ì¤€ì˜ 90% ë©”ëª¨ë¦¬ ì ˆì•½ ìœ ì§€

    Legacy ì´ì‹ ë§µí•‘:
    - _process_chunk_direct_storage()     â†’ _process_current_chunk()
    - _handle_overlap_direct_storage()    â†’ _handle_overlap()
    - _fetch_chunk_from_api()            â†’ _fetch_from_api()
    - _analyze_chunk_overlap()           â†’ _analyze_overlap()
    - _process_api_candles_with_empty_filling() â†’ _process_empty_candles()
    - plan_collection()                  â†’ _plan_collection()
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        empty_candle_detector_factory: Callable[[str, str], EmptyCandleDetector],
        chunk_size: int = 200,
        enable_empty_candle_processing: bool = True
    ):
        """
        ChunkProcessor v2.0 ì´ˆê¸°í™”

        Args:
            repository: ìº”ë“¤ ë°ì´í„° ì €ì¥ì†Œ
            upbit_client: ì—…ë¹„íŠ¸ API í´ë¼ì´ì–¸íŠ¸
            overlap_analyzer: ê²¹ì¹¨ ë¶„ì„ê¸°
            empty_candle_detector_factory: ë¹ˆ ìº”ë“¤ ê°ì§€ê¸° íŒ©í† ë¦¬
            chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ 200, ì—…ë¹„íŠ¸ ì œí•œ)
            enable_empty_candle_processing: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
        """
        # ì˜ì¡´ì„± ì£¼ì…
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer
        self.empty_candle_detector_factory = empty_candle_detector_factory

        # ì„¤ì •
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜
        self.enable_empty_candle_processing = enable_empty_candle_processing

        # Legacy í˜¸í™˜ ì„¤ì •
        self.api_rate_limit_rps = 10  # 10 RPS ê¸°ì¤€

        logger.info("ChunkProcessor v2.0 ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ì²­í¬ í¬ê¸°: {self.chunk_size}, "
                    f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: {'í™œì„±í™”' if enable_empty_candle_processing else 'ë¹„í™œì„±í™”'}, "
                    f"API Rate Limit: {self.api_rate_limit_rps} RPS")

    # =========================================================================
    # ğŸš€ ë…ë¦½ì  ì‚¬ìš©ìš© ë©”ì¸ API
    # =========================================================================

    async def execute_full_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None,
        progress_callback: Optional[ProgressCallback] = None,
        dry_run: bool = False
    ) -> CollectionResult:
        """
        ğŸš€ ì™„ì „ ë…ë¦½ì  ìº”ë“¤ ìˆ˜ì§‘ (ì½”ì¸ ìŠ¤í¬ë¦¬ë„ˆ ë“±ì—ì„œ ì‚¬ìš©)

        Legacy plan_collection + mark_chunk_completed ë¡œì§ì„ ì™„ì „ í†µí•©.
        CandleDataProvider ì—†ì´ë„ ì™„ì „í•œ ìº”ë“¤ ìˆ˜ì§‘ì´ ê°€ëŠ¥.

        Args:
            symbol: ê±°ë˜ ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m', '5m', '1d')
            count: ìˆ˜ì§‘í•  ìº”ë“¤ ê°œìˆ˜
            to: ì‹œì‘ ì‹œì  (ìµœì‹  ìº”ë“¤ ê¸°ì¤€)
            end: ì¢…ë£Œ ì‹œì  (ê³¼ê±° ìº”ë“¤ ê¸°ì¤€)
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
            dry_run: ê±´ì‹ ì‹¤í–‰ (ì‹¤ì œ ì €ì¥í•˜ì§€ ì•ŠìŒ)

        Returns:
            CollectionResult: ì „ì²´ ìˆ˜ì§‘ ê²°ê³¼

        Raises:
            ValueError: ì˜ëª»ëœ íŒŒë¼ë¯¸í„°
            Exception: ìˆ˜ì§‘ ê³¼ì • ì˜¤ë¥˜
        """
        start_time = time.time()

        logger.info(f"ë…ë¦½ì  ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘: {symbol} {timeframe}")
        if count:
            logger.info(f"ê°œìˆ˜: {count:,}ê°œ")
        if to:
            logger.info(f"ì‹œì‘: {to}")
        if end:
            logger.info(f"ì¢…ë£Œ: {end}")
        if dry_run:
            logger.info("ğŸ”„ DRY-RUN ëª¨ë“œ: ì‹¤ì œ ì €ì¥í•˜ì§€ ì•ŠìŒ")

        try:
            # 1. RequestInfo ìƒì„± ë° ê²€ì¦ (Legacy ë¡œì§)
            request_info = self._create_request_info(symbol, timeframe, count, to, end)

            # 2. ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½ (Legacy plan_collection ì´ì‹)
            collection_plan = self._plan_collection(request_info)

            # 3. ë‚´ë¶€ ìˆ˜ì§‘ ìƒíƒœ ìƒì„±
            collection_state = self._create_internal_collection_state(
                request_info, collection_plan
            )

            # 4. ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬ (Legacy mark_chunk_completed ë¡œì§)
            while not collection_state.should_complete():
                # Progress ë³´ê³ 
                if progress_callback:
                    progress = create_collection_progress(
                        collection_state,
                        current_status="processing",
                        last_chunk_info=self._get_last_chunk_info(collection_state)
                    )
                    progress_callback(progress)

                # í˜„ì¬ ì²­í¬ ì²˜ë¦¬ (Legacy _process_chunk_direct_storage)
                chunk_result = await self._process_current_chunk(collection_state, dry_run)

                # ìƒíƒœ ì—…ë°ì´íŠ¸ (Legacy ë¡œì§)
                self._update_collection_state(collection_state, chunk_result)

                # ì™„ë£Œ í™•ì¸ ë° ë‹¤ìŒ ì²­í¬ ì¤€ë¹„ (Legacy ë¡œì§)
                if collection_state.should_complete():
                    break

                # ë‹¤ìŒ ì²­í¬ ì¤€ë¹„ (Legacy _prepare_next_chunk)
                self._prepare_next_chunk(collection_state)

            # 5. ìµœì¢… Progress ë³´ê³ 
            if progress_callback:
                final_progress = create_collection_progress(
                    collection_state,
                    current_status="completed",
                    last_chunk_info="ìˆ˜ì§‘ ì™„ë£Œ"
                )
                progress_callback(final_progress)

            # 6. ìµœì¢… ê²°ê³¼ ìƒì„±
            processing_time = time.time() - start_time
            return self._create_success_result(collection_state, processing_time)

        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"ë…ë¦½ì  ìº”ë“¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self._create_error_result(e, processing_time, collection_state)

    # =========================================================================
    # ğŸ”— CandleDataProvider ì—°ë™ìš© API
    # =========================================================================

    async def execute_single_chunk(
        self,
        chunk_info: ChunkInfo,
        collection_state: CollectionState
    ) -> ChunkResult:
        """
        CandleDataProvider.mark_chunk_completed()ì—ì„œ ì‚¬ìš©
        ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ ì™„ì „ í˜¸í™˜

        Args:
            chunk_info: ì²˜ë¦¬í•  ì²­í¬ ì •ë³´
            collection_state: ì™¸ë¶€ ìˆ˜ì§‘ ìƒíƒœ

        Returns:
            ChunkResult: ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ (Legacy í˜¸í™˜)
        """
        logger.debug(f"ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬: {chunk_info.chunk_id}")

        try:
            # ë‚´ë¶€ ìƒíƒœë¡œ ë³€í™˜
            internal_state = self._convert_to_internal_state(collection_state)
            internal_state.current_chunk = chunk_info

            # ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬ (Legacy ë¡œì§ í™œìš©)
            result = await self._process_current_chunk(internal_state, dry_run=False)

            # ì™¸ë¶€ ìƒíƒœ ì—…ë°ì´íŠ¸
            self._update_external_state(collection_state, internal_state, result)

            return result

        except Exception as e:
            logger.error(f"ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            return create_error_result(
                error=e,
                chunk_id=chunk_info.chunk_id,
                processing_time_ms=0.0
            )

    # =========================================================================
    # ğŸ—ï¸ Legacy ë©”ì„œë“œë“¤ ì™„ì „ ì´ì‹ (Phase 1.2ì—ì„œ êµ¬í˜„ ì˜ˆì •)
    # =========================================================================

    def _create_request_info(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int],
        to: Optional[datetime],
        end: Optional[datetime]
    ) -> RequestInfo:
        """RequestInfo ìƒì„± ë° UTC ì •ê·œí™”"""
        # UTC í†µì¼ (Legacy ë¡œì§)
        normalized_to = TimeUtils.normalize_datetime_to_utc(to) if to else None
        normalized_end = TimeUtils.normalize_datetime_to_utc(end) if end else None

        return RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=normalized_to,
            end=normalized_end
        )

    def _plan_collection(self, request_info: RequestInfo) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½ - Legacy plan_collection ë¡œì§ ì™„ì „ ì´ì‹

        RequestInfoì˜ ì‚¬ì „ ê³„ì‚°ëœ ê°’ë“¤ì„ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ì¸ ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½.
        ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ê³¼ ìš”ì²­ íƒ€ì…ë³„ ì²« ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± í¬í•¨.

        Returns:
            Dict: ìˆ˜ì§‘ ê³„íš ì •ë³´
        """
        logger.info(f"ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½: {request_info.to_internal_log_string()}")

        # ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ - ìš”ì²­ ì‹œì  ê¸°ì¤€ (Legacy ë¡œì§)
        if request_info.to is not None and request_info.to > request_info.request_at:
            raise ValueError(f"to ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {request_info.to}")
        if request_info.end is not None and request_info.end > request_info.request_at:
            raise ValueError(f"end ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {request_info.end}")

        # ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° (ì‚¬ì „ ê³„ì‚°ëœ ê°’ ì‚¬ìš©)
        total_count = request_info.get_expected_count()

        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚°
        estimated_chunks = (total_count + self.chunk_size - 1) // self.chunk_size

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (10 RPS ê¸°ì¤€)
        estimated_duration_seconds = estimated_chunks / self.api_rate_limit_rps

        # ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± (Legacy ë¡œì§)
        first_chunk_params = self._create_first_chunk_params_by_type(request_info)

        plan = {
            'total_count': total_count,
            'estimated_chunks': estimated_chunks,
            'estimated_duration_seconds': estimated_duration_seconds,
            'first_chunk_params': first_chunk_params
        }

        logger.info(f"ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {total_count:,}ê°œ ìº”ë“¤, {estimated_chunks}ì²­í¬, "
                    f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {estimated_duration_seconds:.1f}ì´ˆ")

        return plan

    def _create_first_chunk_params_by_type(self, request_info: RequestInfo) -> Dict[str, Any]:
        """ìš”ì²­ íƒ€ì…ë³„ ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± - Legacy ë¡œì§"""
        request_type = request_info.get_request_type()
        params = {"market": request_info.symbol}

        if request_type == RequestType.COUNT_ONLY:
            # COUNT_ONLY: to íŒŒë¼ë¯¸í„° ì—†ì´ countë§Œ ì‚¬ìš©
            chunk_size = min(request_info.count, self.chunk_size)
            params["count"] = chunk_size
            logger.debug(f"COUNT_ONLY: count={chunk_size} (ì„œë²„ ìµœì‹ ë¶€í„°)")

        elif request_type == RequestType.TO_COUNT:
            # to + count: ì‚¬ì „ ê³„ì‚°ëœ ì •ë ¬ ì‹œê°„ ì‚¬ìš©
            chunk_size = min(request_info.count, self.chunk_size)
            aligned_to = request_info.get_aligned_to_time()

            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            first_chunk_start_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

            params["count"] = chunk_size
            params["to"] = first_chunk_start_time
            logger.debug(f"TO_COUNT: ì§„ì…ì ë³´ì • {aligned_to} â†’ {first_chunk_start_time}")

        elif request_type == RequestType.TO_END:
            # to + end: ì‚¬ì „ ê³„ì‚°ëœ ì •ë ¬ ì‹œê°„ ì‚¬ìš©
            aligned_to = request_info.get_aligned_to_time()

            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            first_chunk_start_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

            params["count"] = self.chunk_size
            params["to"] = first_chunk_start_time
            logger.debug(f"TO_END: ì§„ì…ì ë³´ì • {aligned_to} â†’ {first_chunk_start_time}")

        elif request_type == RequestType.END_ONLY:
            # END_ONLY: COUNT_ONLYì²˜ëŸ¼ to ì—†ì´ countë§Œ ì‚¬ìš©
            params["count"] = self.chunk_size
            logger.debug(f"END_ONLY: count={self.chunk_size} (ì„œë²„ ìµœì‹ ë¶€í„°)")

        return params

    def _create_internal_collection_state(
        self,
        request_info: RequestInfo,
        plan: Dict[str, Any]
    ) -> InternalCollectionState:
        """ë‚´ë¶€ ìˆ˜ì§‘ ìƒíƒœ ìƒì„± - Legacy ë¡œì§ ì™„ì „ ì´ì‹"""
        collection_state = InternalCollectionState(
            request_info=request_info,
            symbol=request_info.symbol,
            timeframe=request_info.timeframe,
            total_requested=plan['total_count'],
            estimated_total_chunks=plan['estimated_chunks']
        )

        # ì²« ë²ˆì§¸ ì²­í¬ ìƒì„± (Legacy ë¡œì§)
        first_chunk = self._create_next_chunk(
            collection_state=collection_state,
            chunk_params=plan['first_chunk_params'],
            chunk_index=0
        )
        collection_state.current_chunk = first_chunk

        logger.debug(f"ë‚´ë¶€ ìˆ˜ì§‘ ìƒíƒœ ìƒì„±: {plan['total_count']:,}ê°œ ìº”ë“¤, "
                     f"{plan['estimated_chunks']}ì²­í¬ ì˜ˆìƒ, ì²« ì²­í¬: {first_chunk.chunk_id}")

        return collection_state

    async def _process_current_chunk(
        self,
        collection_state: InternalCollectionState,
        dry_run: bool = False
    ) -> ChunkResult:
        """ì„±ëŠ¥ ìµœì í™”ëœ ì²­í¬ ì²˜ë¦¬ - Legacy _process_chunk_direct_storage ì™„ì „ ì´ì‹

        Returns:
            ChunkResult: ì²­í¬ ì²˜ë¦¬ ê²°ê³¼
        """
        chunk_info = collection_state.current_chunk
        if not chunk_info:
            raise ValueError("ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ğŸš€ ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ê³„ì‚° (ì²« ì²­í¬ ~ í˜„ì¬ ì²­í¬) - Legacy ë¡œì§
        safe_range_start = None
        safe_range_end = None
        if collection_state.completed_chunks and chunk_info.end:
            # ì²« ë²ˆì§¸ ì™„ë£Œëœ ì²­í¬ì˜ ì‹œì‘ì 
            safe_range_start = collection_state.completed_chunks[0].to
            # í˜„ì¬ ì²­í¬ì˜ ëì 
            safe_range_end = chunk_info.end
            logger.debug(f"ğŸ”’ ì•ˆì „ ë²”ìœ„ ê³„ì‚°: [{safe_range_start}, {safe_range_end}]")

        # ìš”ì²­ íƒ€ì… ê¸°ë°˜ ìµœì í™” - Legacy ë¡œì§
        is_first_chunk = len(collection_state.completed_chunks) == 0
        request_type = collection_state.request_info.get_request_type()

        logger.info(f"ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {chunk_info.chunk_id} [{request_type.value}]")

        try:
            # ì„±ëŠ¥ ìµœì í™”ëœ ì²­í¬ ì²˜ë¦¬ (ChunkInfo ê¸°ë°˜)
            saved_count, _ = await self._process_chunk_direct_storage(
                chunk_info, collection_state, is_first_chunk, request_type, dry_run
            )

            # ì„±ê³µ ê²°ê³¼ ìƒì„±
            return create_success_result(
                storage_result=type('StorageResult', (), {
                    'saved_count': saved_count,
                    'expected_count': chunk_info.count,
                    'storage_time': datetime.now(timezone.utc),
                    'validation_passed': True,
                    'metadata': {}
                })(),
                chunk_id=chunk_info.chunk_id,
                processing_time_ms=100.0
            )

        except Exception as e:
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            return create_error_result(
                error=e,
                chunk_id=chunk_info.chunk_id,
                processing_time_ms=0.0
            )

    async def _process_chunk_direct_storage(
        self,
        chunk_info: ChunkInfo,
        collection_state: InternalCollectionState,
        is_first_chunk: bool,
        request_type: RequestType,
        dry_run: bool = False
    ) -> tuple[int, Optional[str]]:
        """
        ì„±ëŠ¥ ìµœì í™”ëœ ì²­í¬ ì²˜ë¦¬ - ì§ì ‘ ì €ì¥ ë°©ì‹ (Legacy ì™„ì „ ì´ì‹)

        Legacy _process_chunk_direct_storage ë¡œì§ 100% ë³´ì¡´:
        - ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ê³„ì‚°
        - ê²¹ì¹¨ ë¶„ì„ ë° ìµœì í™”
        - ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
        - ë¹ˆ ìº”ë“¤ ì²˜ë¦¬

        Returns:
            tuple[int, Optional[str]]: (saved_count, last_candle_time_str)
        """

        # ğŸš€ ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ê³„ì‚° (ì²« ì²­í¬ ~ í˜„ì¬ ì²­í¬)
        safe_range_start = None
        safe_range_end = None
        if collection_state.completed_chunks and chunk_info.end:
            # ì²« ë²ˆì§¸ ì™„ë£Œëœ ì²­í¬ì˜ ì‹œì‘ì 
            safe_range_start = collection_state.completed_chunks[0].to
            # í˜„ì¬ ì²­í¬ì˜ ëì 
            safe_range_end = chunk_info.end
            logger.debug(f"ğŸ”’ ì•ˆì „ ë²”ìœ„ ê³„ì‚°: [{safe_range_start}, {safe_range_end}]")

        # ê²¹ì¹¨ ë¶„ì„ (API ì ˆì•½ íš¨ê³¼ ìœ ì§€) - Legacy ë¡œì§
        overlap_result = None
        chunk_end = None
        if not (is_first_chunk and request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]):
            chunk_start = chunk_info.to
            chunk_end = self._calculate_chunk_end_time(chunk_info)

            # ğŸ” ë””ë²„ê¹…: ê²¹ì¹¨ ë¶„ì„ì—ì„œì˜ chunk_end
            logger.debug(f"ğŸ” ê²¹ì¹¨ ë¶„ì„ chunk_end: {chunk_end}")

            overlap_result = await self._analyze_overlap(
                collection_state.symbol, collection_state.timeframe, chunk_start, chunk_end
            )

        if overlap_result and hasattr(overlap_result, 'status'):
            # ğŸŸ¢ ê°œì„ : ChunkInfoì— overlap ì •ë³´ ì €ì¥ (í†µí•© ê´€ë¦¬)
            chunk_info.set_overlap_info(overlap_result)

            # ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì§ì ‘ ì €ì¥ (ChunkInfo ê¸°ë°˜ìœ¼ë¡œ last_candle_time ë¶ˆí•„ìš”)
            saved_count, last_candle_time = await self._handle_overlap_direct_storage(
                chunk_info, overlap_result, collection_state, chunk_end, is_first_chunk,
                safe_range_start, safe_range_end, dry_run
            )
        else:
            # í´ë°±: ì§ì ‘ API â†’ ì €ì¥ (COUNT_ONLY/END_ONLY ì²« ì²­í¬ í¬í•¨)
            api_response = await self._fetch_from_api(chunk_info)

            # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
            api_count, _ = chunk_info.get_api_params()
            if len(api_response) < api_count:
                collection_state.reached_upbit_data_end = True
                logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ (í´ë°±): {chunk_info.symbol} {chunk_info.timeframe} - "
                               f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

            # ğŸš€ ì²« ì²­í¬ì—ì„œë„ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í—ˆìš© (EmptyCandleDetector ë‚´ë¶€ ì•ˆì „ ì²˜ë¦¬ ë¡œì§ ì ìš©)
            if is_first_chunk:
                logger.debug("ì²« ì²­í¬: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì ìš©")
                # ì²« ì²­í¬ë¥¼ ìœ„í•œ ì•ˆì „ ë²”ìœ„ ì„¤ì •
                first_chunk_safe_start = chunk_info.to  # ì²­í¬ ì‹œì‘ì 
                first_chunk_safe_end = chunk_info.end   # ì²­í¬ ëì 

                final_candles = await self._process_empty_candles(
                    api_response,
                    collection_state.symbol,
                    collection_state.timeframe,
                    api_start=chunk_info.to,
                    api_end=chunk_info.end,
                    safe_range_start=first_chunk_safe_start,
                    safe_range_end=first_chunk_safe_end,
                    is_first_chunk=True  # ğŸš€ ì²« ì²­í¬ì„ì„ ëª…ì‹œ (api_start +1í‹± ì¶”ê°€ ë°©ì§€)
                )
                # ğŸ†• ìµœì¢… ìº”ë“¤ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì •
                chunk_info.set_final_candle_info(final_candles)
                logger.info(f"ì²« ì²­í¬ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì™„ë£Œ: {len(api_response)}ê°œ â†’ {len(final_candles)}ê°œ")
            else:
                logger.debug("í´ë°± ì¼€ì´ìŠ¤: api_end ì •ë³´ ì—†ìŒ â†’ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°")
                final_candles = api_response
                # ğŸ†• ìµœì¢… ìº”ë“¤ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì • (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì—†ì´)
                chunk_info.set_final_candle_info(final_candles)

            if not dry_run:
                saved_count = await self.repository.save_raw_api_data(
                    collection_state.symbol, collection_state.timeframe, final_candles
                )
            else:
                saved_count = len(final_candles)
                logger.info(f"ğŸ”„ DRY-RUN: ì €ì¥ ì‹œë®¬ë ˆì´ì…˜ {saved_count}ê°œ")

            # âœ… ChunkInfoì—ì„œ ì‹œê°„ ì •ë³´ ìë™ ì¶”ì¶œ (get_effective_end_time í™œìš©)
            last_candle_time = None  # ChunkInfoì—ì„œ ì²˜ë¦¬ë¨

        return saved_count, last_candle_time

    # =========================================================================
    # ğŸ› ï¸ í—¬í¼ ë©”ì„œë“œë“¤ - Legacy ë¡œì§ ì´ì‹
    # =========================================================================

    def _calculate_chunk_end_time(self, chunk_info: ChunkInfo) -> datetime:
        """ì²­í¬ ìš”ì²­ì˜ ì˜ˆìƒ ì¢…ë£Œ ì‹œì  ê³„ì‚° - Legacy ë¡œì§"""
        ticks = -(chunk_info.count - 1)
        end_time = TimeUtils.get_time_by_ticks(chunk_info.to, chunk_info.timeframe, ticks)

        # ğŸ” ë””ë²„ê¹…: ì²­í¬ ê²½ê³„ ê³„ì‚° ê³¼ì • ë¡œê¹…
        logger.debug(f"ğŸ” ì²­í¬ ê²½ê³„ ê³„ì‚°: to={chunk_info.to}, count={chunk_info.count}, "
                     f"ticks={ticks}, calculated_end={end_time}")

        return end_time

    async def _analyze_overlap(
        self,
        symbol: str,
        timeframe: str,
        start_time: datetime,
        end_time: datetime
    ):
        """OverlapAnalyzerë¥¼ í™œìš©í•œ ì²­í¬ ê²¹ì¹¨ ë¶„ì„ - Legacy ë¡œì§"""
        logger.debug(f"ê²¹ì¹¨ ë¶„ì„: {symbol} {timeframe}")

        try:
            # ğŸš€ UTC í†µì¼: ì§„ì…ì ì—ì„œ ì •ê·œí™”ë˜ì–´ ë” ì´ìƒ ê²€ì¦ ë¶ˆí•„ìš”
            safe_start_time = start_time
            safe_end_time = end_time

            # ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
            expected_count = self._calculate_api_count(safe_start_time, safe_end_time, timeframe)

            overlap_request = OverlapRequest(
                symbol=symbol,
                timeframe=timeframe,
                target_start=safe_start_time,
                target_end=safe_end_time,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)
            logger.debug(f"ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼: {overlap_result.status.value}")

            return overlap_result

        except Exception as e:
            logger.warning(f"ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def _calculate_api_count(self, start_time: datetime, end_time: datetime, timeframe: str) -> int:
        """API ìš”ì²­ì— í•„ìš”í•œ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° - Legacy ë¡œì§"""
        return TimeUtils.calculate_expected_count(start_time, end_time, timeframe)

    async def _handle_overlap_direct_storage(
        self,
        chunk_info: ChunkInfo,
        overlap_result,
        collection_state: InternalCollectionState,
        calculated_chunk_end: Optional[datetime] = None,
        is_first_chunk: bool = False,
        safe_range_start: Optional[datetime] = None,
        safe_range_end: Optional[datetime] = None,
        dry_run: bool = False
    ) -> tuple[int, Optional[str]]:
        """
        ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì§ì ‘ ì €ì¥ ì²˜ë¦¬ - Legacy ì™„ì „ ì´ì‹

        OverlapStatusë³„ ì„¸ë°€í•œ ì²˜ë¦¬ë¡œ API í˜¸ì¶œ ìµœì í™”ì™€ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥.
        Legacy _handle_overlap_direct_storage ë¡œì§ 100% ë³´ì¡´.

        Args:
            chunk_info: ì²­í¬ ì •ë³´
            overlap_result: ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼
            calculated_chunk_end: ê³„ì‚°ëœ ì²­í¬ ì¢…ë£Œ ì‹œê°„
            is_first_chunk: ì²« ë²ˆì§¸ ì²­í¬ ì—¬ë¶€
            safe_range_start: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ì‹œì‘
            safe_range_end: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ë
            dry_run: ê±´ì‹ ì‹¤í–‰ ì—¬ë¶€

        Returns:
            tuple[int, Optional[str]]: (saved_count, last_candle_time_str)
        """
        status = overlap_result.status

        if status == OverlapStatus.COMPLETE_OVERLAP:
            # ì™„ì „ ê²¹ì¹¨: ì €ì¥í•  ê²ƒ ì—†ìŒ (ì´ë¯¸ DBì— ì¡´ì¬) - Legacy ë¡œì§
            logger.debug("ì™„ì „ ê²¹ì¹¨ â†’ ì €ì¥ ìƒëµ")

            # ChunkInfo API ìš”ì²­ ì •ë³´ ì„¤ì • (ì™„ì „ ê²¹ì¹¨ì´ë¯€ë¡œ 0ê°œ)
            chunk_info.api_request_count = 0
            chunk_info.api_request_start = None
            chunk_info.api_request_end = None

            # ChunkInfo API ì‘ë‹µ ì •ë³´ ì„¤ì • (API í˜¸ì¶œ ì—†ìŒ)
            chunk_info.set_api_response_info([])

            # ChunkInfo ìµœì¢… ìº”ë“¤ ì •ë³´ ì„¤ì • (ì²˜ë¦¬ ì—†ìŒ)
            chunk_info.set_final_candle_info([])

            # ğŸ”„ ChunkInfoì—ì„œ ìë™ ì²˜ë¦¬: calculated_chunk_endë¥¼ final_candle_endë¡œ ì„¤ì •
            if calculated_chunk_end:
                chunk_info.final_candle_end = calculated_chunk_end

            # ê²¹ì¹¨ ìµœì í™” ê¸°ë¡
            collection_state.mark_overlap_optimization()

            saved_count = 0
            last_candle_time_str = None

        elif status == OverlapStatus.NO_OVERLAP:
            # ê²¹ì¹¨ ì—†ìŒ: API â†’ ì§ì ‘ ì €ì¥ - Legacy ë¡œì§
            logger.debug("ê²¹ì¹¨ ì—†ìŒ â†’ ì „ì²´ API ì§ì ‘ ì €ì¥")

            saved_count, last_candle_time_str = await self._fetch_and_store_full_chunk(
                chunk_info, is_first_chunk, safe_range_start, safe_range_end,
                calculated_chunk_end, collection_state, overlap_result, dry_run
            )

        elif status in [OverlapStatus.PARTIAL_START, OverlapStatus.PARTIAL_MIDDLE_CONTINUOUS]:
            # ë¶€ë¶„ ê²¹ì¹¨: API ë¶€ë¶„ë§Œ ì €ì¥ (DB ë¶€ë¶„ì€ ì´ë¯¸ ì¡´ì¬) - Legacy ë¡œì§
            logger.debug(f"ë¶€ë¶„ ê²¹ì¹¨ ({status.value}) â†’ API ë¶€ë¶„ë§Œ ì§ì ‘ ì €ì¥")

            if overlap_result.api_start and overlap_result.api_end:
                # ChunkInfoì— overlap ì •ë³´ ì„¤ì • (temp_chunk ìƒì„± ì œê±°)
                api_count = self._calculate_api_count(
                    overlap_result.api_start,
                    overlap_result.api_end,
                    chunk_info.timeframe
                )

                chunk_info.set_overlap_info(overlap_result, api_count)

                saved_count, last_candle_time_str = await self._fetch_and_store_partial_chunk(
                    chunk_info, overlap_result, is_first_chunk, safe_range_start, safe_range_end,
                    calculated_chunk_end, collection_state, dry_run
                )
            else:
                # API ì •ë³´ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
                saved_count = 0
                last_candle_time_str = TimeUtils.format_datetime_utc(calculated_chunk_end) if calculated_chunk_end else None

        else:
            # PARTIAL_MIDDLE_FRAGMENT ë˜ëŠ” ê¸°íƒ€: ì•ˆì „í•œ í´ë°± â†’ ì „ì²´ API ì €ì¥ - Legacy ë¡œì§
            logger.debug("ë³µì¡í•œ ê²¹ì¹¨ â†’ ì „ì²´ API ì§ì ‘ ì €ì¥ í´ë°±")

            saved_count, last_candle_time_str = await self._fetch_and_store_full_chunk(
                chunk_info, is_first_chunk, safe_range_start, safe_range_end,
                calculated_chunk_end, collection_state, None, dry_run, fallback_mode=True
            )

        return saved_count, last_candle_time_str

    async def _fetch_and_store_full_chunk(
        self,
        chunk_info: ChunkInfo,
        is_first_chunk: bool,
        safe_range_start: Optional[datetime],
        safe_range_end: Optional[datetime],
        calculated_chunk_end: Optional[datetime],
        collection_state: InternalCollectionState,
        overlap_result,
        dry_run: bool,
        fallback_mode: bool = False
    ) -> tuple[int, Optional[str]]:
        """ì „ì²´ ì²­í¬ API í˜¸ì¶œ ë° ì €ì¥ - Legacy ë¡œì§"""
        # API í˜¸ì¶œ
        api_response = await self._fetch_from_api(chunk_info)

        # API í˜¸ì¶œ ê¸°ë¡
        collection_state.mark_api_call()

        # ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
        api_count, _ = chunk_info.get_api_params()
        if len(api_response) < api_count:
            collection_state.reached_upbit_data_end = True
            warning_suffix = " (í´ë°±)" if fallback_mode else ""
            logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬{warning_suffix}: {chunk_info.symbol} {chunk_info.timeframe} - "
                           f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

        # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
        if is_first_chunk or (overlap_result and hasattr(overlap_result, 'api_start') and hasattr(overlap_result, 'api_end')):
            # ì²« ì²­í¬ ë˜ëŠ” NO_OVERLAPì¼ ë•Œ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
            api_start = overlap_result.api_start if overlap_result and hasattr(overlap_result, 'api_start') else chunk_info.to
            api_end = overlap_result.api_end if overlap_result and hasattr(overlap_result, 'api_end') else chunk_info.end

            if self._should_process_empty_candles(api_response, api_end):
                final_candles = await self._process_empty_candles(
                    api_response, chunk_info.symbol, chunk_info.timeframe,
                    api_start, api_end, safe_range_start, safe_range_end, is_first_chunk
                )

                # ë¹ˆ ìº”ë“¤ ì±„ì›€ ê¸°ë¡
                empty_count = len(final_candles) - len(api_response)
                if empty_count > 0:
                    collection_state.mark_empty_candles_filled(empty_count)
            else:
                final_candles = api_response
        else:
            # í´ë°± ì¼€ì´ìŠ¤: api_end ì •ë³´ ì—†ìŒ â†’ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
            logger.debug("í´ë°± ì¼€ì´ìŠ¤: api_end ì •ë³´ ì—†ìŒ â†’ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°")
            final_candles = api_response

        # ChunkInfo ìµœì¢… ìº”ë“¤ ì •ë³´ ì„¤ì •
        chunk_info.set_final_candle_info(final_candles)

        # ì €ì¥ ì²˜ë¦¬
        if not dry_run:
            saved_count = await self.repository.save_raw_api_data(
                chunk_info.symbol, chunk_info.timeframe, final_candles
            )
        else:
            saved_count = len(final_candles)
            logger.info(f"ğŸ”„ DRY-RUN: ì €ì¥ ì‹œë®¬ë ˆì´ì…˜ {saved_count}ê°œ")

        # ChunkInfo ìë™ ì²˜ë¦¬: calculated_chunk_end ì„¤ì •
        if calculated_chunk_end:
            chunk_info.final_candle_end = calculated_chunk_end

        # ì²­í¬ ë ì‹œê°„ ìš°ì„  ì‚¬ìš© (ë¹ˆ ìº”ë“¤ê³¼ ë¬´ê´€í•œ ì—°ì†ì„± ë³´ì¥)
        last_candle_time_str = TimeUtils.format_datetime_utc(calculated_chunk_end) if calculated_chunk_end else None

        return saved_count, last_candle_time_str

    async def _fetch_and_store_partial_chunk(
        self,
        chunk_info: ChunkInfo,
        overlap_result,
        is_first_chunk: bool,
        safe_range_start: Optional[datetime],
        safe_range_end: Optional[datetime],
        calculated_chunk_end: Optional[datetime],
        collection_state: InternalCollectionState,
        dry_run: bool
    ) -> tuple[int, Optional[str]]:
        """ë¶€ë¶„ ê²¹ì¹¨ ì²­í¬ API í˜¸ì¶œ ë° ì €ì¥ - Legacy ë¡œì§"""
        # API í˜¸ì¶œ
        api_response = await self._fetch_from_api(chunk_info)

        # API í˜¸ì¶œ ë° ê²¹ì¹¨ ìµœì í™” ê¸°ë¡
        collection_state.mark_api_call()
        collection_state.mark_overlap_optimization()

        # ë¶€ë¶„ ê²¹ì¹¨ì—ì„œë„ ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
        api_count = chunk_info.api_request_count or chunk_info.count
        if len(api_response) < api_count:
            collection_state.reached_upbit_data_end = True
            logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ (ë¶€ë¶„ê²¹ì¹¨): {chunk_info.symbol} {chunk_info.timeframe} - "
                           f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

        # ì²« ì²­í¬ì—ì„œë„ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í—ˆìš© (PARTIAL_OVERLAP)
        api_start = overlap_result.api_start if hasattr(overlap_result, 'api_start') else None
        api_end = overlap_result.api_end if hasattr(overlap_result, 'api_end') else None

        # ì¡°ê±´ë¶€ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
        if self._should_process_empty_candles(api_response, api_end):
            final_candles = await self._process_empty_candles(
                api_response, chunk_info.symbol, chunk_info.timeframe,
                api_start, api_end, safe_range_start, safe_range_end, is_first_chunk
            )

            # ë¹ˆ ìº”ë“¤ ì±„ì›€ ê¸°ë¡
            empty_count = len(final_candles) - len(api_response)
            if empty_count > 0:
                collection_state.mark_empty_candles_filled(empty_count)
        else:
            final_candles = api_response

        # ChunkInfo ìµœì¢… ìº”ë“¤ ì •ë³´ ì„¤ì •
        chunk_info.set_final_candle_info(final_candles)

        # ì €ì¥ ì²˜ë¦¬
        if not dry_run:
            saved_count = await self.repository.save_raw_api_data(
                chunk_info.symbol, chunk_info.timeframe, final_candles
            )
        else:
            saved_count = len(final_candles)
            logger.info(f"ğŸ”„ DRY-RUN: ì €ì¥ ì‹œë®¬ë ˆì´ì…˜ {saved_count}ê°œ")

        # ì²­í¬ ë ì‹œê°„ ìš°ì„  ì‚¬ìš© (ë¹ˆ ìº”ë“¤ê³¼ ë¬´ê´€í•œ ì—°ì†ì„± ë³´ì¥)
        if calculated_chunk_end:
            last_candle_time_str = TimeUtils.format_datetime_utc(calculated_chunk_end)
        else:
            last_candle_time_str = None  # ChunkInfoì—ì„œ ì²˜ë¦¬ë¨

        return saved_count, last_candle_time_str

    def _should_process_empty_candles(
        self,
        api_response: List[Dict[str, Any]],
        api_end: Optional[datetime]
    ) -> bool:
        """
        API ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ê³¼ api_end ë¹„êµí•˜ì—¬ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨ - Legacy ë¡œì§

        Args:
            api_response: ì—…ë¹„íŠ¸ API ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
            api_end: ì˜ˆìƒë˜ëŠ” ì²­í¬ ì¢…ë£Œ ì‹œê°„

        Returns:
            bool: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        if not api_response or not api_end:
            logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: api_response ë˜ëŠ” api_endê°€ ì—†ìŒ â†’ ì²˜ë¦¬ ì•ˆ í•¨")
            return False

        try:
            # ì—…ë¹„íŠ¸ APIëŠ” ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ ë§ˆì§€ë§‰ ìš”ì†Œê°€ ê°€ì¥ ê³¼ê±° ìº”ë“¤
            last_candle = api_response[-1]
            candle_time_utc = last_candle.get('candle_date_time_utc')

            if candle_time_utc and isinstance(candle_time_utc, str):
                # ğŸš€ UTC í†µì¼: TimeUtilsë¥¼ í†µí•œ í‘œì¤€ ì •ê·œí™” (aware datetime ë³´ì¥)
                parsed_time = datetime.fromisoformat(candle_time_utc.replace('Z', '+00:00'))
                last_candle_time = TimeUtils.normalize_datetime_to_utc(parsed_time)

                # ğŸš€ UTC í†µì¼: ë™ì¼í•œ í˜•ì‹(aware datetime) ê°„ ë¹„êµë¡œ ì •í™•ì„± ë³´ì¥
                needs_processing = last_candle_time != api_end

                if needs_processing:
                    logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time} vs api_end={api_end}")
                else:
                    logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë¶ˆí•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time} == api_end={api_end}")

                return needs_processing

        except Exception as e:
            logger.warning(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨: {e} â†’ ì•ˆì „í•œ í´ë°±ìœ¼ë¡œ ì²˜ë¦¬ ì•ˆ í•¨")
            return False

        logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: ìº”ë“¤ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ â†’ ì²˜ë¦¬ ì•ˆ í•¨")
        return False

    async def _fetch_from_api(self, chunk_info: ChunkInfo) -> List[Dict[str, Any]]:
        """
        ì‹¤ì œ API í˜¸ì¶œì„ í†µí•œ ì²­í¬ ë°ì´í„° ìˆ˜ì§‘ - Legacy ì™„ì „ ì´ì‹

        íƒ€ì„í”„ë ˆì„ë³„ API ë¶„ê¸°ì™€ ì§€ì¶œì  ë³´ì • ë¡œì§ì„ Legacyì—ì„œ 100% ì´ì‹.
        Overlap ìµœì í™”ë¥¼ ì§€ì›í•˜ì—¬ ChunkInfoì˜ ìµœì í™”ëœ API íŒŒë¼ë¯¸í„° í™œìš©.

        Returns:
            List[Dict[str, Any]]: ìº”ë“¤ ë°ì´í„°
        """
        logger.debug(f"API ì²­í¬ ìš”ì²­: {chunk_info.chunk_id}")

        # ğŸŸ¢ ê°œì„ : ChunkInfoì—ì„œ ìµœì í™”ëœ API íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        api_count, api_to = chunk_info.get_api_params()

        if chunk_info.has_overlap_info() and chunk_info.needs_partial_api_call():
            logger.debug(f"ë¶€ë¶„ API í˜¸ì¶œ: count={api_count}, to={api_to} (overlap ìµœì í™”)")
        else:
            logger.debug(f"ì „ì²´ API í˜¸ì¶œ: count={api_count}, to={api_to}")

        try:
            # íƒ€ì„í”„ë ˆì„ë³„ API ë©”ì„œë“œ ì„ íƒ - Legacy ë¡œì§ ì™„ì „ ì´ì‹
            if chunk_info.timeframe == '1s':
                # ì´ˆë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    timeframe_delta = TimeUtils.get_timeframe_delta(chunk_info.timeframe)
                    fetch_time = api_to + timeframe_delta
                    to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_seconds(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe.endswith('m'):
                # ë¶„ë´‰
                unit = int(chunk_info.timeframe[:-1])
                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ë´‰ ë‹¨ìœ„: {unit}")

                # ë¶„ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    timeframe_delta = TimeUtils.get_timeframe_delta(chunk_info.timeframe)
                    fetch_time = api_to + timeframe_delta
                    to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit,
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1d':
                # ì¼ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%d")

                candles = await self.upbit_client.get_candles_days(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1w':
                # ì£¼ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%d")

                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1M':
                # ì›”ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m")

                candles = await self.upbit_client.get_candles_months(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1y':
                # ì—°ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y")

                candles = await self.upbit_client.get_candles_years(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk_info.timeframe}")

            # ğŸ†• API ì‘ë‹µ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì •
            chunk_info.set_api_response_info(candles)

            # ê°œì„ : ìµœì í™”ëœ ë¡œê¹… (overlap ì •ë³´ í‘œì‹œ)
            overlap_info = f" (overlap: {chunk_info.overlap_status.value})" if chunk_info.has_overlap_info() else ""
            logger.info(f"API ì²­í¬ ì™„ë£Œ: {chunk_info.chunk_id}, ìˆ˜ì§‘: {len(candles)}ê°œ{overlap_info}")

            return candles

        except Exception as e:
            logger.error(f"API ì²­í¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    async def _process_empty_candles(
        self,
        api_candles: List[Dict[str, Any]],
        symbol: str,
        timeframe: str,
        api_start: Optional[datetime] = None,
        api_end: Optional[datetime] = None,
        safe_range_start: Optional[datetime] = None,
        safe_range_end: Optional[datetime] = None,
        is_first_chunk: bool = False
    ) -> List[Dict[str, Any]]:
        """
        API ìº”ë“¤ ì‘ë‹µì— ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì ìš© - Legacy ì™„ì „ ì´ì‹

        Legacy _process_api_candles_with_empty_filling ë¡œì§ 100% ë³´ì¡´:
        - EmptyCandleDetector í™œìš©
        - ì²« ì²­í¬ íŠ¹ë³„ ì²˜ë¦¬ (api_start +1í‹± ì¶”ê°€ ë°©ì§€)
        - ì•ˆì „í•œ ë²”ìœ„ ë‚´ì—ì„œë§Œ ë¹ˆ ìº”ë“¤ ê°ì§€ ë° ì±„ì›€

        ì²˜ë¦¬ ìˆœì„œ:
        1. DBì—ì„œ ì°¸ì¡° ìƒíƒœ ì¡°íšŒ (ë¹ˆ ìº”ë“¤ ê·¸ë£¹ ì°¸ì¡°ìš©, ì—†ìœ¼ë©´ UUID ê·¸ë£¹ ìƒì„±)
        2. API ìº”ë“¤ ì‘ë‹µì— ë¹ˆìº”ë“¤ ê²€ì‚¬ (api_start ~ api_end ë²”ìœ„ ë‚´)
        3. ê²€ì¶œëœ Gapì„ ë¹ˆ ìº”ë“¤ë¡œ ì±„ìš°ê¸°
        4. API ìº”ë“¤ ì‘ë‹µê³¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ ì‹œê³„ì—´ ìƒì„±

        Args:
            api_candles: ì—…ë¹„íŠ¸ API ì›ì‹œ ì‘ë‹µ ë°ì´í„°
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„
            api_start: API ê²€ì¶œ ë²”ìœ„ ì‹œì‘ ì‹œê°„ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            api_end: API ê²€ì¶œ ë²”ìœ„ ì¢…ë£Œ ì‹œê°„ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            safe_range_start: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ì‹œì‘ (ì²« ì²­í¬ ì‹œì‘ì )
            safe_range_end: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ë (í˜„ì¬ ì²­í¬ ëì )
            is_first_chunk: ì²« ì²­í¬ ì—¬ë¶€ (api_start +1í‹± ì¶”ê°€ ì œì–´ìš©)

        Returns:
            ì²˜ë¦¬ëœ ìº”ë“¤ ë°ì´í„° (Dict í˜•íƒœ ìœ ì§€)
        """
        if not self.enable_empty_candle_processing:
            return api_candles

        # EmptyCandleDetector ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê¸°
        detector = self._get_empty_candle_detector(symbol, timeframe)

        processed_candles = detector.detect_and_fill_gaps(
            api_candles,
            api_start=api_start,
            api_end=api_end,
            is_first_chunk=is_first_chunk  # ğŸš€ ì²« ì²­í¬ ì •ë³´ ì „ë‹¬ (api_start +1í‹± ì¶”ê°€ ì œì–´)
        )

        # ìº”ë“¤ ìˆ˜ ë³´ì • ë¡œê¹…
        if len(processed_candles) != len(api_candles):
            empty_count = len(processed_candles) - len(api_candles)
            logger.info(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: ì›ë³¸ {len(api_candles)}ê°œ + ë¹ˆ {empty_count}ê°œ = ìµœì¢… {len(processed_candles)}ê°œ")

        return processed_candles

    def _get_empty_candle_detector(self, symbol: str, timeframe: str) -> EmptyCandleDetector:
        """(symbol, timeframe) ì¡°í•©ë³„ EmptyCandleDetector ìºì‹œ - Legacy ë¡œì§"""
        cache_key = f"{symbol}_{timeframe}"
        detector = self.empty_candle_detector_factory(symbol, timeframe)
        logger.debug(f"EmptyCandleDetector ìƒì„±: {symbol} {timeframe}")
        return detector

    def _create_first_chunk_params(self, request_info: RequestInfo) -> Dict[str, Any]:
        """ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± - Legacy ë¡œì§ ì™„ì „ ì´ì‹"""
        # Legacy _create_first_chunk_params_by_type ë¡œì§ ì‚¬ìš©
        return self._create_first_chunk_params_by_type(request_info)

    def _get_last_chunk_info(self, collection_state: InternalCollectionState) -> str:
        """ë§ˆì§€ë§‰ ì²­í¬ ì •ë³´ ë¬¸ìì—´ ìƒì„±"""
        if collection_state.current_chunk:
            return f"ì²˜ë¦¬ ì¤‘: {collection_state.current_chunk.chunk_id}"
        return "ëŒ€ê¸° ì¤‘"

    def _update_collection_state(
        self,
        collection_state: InternalCollectionState,
        chunk_result: ChunkResult
    ) -> None:
        """ìˆ˜ì§‘ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        if chunk_result.success and collection_state.current_chunk:
            collection_state.add_completed_chunk(
                collection_state.current_chunk,
                chunk_result.saved_count
            )

    def _prepare_next_chunk(self, collection_state: InternalCollectionState) -> None:
        """ë‹¤ìŒ ì²­í¬ ì¤€ë¹„ - Legacy ë¡œì§ ì™„ì „ ì´ì‹"""
        next_chunk_index = len(collection_state.completed_chunks)
        remaining_count = collection_state.total_requested - collection_state.total_collected
        next_chunk_size = min(remaining_count, self.chunk_size)

        # ë‹¤ìŒ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± - ì—°ì†ì„± ë³´ì¥ (Legacy ë¡œì§)
        params = {
            "market": collection_state.symbol,
            "count": next_chunk_size
        }

        # ChunkInfo ê¸°ë°˜ ì—°ì†ì„± (ëª¨ë“  ì²­í¬ íƒ€ì…ì—ì„œ ì™„ì „í•œ ì‹œê°„ ì •ë³´ ì§€ì›)
        last_effective_time = collection_state.get_last_effective_time_datetime()
        if last_effective_time:
            try:
                # ë‹¤ìŒ ì²­í¬ ì‹œì‘ = ì´ì „ ì²­í¬ ìœ íš¨ ëì‹œê°„ - 1í‹± (ì—°ì†ì„± ë³´ì¥)
                next_chunk_start = TimeUtils.get_time_by_ticks(last_effective_time, collection_state.timeframe, -1)
                params["to"] = next_chunk_start

                time_source = collection_state.get_last_time_source()
                logger.debug(f"ChunkInfo ì—°ì†ì„±: {last_effective_time} (ì¶œì²˜: {time_source}) â†’ {next_chunk_start}")

            except Exception as e:
                logger.warning(f"ChunkInfo ì—°ì†ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")

        # ë‹¤ìŒ ì²­í¬ ìƒì„±
        next_chunk = self._create_next_chunk(
            collection_state=collection_state,
            chunk_params=params,
            chunk_index=next_chunk_index
        )
        collection_state.current_chunk = next_chunk

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ìƒì„±: {next_chunk.chunk_id}")

    def _create_next_chunk(
        self,
        collection_state: InternalCollectionState,
        chunk_params: Dict[str, Any],
        chunk_index: int
    ) -> ChunkInfo:
        """ë‹¤ìŒ ì²­í¬ ì •ë³´ ìƒì„± - Legacy ë¡œì§ ì™„ì „ ì´ì‹"""
        chunk_id = f"{collection_state.symbol}_{collection_state.timeframe}_{chunk_index:05d}"

        # 'to' íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        if "to" in chunk_params and chunk_params["to"]:
            to_param = chunk_params["to"]
            if isinstance(to_param, datetime):
                to_datetime = to_param
            elif isinstance(to_param, str):
                try:
                    # ğŸš€ UTC í†µì¼: ë‹¨ìˆœí•œ fromisoformat (ì§„ì…ì ì—ì„œ ì´ë¯¸ ì •ê·œí™”ë¨)
                    to_datetime = datetime.fromisoformat(to_param)
                except (ValueError, TypeError):
                    logger.warning(f"'to' íŒŒë¼ë¯¸í„° íŒŒì‹± ì‹¤íŒ¨: {to_param}")
                    to_datetime = None
            else:
                logger.warning(f"'to' íŒŒë¼ë¯¸í„° íƒ€ì… ì˜¤ë¥˜: {type(to_param)}")
                to_datetime = None
        else:
            to_datetime = None  # COUNT_ONLYëŠ” None ìœ ì§€

        # end ì‹œê°„ ê³„ì‚° - ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë
        calculated_end = None
        if to_datetime and chunk_params.get("count"):
            # toì™€ countê°€ ìˆìœ¼ë©´ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë ê³„ì‚°
            calculated_end = TimeUtils.get_time_by_ticks(
                to_datetime,
                collection_state.timeframe,
                -(chunk_params["count"] - 1)
            )
        else:
            # COUNT_ONLY, END_ONLY ê²½ìš° None ìœ ì§€ (í˜„ì¬ ì‹œê°„ ì‚¬ìš© ì•ˆ í•¨)
            calculated_end = None

        chunk_info = ChunkInfo(
            chunk_id=chunk_id,
            chunk_index=chunk_index,
            symbol=collection_state.symbol,
            timeframe=collection_state.timeframe,
            count=chunk_params["count"],
            to=to_datetime,
            end=calculated_end,
            status="pending"
        )

        return chunk_info

    def _create_success_result(
        self,
        collection_state: InternalCollectionState,
        processing_time: float
    ) -> CollectionResult:
        """ì„±ê³µ ê²°ê³¼ ìƒì„± - Legacy ë²”ìœ„ ê³„ì‚° í¬í•¨"""

        # Legacy ì›ë³¸ì˜ ì •êµí•œ ë²”ìœ„ ê³„ì‚° ë¡œì§ ì ìš©
        collected_start_time = None
        collected_end_time = None

        try:
            # ğŸš€ ì—…ë¹„íŠ¸ API íŠ¹ì„± ê³ ë ¤í•œ ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚° (Legacy ë¡œì§)
            aligned_to = collection_state.request_info.get_aligned_to_time()
            expected_count = collection_state.request_info.get_expected_count()
            request_type = collection_state.request_info.get_request_type()
            timeframe = collection_state.timeframe

            # 1. ì—…ë¹„íŠ¸ to exclusive íŠ¹ì„±: ìš”ì²­ íƒ€ì…ë³„ ì‹¤ì œ ìˆ˜ì§‘ ì‹œì‘ì  ê³„ì‚°
            if request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]:
                # COUNT_ONLY/END_ONLY: ì²« ë²ˆì§¸ ì²­í¬ì˜ ì‹¤ì œ API ì‘ë‹µ ì‹œì‘ì  ì‚¬ìš©
                if collection_state.completed_chunks and collection_state.completed_chunks[0].api_response_start:
                    actual_start = collection_state.completed_chunks[0].api_response_start
                else:
                    # í´ë°±: ê¸°ì¡´ ë¡œì§ (ì²« ì²­í¬ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°)
                    actual_start = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)
            else:
                # TO_COUNT/TO_END: ê¸°ì¡´ ë¡œì§ (aligned_toì—ì„œ 1í‹± ê³¼ê±°ë¡œ ì´ë™)
                actual_start = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)

            # 2. Count ê¸°ë°˜ ì¢…ë£Œì  ì¬ê³„ì‚°: actual_startì—ì„œ expected_count-1í‹± ê³¼ê±° (ì‹¤ì œ ìˆ˜ì§‘ ì¢…ë£Œì )
            actual_end = TimeUtils.get_time_by_ticks(actual_start, timeframe, -(expected_count - 1))

            collected_start_time = actual_start
            collected_end_time = actual_end

            logger.debug(f"ğŸ” ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚°: {aligned_to} â†’ {actual_start} ~ {actual_end} ({expected_count}ê°œ)")

        except Exception as e:
            logger.warning(f"ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚° ì‹¤íŒ¨: {e} - ë²”ìœ„ ì •ë³´ ì—†ì´ ê²°ê³¼ ìƒì„±")

        return create_success_collection_result(
            collected_count=collection_state.total_collected,
            requested_count=collection_state.total_requested,
            processing_time_seconds=processing_time,
            chunks_processed=len(collection_state.completed_chunks),
            api_calls_made=collection_state.api_calls_made,
            overlap_optimizations=collection_state.overlap_optimizations,
            empty_candles_filled=collection_state.empty_candles_filled,
            collected_start_time=collected_start_time,
            collected_end_time=collected_end_time
        )

    def _create_error_result(
        self,
        error: Exception,
        processing_time: float,
        collection_state: Optional[InternalCollectionState] = None
    ) -> CollectionResult:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        error_chunk_id = None
        if collection_state and collection_state.current_chunk:
            error_chunk_id = collection_state.current_chunk.chunk_id

        return create_error_collection_result(
            error=error,
            collected_count=collection_state.total_collected if collection_state else 0,
            requested_count=collection_state.total_requested if collection_state else 0,
            processing_time_seconds=processing_time,
            error_chunk_id=error_chunk_id
        )

    def _convert_to_internal_state(self, external_state: CollectionState) -> InternalCollectionState:
        """ì™¸ë¶€ ìƒíƒœë¥¼ ë‚´ë¶€ ìƒíƒœë¡œ ë³€í™˜ - CandleDataProvider ì—°ë™ìš©"""
        internal_state = InternalCollectionState(
            request_info=external_state.request_info,
            symbol=external_state.symbol,
            timeframe=external_state.timeframe,
            total_requested=external_state.total_requested,
            estimated_total_chunks=external_state.estimated_total_chunks,
            total_collected=external_state.total_collected
        )

        # í˜„ì¬ ì²­í¬ ì •ë³´ ë³µì‚¬
        if external_state.current_chunk:
            internal_state.current_chunk = external_state.current_chunk

        # ì™„ë£Œëœ ì²­í¬ ì •ë³´ ë³µì‚¬
        if external_state.completed_chunks:
            internal_state.completed_chunks = external_state.completed_chunks.copy()

        # ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ì—¬ë¶€ ë³µì‚¬
        if hasattr(external_state, 'reached_upbit_data_end'):
            internal_state.reached_upbit_data_end = external_state.reached_upbit_data_end

        return internal_state

    def _update_external_state(
        self,
        external_state: CollectionState,
        internal_state: InternalCollectionState,
        result: ChunkResult
    ) -> None:
        """ë‚´ë¶€ ìƒíƒœë¥¼ ì™¸ë¶€ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸ - CandleDataProvider ì—°ë™ìš©"""
        # ìˆ˜ì§‘ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        external_state.total_collected = internal_state.total_collected

        # í˜„ì¬ ì²­í¬ ì •ë³´ ì—…ë°ì´íŠ¸
        external_state.current_chunk = internal_state.current_chunk

        # ì™„ë£Œëœ ì²­í¬ ì •ë³´ ì—…ë°ì´íŠ¸
        if result.success and internal_state.current_chunk:
            if internal_state.current_chunk not in external_state.completed_chunks:
                external_state.completed_chunks.append(internal_state.current_chunk)

        # ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ì—¬ë¶€ ì—…ë°ì´íŠ¸
        if hasattr(internal_state, 'reached_upbit_data_end'):
            external_state.reached_upbit_data_end = internal_state.reached_upbit_data_end

        # ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¶”ì • ì—…ë°ì´íŠ¸
        external_state.last_update_time = datetime.now(timezone.utc)
