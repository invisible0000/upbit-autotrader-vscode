# ì²« ì²­í¬ ì²˜ë¦¬ ê°œì„  ì‹¤í–‰ ê³„íšì„œ v2.0

> ğŸ“‹ **ëª©ì **: ìµœì†Œ ë³€ê²½ ì›ì¹™ ê¸°ë°˜ ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ
> ğŸ“… **ì‘ì„±ì¼**: 2025-09-24
> ğŸ¯ **ê¸°ì¤€ ë¬¸ì„œ**: ì²«_ì²­í¬_ì²˜ë¦¬_ê°œì„ _ë°©ì•ˆ_v2_ìµœì†Œë³€ê²½.md

## ğŸš€ Phase 1: ìƒˆ ë©”ì„œë“œ ì¶”ê°€ (Zero Impact)

### 1.1 `_process_first_chunk` ë©”ì„œë“œ êµ¬í˜„

**ğŸ“ ìœ„ì¹˜**: `chunk_processor.py` ì˜ `_process_single_chunk` ë©”ì„œë“œ ë°”ë¡œ ì•

**ğŸ”§ êµ¬í˜„ ë‚´ìš©**:
```python
async def _process_first_chunk(self, request_info: RequestInfo, chunk: ChunkInfo) -> None:
    """
    ì²« ì²­í¬ ì „ìš© ì²˜ë¦¬ ë¡œì§

    RequestTypeë³„ ìµœì í™”ëœ ì²˜ë¦¬:
    - COUNT_ONLY, END_ONLY: ê²¹ì¹¨ ë¶„ì„ ê±´ë„ˆëœ€
    - TO_COUNT, TO_END: ê²¹ì¹¨ ë¶„ì„ ìˆ˜í–‰

    Args:
        request_info: ìš”ì²­ ì •ë³´ (RequestType íŒë‹¨ìš©)
        chunk: ì²« ë²ˆì§¸ ì²­í¬ ì •ë³´
    """
    logger.info(f"ì²« ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {chunk.chunk_id} (íƒ€ì…: {request_info.get_request_type().value})")
    chunk.mark_processing()

    try:
        # 1. ê¸°ì¡´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ í™œìš©í•˜ì—¬ API íŒŒë¼ë¯¸í„° ìƒì„±
        from upbit_auto_trading.infrastructure.market_data.candle.models.candle_business_models import (
            _create_first_chunk_params_by_type
        )
        api_params = _create_first_chunk_params_by_type(request_info, self.chunk_size)

        # 2. ê²¹ì¹¨ ë¶„ì„ ì¡°ê±´ë¶€ ì‹¤í–‰ (RequestType ê¸°ë°˜)
        if not request_info.should_skip_overlap_analysis_for_first_chunk():
            logger.debug(f"ì²« ì²­í¬ ê²¹ì¹¨ ë¶„ì„ ì‹¤í–‰: {chunk.chunk_id}")
            overlap_result = await self._analyze_chunk_overlap(chunk)
            if overlap_result:
                chunk.set_overlap_info(overlap_result)
        else:
            logger.debug(f"ì²« ì²­í¬ ê²¹ì¹¨ ë¶„ì„ ê±´ë„ˆëœ€: {chunk.chunk_id}")

        # 3. API íŒŒë¼ë¯¸í„° ì¶”ì¶œí•˜ì—¬ ì§ì ‘ í˜¸ì¶œ
        symbol = api_params['market']
        count = api_params['count']
        to = api_params.get('to')

        logger.debug(f"ì²« ì²­í¬ API í˜¸ì¶œ: symbol={symbol}, count={count}, to={to}")

        # 4. ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
        if chunk.needs_api_call():
            api_data = await self._fetch_api_data(
                symbol=symbol,
                timeframe=request_info.timeframe,
                count=count,
                to=to
            )
            chunk.set_api_response_info(api_data)

            # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
            processed_data = await self._process_empty_candles(api_data, chunk, is_first_chunk=True)
            chunk.set_final_candle_info(processed_data)

            # ì €ì¥ ì²˜ë¦¬
            if not self.dry_run and processed_data:
                await self.repository.store_candles(processed_data)

        else:
            logger.info(f"ì™„ì „ ê²¹ì¹¨ìœ¼ë¡œ API í˜¸ì¶œ ê±´ë„ˆëœ€: {chunk.chunk_id}")
            # DBì—ì„œ ê¸°ì¡´ ë°ì´í„° ì •ë³´ë§Œ ì„¤ì •
            chunk.set_final_candle_info([])

        # 5. ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
        chunk.mark_completed()
        self._log_chunk_info_debug(chunk, status="first_chunk_completed")
        logger.info(f"ì²« ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: {chunk.chunk_id}")

    except Exception as e:
        chunk.mark_failed()
        logger.error(f"ì²« ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
        raise
```

**ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë©”ì„œë“œ êµ¬í˜„ ì™„ë£Œ
- [ ] ë¡œê¹… ë©”ì‹œì§€ í™•ì¸
- [ ] RequestTypeë³„ ë¶„ê¸° ë¡œì§ ê²€ì¦
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ í™•ì¸

### 1.2 ê¸°ì¡´ `_fetch_api_data` ë©”ì„œë“œ ì§ì ‘ íŒŒë¼ë¯¸í„° ì§€ì› ì¶”ê°€

**ğŸ“ ìœ„ì¹˜**: ê¸°ì¡´ `_fetch_api_data` ë©”ì„œë“œ ìˆ˜ì •

**ğŸ”§ êµ¬í˜„ ë‚´ìš©**:
```python
async def _fetch_api_data(
    self,
    chunk: Optional[ChunkInfo] = None,
    symbol: Optional[str] = None,
    timeframe: Optional[str] = None,
    count: Optional[int] = None,
    to: Optional[datetime] = None
) -> List[Dict[str, Any]]:
    """
    API ë°ì´í„° ìˆ˜ì§‘ - ì§ì ‘ íŒŒë¼ë¯¸í„°ì™€ ChunkInfo ëª¨ë‘ ì§€ì›

    Args:
        chunk: ê¸°ì¡´ ChunkInfo ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        symbol: ì§ì ‘ íŒŒë¼ë¯¸í„° - ê±°ë˜ ì‹¬ë³¼
        timeframe: ì§ì ‘ íŒŒë¼ë¯¸í„° - íƒ€ì„í”„ë ˆì„
        count: ì§ì ‘ íŒŒë¼ë¯¸í„° - ìº”ë“¤ ê°œìˆ˜
        to: ì§ì ‘ íŒŒë¼ë¯¸í„° - ì‹œì‘ ì‹œì 
    """
    # íŒŒë¼ë¯¸í„° ì²˜ë¦¬: ì§ì ‘ íŒŒë¼ë¯¸í„° ìš°ì„ , ì—†ìœ¼ë©´ ChunkInfoì—ì„œ ì¶”ì¶œ
    if symbol and timeframe and count:
        # ì§ì ‘ íŒŒë¼ë¯¸í„° ì‚¬ìš© (ì²« ì²­í¬ ì²˜ë¦¬ìš©)
        api_symbol = symbol
        api_timeframe = timeframe
        api_count = count
        api_to = to
        logger.debug(f"ì§ì ‘ íŒŒë¼ë¯¸í„° API í˜¸ì¶œ: {api_symbol} {api_timeframe}, count={api_count}, to={api_to}")
    elif chunk:
        # ê¸°ì¡´ ChunkInfo ë°©ì‹ (í›„ì† ì²­í¬ìš©)
        api_count, api_to = chunk.get_api_params()
        api_symbol = chunk.symbol
        api_timeframe = chunk.timeframe
        logger.debug(f"ChunkInfo ê¸°ë°˜ API í˜¸ì¶œ: {api_symbol} {api_timeframe}, count={api_count}, to={api_to}")
    else:
        raise ValueError("chunk ë˜ëŠ” (symbol, timeframe, count) íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤")

    try:
        # Upbit to íŒŒë¼ë¯¸í„° ì¡°ì • (ë‹¤ìŒ í‹±ì„ ê°€ë¦¬í‚¤ë„ë¡)
        to_param = None
        if api_to is not None:
            fetch_time = TimeUtils.get_time_by_ticks(api_to, api_timeframe, 1)
            to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")
            logger.debug(f"to íŒŒë¼ë¯¸í„° ë³€í™˜: {api_to} â†’ {to_param}")

        # íƒ€ì„í”„ë ˆì„ë³„ API í˜¸ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if api_timeframe in ['1m', '3m', '5m', '15m', '10m', '30m', '1h', '4h']:
            unit = int(api_timeframe.rstrip('mh'))
            api_data = await self.upbit_client.get_candles_minutes(
                unit=unit,
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1d':
            api_data = await self.upbit_client.get_candles_days(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1w':
            api_data = await self.upbit_client.get_candles_weeks(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1M':
            api_data = await self.upbit_client.get_candles_months(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {api_timeframe}")

        logger.debug(f"API ì‘ë‹µ ìˆ˜ì‹ : {len(api_data)}ê°œ ìº”ë“¤")
        return api_data

    except Exception as e:
        logger.error(f"API ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {api_symbol} {api_timeframe}, ì˜¤ë¥˜: {e}")
        raise
```

**ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì§ì ‘ íŒŒë¼ë¯¸í„° ì§€ì› ì¶”ê°€ ì™„ë£Œ
- [ ] ê¸°ì¡´ ChunkInfo ë°©ì‹ ì™„ì „ í˜¸í™˜ì„± ìœ ì§€
- [ ] íƒ€ì„í”„ë ˆì„ë³„ ë¶„ê¸° ë¡œì§ ê²€ì¦
- [ ] to íŒŒë¼ë¯¸í„° ë³€í™˜ ë¡œì§ í™•ì¸
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹… í™•ì¸

## ğŸ”„ Phase 2: ê¸°ì¡´ ë©”ì„œë“œ ìˆ˜ì • (Gradual Impact)

### 2.1 `process_collection` ë©”ì„œë“œ ìˆ˜ì •

**ğŸ“ ìœ„ì¹˜**: `process_collection` ë©”ì„œë“œì˜ ì²­í¬ ë°˜ë³µ ì²˜ë¦¬ ë¶€ë¶„

**ğŸ”§ ìˆ˜ì • ì „í›„ ë¹„êµ**:

**ìˆ˜ì • ì „**:
```python
# 3. ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬ (ë‹¨ìˆœí•œ ë¦¬ìŠ¤íŠ¸ ê´€ë¦¬)
chunks: List[ChunkInfo] = []

for chunk_index in range(plan.estimated_chunks):
    # ì²­í¬ ìƒì„± ë° ì²˜ë¦¬ (ì²« ì²­í¬ ë¡œì§ ìˆ¨ê²¨ì§)
    chunk = self._create_chunk(chunk_index, request_info, plan, chunks)
    await self._process_single_chunk(chunk)  # ë‚´ë¶€ì—ì„œ ì²« ì²­í¬ ì²´í¬
    chunks.append(chunk)

    # ì§„í–‰ ìƒí™© ì½œë°±
    if progress_callback:
        progress_callback(len(chunks), plan.estimated_chunks)

    # ì™„ë£Œ ì¡°ê±´ í™•ì¸
    if should_complete_collection(request_info, chunks):
        break
```

**ìˆ˜ì • í›„**:
```python
# 3. ì²« ì²­í¬ ëª…ì‹œì  ì²˜ë¦¬ â­ï¸ í•µì‹¬ ê°œì„ 
first_chunk = self._create_chunk(0, request_info, plan, [])
await self._process_first_chunk(request_info, first_chunk)
chunks = [first_chunk]

# ì§„í–‰ ìƒí™© ì½œë°±
if progress_callback:
    progress_callback(1, plan.estimated_chunks)

# 4. ì™„ë£Œ ì¡°ê±´ í™•ì¸ (ì²« ì²­í¬ë§Œìœ¼ë¡œ ì™„ë£Œ ê°€ëŠ¥)
if should_complete_collection(request_info, chunks):
    logger.info(f"ì²« ì²­í¬ë§Œìœ¼ë¡œ ìˆ˜ì§‘ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
    processing_time = time.time() - start_time
    logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, ì²˜ë¦¬ ì‹œê°„ {processing_time:.2f}s")
    return self._create_success_result(chunks, request_info)

# 5. í›„ì† ì²­í¬ ë°˜ë³µ ì²˜ë¦¬ (í•„ìš”í•œ ê²½ìš°ë§Œ)
chunk_index = 1
while not should_complete_collection(request_info, chunks):
    # í›„ì† ì²­í¬ ìƒì„± ë° ì²˜ë¦¬
    next_chunk = self._create_chunk(chunk_index, request_info, plan, chunks)
    await self._process_single_chunk(next_chunk)  # ì²« ì²­í¬ ë¡œì§ ì—†ëŠ” ìˆœìˆ˜ ë²„ì „
    chunks.append(next_chunk)

    # ì§„í–‰ ìƒí™© ì½œë°±
    if progress_callback:
        progress_callback(len(chunks), max(plan.estimated_chunks, len(chunks)))

    chunk_index += 1

    # ë¬´í•œ ë£¨í”„ ë°©ì§€
    if chunk_index > plan.estimated_chunks * 2:
        logger.warning(f"ì˜ˆìƒ ì²­í¬ ìˆ˜ ì´ˆê³¼ë¡œ ê°•ì œ ì¢…ë£Œ: {chunk_index}")
        break
```

**ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì²« ì²­í¬ ëª…ì‹œì  ì²˜ë¦¬ êµ¬í˜„
- [ ] ì™„ë£Œ ì¡°ê±´ í™•ì¸ ë¡œì§ ì¶”ê°€
- [ ] í›„ì† ì²­í¬ ë°˜ë³µ ì²˜ë¦¬ êµ¬í˜„
- [ ] ì§„í–‰ ìƒí™© ì½œë°± ì—…ë°ì´íŠ¸
- [ ] ë¬´í•œ ë£¨í”„ ë°©ì§€ ë¡œì§ í™•ì¸

### 2.2 `_create_chunk` ë©”ì„œë“œ ìˆ˜ì •

**ğŸ”§ ì£¼ìš” ìˆ˜ì • ì‚¬í•­**:

1. **plan.first_chunk_params ì˜ì¡´ì„± ì œê±°**
2. **RequestInfo ì‚¬ì „ ê³„ì‚°ëœ ê°’ í™œìš©**
3. **ì²« ì²­í¬ ë¡œì§ ë‹¨ìˆœí™”**

**ìˆ˜ì •í•  ë¶€ë¶„**:
```python
def _create_chunk(
    self,
    chunk_index: int,
    request_info: RequestInfo,
    plan: CollectionPlan,
    completed_chunks: List[ChunkInfo]
) -> ChunkInfo:
    """ì²­í¬ ìƒì„± - ì²«/í›„ì† ì²­í¬ í†µí•© ì²˜ë¦¬ (plan.first_chunk_params ì˜ì¡´ì„± ì œê±°)"""

    # ì²­í¬ í¬ê¸° ê³„ì‚° (ë‚¨ì€ ê°œìˆ˜ ê³ ë ¤)
    collected_count = sum(c.calculate_effective_candle_count() for c in completed_chunks if c.is_completed())
    remaining_count = request_info.expected_count - collected_count
    chunk_count = min(remaining_count, self.chunk_size)

    if chunk_index == 0:
        # ì²« ì²­í¬: RequestInfoì˜ ì‚¬ì „ ê³„ì‚°ëœ ê°’ í™œìš© (plan.first_chunk_params ì œê±°)
        to_time = None
        if request_info.should_align_time():
            aligned_to = request_info.get_aligned_to_time()
            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            to_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

        # end ì‹œê°„ ê³„ì‚°
        end_time = None
        if to_time and chunk_count:
            end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

    else:
        # í›„ì† ì²­í¬: ì´ì „ ì²­í¬ì˜ ìœ íš¨ ë ì‹œê°„ ê¸°ë°˜ ì—°ì†ì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        last_chunk = completed_chunks[-1]
        last_effective_time = last_chunk.get_effective_end_time()

        if not last_effective_time:
            raise ValueError(f"ì´ì „ ì²­í¬ {last_chunk.chunk_id}ì˜ ìœ íš¨ ë ì‹œê°„ì´ ì—†ìŠµë‹ˆë‹¤")

        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ = ì´ì „ ì²­í¬ ë - 1í‹± (ì—°ì†ì„±)
        to_time = TimeUtils.get_time_by_ticks(last_effective_time, request_info.timeframe, -1)
        end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

    # ChunkInfo ìƒì„± (ê³µí†µ)
    chunk = ChunkInfo(
        chunk_id=f"{request_info.symbol}_{request_info.timeframe}_{chunk_index:05d}",
        chunk_index=chunk_index,
        symbol=request_info.symbol,
        timeframe=request_info.timeframe,
        count=chunk_count,
        to=to_time,
        end=end_time
    )

    self._log_chunk_info_debug(chunk, status="created")
    return chunk
```

**ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] plan.first_chunk_params ì˜ì¡´ì„± ì œê±°
- [ ] RequestInfo ì‚¬ì „ ê³„ì‚° ê°’ í™œìš©
- [ ] ì²« ì²­í¬ ë¡œì§ ë‹¨ìˆœí™”
- [ ] í›„ì† ì²­í¬ ë¡œì§ ìœ ì§€
- [ ] ì‹œê°„ ê³„ì‚° ë¡œì§ ê²€ì¦

### 2.3 `_process_single_chunk` ë©”ì„œë“œ ì •ë¦¬

**ğŸ”§ ì œê±°í•  ë¡œì§**:
```python
# ì œê±° ëŒ€ìƒ
request_type = self._get_request_type_from_chunk(chunk)
is_first_chunk = chunk.chunk_index == 0

if not self._should_skip_overlap_analysis(is_first_chunk, request_type):
    # ê²¹ì¹¨ ë¶„ì„ ë¡œì§
```

**ğŸ”§ ìˆ˜ì • í›„**:
```python
async def _process_single_chunk(self, chunk: ChunkInfo) -> None:
    """
    í›„ì† ì²­í¬ ì²˜ë¦¬ (ì²« ì²­í¬ ë¡œì§ ì œê±°ëœ ìˆœìˆ˜ ë²„ì „)

    ì´ ë©”ì„œë“œëŠ” ì²« ì²­í¬ ì´í›„ì˜ ì²­í¬ë“¤ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    ì²« ì²­í¬ëŠ” _process_first_chunk()ì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    """
    logger.info(f"í›„ì† ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {chunk.chunk_id}")
    chunk.mark_processing()

    try:
        # 1. ê²¹ì¹¨ ë¶„ì„ (í›„ì† ì²­í¬ëŠ” í•­ìƒ ì‹¤í–‰)
        overlap_result = await self._analyze_chunk_overlap(chunk)
        if overlap_result:
            chunk.set_overlap_info(overlap_result)

        # 2. ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
        if chunk.needs_api_call():
            api_data = await self._fetch_api_data(chunk=chunk)  # ê¸°ì¡´ ChunkInfo ë°©ì‹ ìœ ì§€
            chunk.set_api_response_info(api_data)

            # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬
            processed_data = await self._process_empty_candles(api_data, chunk, is_first_chunk=False)
            chunk.set_final_candle_info(processed_data)

            # ì €ì¥ ì²˜ë¦¬
            if not self.dry_run and processed_data:
                await self.repository.store_candles(processed_data)
        else:
            logger.info(f"ì™„ì „ ê²¹ì¹¨ìœ¼ë¡œ API í˜¸ì¶œ ê±´ë„ˆëœ€: {chunk.chunk_id}")
            chunk.set_final_candle_info([])

        # 3. ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
        chunk.mark_completed()
        self._log_chunk_info_debug(chunk, status="completed")
        logger.info(f"í›„ì† ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ: {chunk.chunk_id}")

    except Exception as e:
        chunk.mark_failed()
        logger.error(f"í›„ì† ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {chunk.chunk_id}, ì˜¤ë¥˜: {e}")
        raise
```

## ğŸ—‘ï¸ Phase 3: ë¶ˆí•„ìš” ì½”ë“œ ì •ë¦¬ (Cleanup)

### 3.1 `CollectionPlan` ìˆ˜ì •

**ğŸ“ ìœ„ì¹˜**: `candle_business_models.py`

**ğŸ”§ ìˆ˜ì • ë‚´ìš©**:
- `first_chunk_params: Dict[str, Any]` í•„ë“œ ì œê±°
- `create_collection_plan()` í•¨ìˆ˜ì—ì„œ ê´€ë ¨ ë¡œì§ ì œê±°

### 3.2 ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë©”ì„œë“œ ì œê±°

**ğŸ—‘ï¸ ì œê±° ëŒ€ìƒ**:
- `_get_request_type_from_chunk()` ë©”ì„œë“œ
- `_should_skip_overlap_analysis()` ë©”ì„œë“œ

**ğŸ“‹ ì œê±° ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ë©”ì„œë“œ ì‚¬ìš©ì²˜ ì „ì²´ ê²€ìƒ‰ ë° í™•ì¸
- [ ] í…ŒìŠ¤íŠ¸ ì½”ë“œì—ì„œ ì‚¬ìš© ì—¬ë¶€ í™•ì¸
- [ ] ë‹¤ë¥¸ íŒŒì¼ì—ì„œ import ì—¬ë¶€ í™•ì¸

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì „ëµ

### Unit Tests
1. `_process_first_chunk()` RequestTypeë³„ ë™ì‘ ê²€ì¦
2. `_fetch_api_data()` ì§ì ‘ íŒŒë¼ë¯¸í„°ì™€ ChunkInfo ë°©ì‹ ëª¨ë‘ í…ŒìŠ¤íŠ¸
3. `_create_chunk()` ì²«/í›„ì† ì²­í¬ ìƒì„± ë¡œì§ ê²€ì¦

### Integration Tests
1. `process_collection()` ì „ì²´ í”Œë¡œìš° í…ŒìŠ¤íŠ¸
2. ì²« ì²­í¬ë§Œìœ¼ë¡œ ì™„ë£Œë˜ëŠ” ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸
3. ë‹¤ì¤‘ ì²­í¬ ì²˜ë¦¬ ì¼€ì´ìŠ¤ í…ŒìŠ¤íŠ¸

### Regression Tests
1. ê¸°ì¡´ ê¸°ëŠ¥ ë™ì‘ ë³€í™” ì—†ìŒ í™•ì¸
2. API í˜¸ì¶œ íŒ¨í„´ ì¼ì¹˜ì„± ê²€ì¦
3. ì„±ëŠ¥ ì˜í–¥ë„ ì¸¡ì •

## ğŸ“Š êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1 âœ…
- [ ] `_process_first_chunk()` êµ¬í˜„
- [ ] `_fetch_api_data()` ì§ì ‘ íŒŒë¼ë¯¸í„° ì§€ì› ì¶”ê°€
- [ ] Unit Test ì‘ì„±

### Phase 2 ğŸ”„
- [ ] `process_collection()` ë¦¬íŒ©í„°ë§
- [ ] `_create_chunk()` ìˆ˜ì •
- [ ] `_process_single_chunk()` ì •ë¦¬
- [ ] Integration Test

### Phase 3 ğŸ—‘ï¸
- [ ] `CollectionPlan.first_chunk_params` ì œê±°
- [ ] ë¶ˆí•„ìš” ë©”ì„œë“œ ì œê±°
- [ ] ìµœì¢… ê²€ì¦ í…ŒìŠ¤íŠ¸

## ğŸš¨ ì£¼ì˜ì‚¬í•­

1. **ì ì§„ì  ì ìš©**: ê° Phaseë³„ë¡œ í…ŒìŠ¤íŠ¸ í›„ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
2. **ë°±ì—… ìƒì„±**: ìˆ˜ì • ì „ í˜„ì¬ ì½”ë“œ ë°±ì—… í•„ìˆ˜
3. **ì˜í–¥ë„ í™•ì¸**: ê° ë³€ê²½ì‚¬í•­ì˜ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ ì˜í–¥ ê²€í† 
4. **ë¡œê¹… í™œìš©**: ë³€ê²½ ì „í›„ ë™ì‘ ë¹„êµë¥¼ ìœ„í•œ ìƒì„¸ ë¡œê¹…

---

**ğŸ’¡ ì´ ê³„íšì„œë¥¼ í†µí•´ ì•ˆì „í•˜ê³  ì²´ê³„ì ì¸ ë¦¬íŒ©í„°ë§ì„ ì§„í–‰í•˜ì—¬ ì²« ì²­í¬ ì²˜ë¦¬ì˜ ë³µì¡ì„±ì„ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
