# 첫 청크 처리 개선 실행 계획서 v2.0

> 📋 **목적**: 최소 변경 원칙 기반 단계별 구현 가이드
> 📅 **작성일**: 2025-09-24
> 🎯 **기준 문서**: 첫_청크_처리_개선_방안_v2_최소변경.md

## 🚀 Phase 1: 새 메서드 추가 (Zero Impact)

### 1.1 `_process_first_chunk` 메서드 구현

**📍 위치**: `chunk_processor.py` 의 `_process_single_chunk` 메서드 바로 앞

**🔧 구현 내용**:
```python
async def _process_first_chunk(self, request_info: RequestInfo, chunk: ChunkInfo) -> None:
    """
    첫 청크 전용 처리 로직

    RequestType별 최적화된 처리:
    - COUNT_ONLY, END_ONLY: 겹침 분석 건너뜀
    - TO_COUNT, TO_END: 겹침 분석 수행

    Args:
        request_info: 요청 정보 (RequestType 판단용)
        chunk: 첫 번째 청크 정보
    """
    logger.info(f"첫 청크 처리 시작: {chunk.chunk_id} (타입: {request_info.get_request_type().value})")
    chunk.mark_processing()

    try:
        # 1. 기존 유틸리티 함수 활용하여 API 파라미터 생성
        from upbit_auto_trading.infrastructure.market_data.candle.models.candle_business_models import (
            _create_first_chunk_params_by_type
        )
        api_params = _create_first_chunk_params_by_type(request_info, self.chunk_size)

        # 2. 겹침 분석 조건부 실행 (RequestType 기반)
        if not request_info.should_skip_overlap_analysis_for_first_chunk():
            logger.debug(f"첫 청크 겹침 분석 실행: {chunk.chunk_id}")
            overlap_result = await self._analyze_chunk_overlap(chunk)
            if overlap_result:
                chunk.set_overlap_info(overlap_result)
        else:
            logger.debug(f"첫 청크 겹침 분석 건너뜀: {chunk.chunk_id}")

        # 3. API 파라미터 추출하여 직접 호출
        symbol = api_params['market']
        count = api_params['count']
        to = api_params.get('to')

        logger.debug(f"첫 청크 API 호출: symbol={symbol}, count={count}, to={to}")

        # 4. 데이터 수집 및 처리
        if chunk.needs_api_call():
            api_data = await self._fetch_api_data(
                symbol=symbol,
                timeframe=request_info.timeframe,
                count=count,
                to=to
            )
            chunk.set_api_response_info(api_data)

            # 빈 캔들 처리
            processed_data = await self._process_empty_candles(api_data, chunk, is_first_chunk=True)
            chunk.set_final_candle_info(processed_data)

            # 저장 처리
            if not self.dry_run and processed_data:
                await self.repository.store_candles(processed_data)

        else:
            logger.info(f"완전 겹침으로 API 호출 건너뜀: {chunk.chunk_id}")
            # DB에서 기존 데이터 정보만 설정
            chunk.set_final_candle_info([])

        # 5. 청크 완료 처리
        chunk.mark_completed()
        self._log_chunk_info_debug(chunk, status="first_chunk_completed")
        logger.info(f"첫 청크 처리 완료: {chunk.chunk_id}")

    except Exception as e:
        chunk.mark_failed()
        logger.error(f"첫 청크 처리 실패: {chunk.chunk_id}, 오류: {e}")
        raise
```

**📋 체크리스트**:
- [ ] 메서드 구현 완료
- [ ] 로깅 메시지 확인
- [ ] RequestType별 분기 로직 검증
- [ ] 에러 핸들링 확인

### 1.2 기존 `_fetch_api_data` 메서드 직접 파라미터 지원 추가

**📍 위치**: 기존 `_fetch_api_data` 메서드 수정

**🔧 구현 내용**:
```python
async def _fetch_api_data(
    self,
    chunk: Optional[ChunkInfo] = None,
    symbol: Optional[str] = None,
    timeframe: Optional[str] = None,
    count: Optional[int] = None,
    to: Optional[datetime] = None
) -> List[Dict[str, Any]]:
    """
    API 데이터 수집 - 직접 파라미터와 ChunkInfo 모두 지원

    Args:
        chunk: 기존 ChunkInfo 방식 (하위 호환성)
        symbol: 직접 파라미터 - 거래 심볼
        timeframe: 직접 파라미터 - 타임프레임
        count: 직접 파라미터 - 캔들 개수
        to: 직접 파라미터 - 시작 시점
    """
    # 파라미터 처리: 직접 파라미터 우선, 없으면 ChunkInfo에서 추출
    if symbol and timeframe and count:
        # 직접 파라미터 사용 (첫 청크 처리용)
        api_symbol = symbol
        api_timeframe = timeframe
        api_count = count
        api_to = to
        logger.debug(f"직접 파라미터 API 호출: {api_symbol} {api_timeframe}, count={api_count}, to={api_to}")
    elif chunk:
        # 기존 ChunkInfo 방식 (후속 청크용)
        api_count, api_to = chunk.get_api_params()
        api_symbol = chunk.symbol
        api_timeframe = chunk.timeframe
        logger.debug(f"ChunkInfo 기반 API 호출: {api_symbol} {api_timeframe}, count={api_count}, to={api_to}")
    else:
        raise ValueError("chunk 또는 (symbol, timeframe, count) 파라미터가 필요합니다")

    try:
        # Upbit to 파라미터 조정 (다음 틱을 가리키도록)
        to_param = None
        if api_to is not None:
            fetch_time = TimeUtils.get_time_by_ticks(api_to, api_timeframe, 1)
            to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")
            logger.debug(f"to 파라미터 변환: {api_to} → {to_param}")

        # 타임프레임별 API 호출 (기존 로직 유지)
        if api_timeframe in ['1m', '3m', '5m', '15m', '10m', '30m', '1h', '4h']:
            unit = int(api_timeframe.rstrip('mh'))
            api_data = await self.upbit_client.get_candles_minutes(
                unit=unit,
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1d':
            api_data = await self.upbit_client.get_candles_days(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1w':
            api_data = await self.upbit_client.get_candles_weeks(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1M':
            api_data = await self.upbit_client.get_candles_months(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        else:
            raise ValueError(f"지원하지 않는 타임프레임: {api_timeframe}")

        logger.debug(f"API 응답 수신: {len(api_data)}개 캔들")
        return api_data

    except Exception as e:
        logger.error(f"API 데이터 수집 실패: {api_symbol} {api_timeframe}, 오류: {e}")
        raise
```

**📋 체크리스트**:
- [ ] 직접 파라미터 지원 추가 완료
- [ ] 기존 ChunkInfo 방식 완전 호환성 유지
- [ ] 타임프레임별 분기 로직 검증
- [ ] to 파라미터 변환 로직 확인
- [ ] 에러 핸들링 및 로깅 확인

## 🔄 Phase 2: 기존 메서드 수정 (Gradual Impact)

### 2.1 `process_collection` 메서드 수정

**📍 위치**: `process_collection` 메서드의 청크 반복 처리 부분

**🔧 수정 전후 비교**:

**수정 전**:
```python
# 3. 청크별 순차 처리 (단순한 리스트 관리)
chunks: List[ChunkInfo] = []

for chunk_index in range(plan.estimated_chunks):
    # 청크 생성 및 처리 (첫 청크 로직 숨겨짐)
    chunk = self._create_chunk(chunk_index, request_info, plan, chunks)
    await self._process_single_chunk(chunk)  # 내부에서 첫 청크 체크
    chunks.append(chunk)

    # 진행 상황 콜백
    if progress_callback:
        progress_callback(len(chunks), plan.estimated_chunks)

    # 완료 조건 확인
    if should_complete_collection(request_info, chunks):
        break
```

**수정 후**:
```python
# 3. 첫 청크 명시적 처리 ⭐️ 핵심 개선
first_chunk = self._create_chunk(0, request_info, plan, [])
await self._process_first_chunk(request_info, first_chunk)
chunks = [first_chunk]

# 진행 상황 콜백
if progress_callback:
    progress_callback(1, plan.estimated_chunks)

# 4. 완료 조건 확인 (첫 청크만으로 완료 가능)
if should_complete_collection(request_info, chunks):
    logger.info(f"첫 청크만으로 수집 완료: {len(chunks)}개 청크")
    processing_time = time.time() - start_time
    logger.info(f"수집 완료: {len(chunks)}개 청크, 처리 시간 {processing_time:.2f}s")
    return self._create_success_result(chunks, request_info)

# 5. 후속 청크 반복 처리 (필요한 경우만)
chunk_index = 1
while not should_complete_collection(request_info, chunks):
    # 후속 청크 생성 및 처리
    next_chunk = self._create_chunk(chunk_index, request_info, plan, chunks)
    await self._process_single_chunk(next_chunk)  # 첫 청크 로직 없는 순수 버전
    chunks.append(next_chunk)

    # 진행 상황 콜백
    if progress_callback:
        progress_callback(len(chunks), max(plan.estimated_chunks, len(chunks)))

    chunk_index += 1

    # 무한 루프 방지
    if chunk_index > plan.estimated_chunks * 2:
        logger.warning(f"예상 청크 수 초과로 강제 종료: {chunk_index}")
        break
```

**📋 체크리스트**:
- [ ] 첫 청크 명시적 처리 구현
- [ ] 완료 조건 확인 로직 추가
- [ ] 후속 청크 반복 처리 구현
- [ ] 진행 상황 콜백 업데이트
- [ ] 무한 루프 방지 로직 확인

### 2.2 `_create_chunk` 메서드 수정

**🔧 주요 수정 사항**:

1. **plan.first_chunk_params 의존성 제거**
2. **RequestInfo 사전 계산된 값 활용**
3. **첫 청크 로직 단순화**

**수정할 부분**:
```python
def _create_chunk(
    self,
    chunk_index: int,
    request_info: RequestInfo,
    plan: CollectionPlan,
    completed_chunks: List[ChunkInfo]
) -> ChunkInfo:
    """청크 생성 - 첫/후속 청크 통합 처리 (plan.first_chunk_params 의존성 제거)"""

    # 청크 크기 계산 (남은 개수 고려)
    collected_count = sum(c.calculate_effective_candle_count() for c in completed_chunks if c.is_completed())
    remaining_count = request_info.expected_count - collected_count
    chunk_count = min(remaining_count, self.chunk_size)

    if chunk_index == 0:
        # 첫 청크: RequestInfo의 사전 계산된 값 활용 (plan.first_chunk_params 제거)
        to_time = None
        if request_info.should_align_time():
            aligned_to = request_info.get_aligned_to_time()
            # 진입점 보정 (사용자 시간 → 내부 시간 변환)
            to_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

        # end 시간 계산
        end_time = None
        if to_time and chunk_count:
            end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

    else:
        # 후속 청크: 이전 청크의 유효 끝 시간 기반 연속성 (기존 로직 유지)
        last_chunk = completed_chunks[-1]
        last_effective_time = last_chunk.get_effective_end_time()

        if not last_effective_time:
            raise ValueError(f"이전 청크 {last_chunk.chunk_id}의 유효 끝 시간이 없습니다")

        # 다음 청크 시작 = 이전 청크 끝 - 1틱 (연속성)
        to_time = TimeUtils.get_time_by_ticks(last_effective_time, request_info.timeframe, -1)
        end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

    # ChunkInfo 생성 (공통)
    chunk = ChunkInfo(
        chunk_id=f"{request_info.symbol}_{request_info.timeframe}_{chunk_index:05d}",
        chunk_index=chunk_index,
        symbol=request_info.symbol,
        timeframe=request_info.timeframe,
        count=chunk_count,
        to=to_time,
        end=end_time
    )

    self._log_chunk_info_debug(chunk, status="created")
    return chunk
```

**📋 체크리스트**:
- [ ] plan.first_chunk_params 의존성 제거
- [ ] RequestInfo 사전 계산 값 활용
- [ ] 첫 청크 로직 단순화
- [ ] 후속 청크 로직 유지
- [ ] 시간 계산 로직 검증

### 2.3 `_process_single_chunk` 메서드 정리

**🔧 제거할 로직**:
```python
# 제거 대상
request_type = self._get_request_type_from_chunk(chunk)
is_first_chunk = chunk.chunk_index == 0

if not self._should_skip_overlap_analysis(is_first_chunk, request_type):
    # 겹침 분석 로직
```

**🔧 수정 후**:
```python
async def _process_single_chunk(self, chunk: ChunkInfo) -> None:
    """
    후속 청크 처리 (첫 청크 로직 제거된 순수 버전)

    이 메서드는 첫 청크 이후의 청크들만 처리합니다.
    첫 청크는 _process_first_chunk()에서 처리됩니다.
    """
    logger.info(f"후속 청크 처리 시작: {chunk.chunk_id}")
    chunk.mark_processing()

    try:
        # 1. 겹침 분석 (후속 청크는 항상 실행)
        overlap_result = await self._analyze_chunk_overlap(chunk)
        if overlap_result:
            chunk.set_overlap_info(overlap_result)

        # 2. 데이터 수집 및 처리
        if chunk.needs_api_call():
            api_data = await self._fetch_api_data(chunk=chunk)  # 기존 ChunkInfo 방식 유지
            chunk.set_api_response_info(api_data)

            # 빈 캔들 처리
            processed_data = await self._process_empty_candles(api_data, chunk, is_first_chunk=False)
            chunk.set_final_candle_info(processed_data)

            # 저장 처리
            if not self.dry_run and processed_data:
                await self.repository.store_candles(processed_data)
        else:
            logger.info(f"완전 겹침으로 API 호출 건너뜀: {chunk.chunk_id}")
            chunk.set_final_candle_info([])

        # 3. 청크 완료 처리
        chunk.mark_completed()
        self._log_chunk_info_debug(chunk, status="completed")
        logger.info(f"후속 청크 처리 완료: {chunk.chunk_id}")

    except Exception as e:
        chunk.mark_failed()
        logger.error(f"후속 청크 처리 실패: {chunk.chunk_id}, 오류: {e}")
        raise
```

## 🗑️ Phase 3: 불필요 코드 정리 (Cleanup)

### 3.1 `CollectionPlan` 수정

**📍 위치**: `candle_business_models.py`

**🔧 수정 내용**:
- `first_chunk_params: Dict[str, Any]` 필드 제거
- `create_collection_plan()` 함수에서 관련 로직 제거

### 3.2 사용하지 않는 메서드 제거

**🗑️ 제거 대상**:
- `_get_request_type_from_chunk()` 메서드
- `_should_skip_overlap_analysis()` 메서드

**📋 제거 전 체크리스트**:
- [ ] 메서드 사용처 전체 검색 및 확인
- [ ] 테스트 코드에서 사용 여부 확인
- [ ] 다른 파일에서 import 여부 확인

## 🧪 테스트 전략

### Unit Tests
1. `_process_first_chunk()` RequestType별 동작 검증
2. `_fetch_api_data()` 직접 파라미터와 ChunkInfo 방식 모두 테스트
3. `_create_chunk()` 첫/후속 청크 생성 로직 검증

### Integration Tests
1. `process_collection()` 전체 플로우 테스트
2. 첫 청크만으로 완료되는 케이스 테스트
3. 다중 청크 처리 케이스 테스트

### Regression Tests
1. 기존 기능 동작 변화 없음 확인
2. API 호출 패턴 일치성 검증
3. 성능 영향도 측정

## 📊 구현 체크리스트

### Phase 1 ✅
- [ ] `_process_first_chunk()` 구현
- [ ] `_fetch_api_data()` 직접 파라미터 지원 추가
- [ ] Unit Test 작성

### Phase 2 🔄
- [ ] `process_collection()` 리팩터링
- [ ] `_create_chunk()` 수정
- [ ] `_process_single_chunk()` 정리
- [ ] Integration Test

### Phase 3 🗑️
- [ ] `CollectionPlan.first_chunk_params` 제거
- [ ] 불필요 메서드 제거
- [ ] 최종 검증 테스트

## 🚨 주의사항

1. **점진적 적용**: 각 Phase별로 테스트 후 다음 단계 진행
2. **백업 생성**: 수정 전 현재 코드 백업 필수
3. **영향도 확인**: 각 변경사항의 다른 컴포넌트 영향 검토
4. **로깅 활용**: 변경 전후 동작 비교를 위한 상세 로깅

---

**💡 이 계획서를 통해 안전하고 체계적인 리팩터링을 진행하여 첫 청크 처리의 복잡성을 해결할 수 있습니다.**
