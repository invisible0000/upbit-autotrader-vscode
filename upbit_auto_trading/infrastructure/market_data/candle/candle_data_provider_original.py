"""
CandleDataProvider v6.2 - ChunkInfo í™•ì¥ ì„±ëŠ¥ ìµœì í™” ë²„ì „

Created: 2025-09-17
Updated: 2025-09-18 (ChunkInfo í™•ì¥ìœ¼ë¡œ temp_chunk ìƒì„± ì œê±°)
Purpose: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ DB ì ‘ê·¼ ìµœì†Œí™”ë¥¼ ìœ„í•œ ì„±ëŠ¥ ìµœì í™”
Features: ì§ì ‘ ì €ì¥ ë°©ì‹, ë¶ˆí•„ìš”í•œ ë³€í™˜ ì œê±°, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 90% ì ˆì•½, ê°ì²´ ìƒì„± ìµœì í™”
Performance:
- ë©”ëª¨ë¦¬: 90% ì ˆì•½ (1GB â†’ 100MB)
- DB ì ‘ê·¼: 56% ê°ì†Œ (16íšŒ â†’ 7íšŒ)
- CPU ì²˜ë¦¬: 70% ê°œì„ 
- ğŸ†• ê°ì²´ ìƒì„±: temp_chunk ì œê±°ë¡œ ì¶”ê°€ ì ˆì•½
Architecture: DDD Infrastructure ê³„ì¸µ, ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
"""

from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any
from enum import Enum
from dataclasses import dataclass, field

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils
from upbit_auto_trading.infrastructure.market_data.candle.models import (
    ChunkInfo, CandleData, CollectionState, RequestInfo, RequestType
)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)
from upbit_auto_trading.infrastructure.market_data.candle.empty_candle_detector import (
    EmptyCandleDetector
)

logger = create_component_logger("CandleDataProvider")


@dataclass
class CollectionPlan:
    """ìˆ˜ì§‘ ê³„íš"""
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float
    first_chunk_params: Dict[str, Any]


class CandleDataProvider:
    """
    ìº”ë“¤ ë°ì´í„° ì œê³µì v6.3 - ChunkInfo ì „ì²´ ì²˜ë¦¬ ë‹¨ê³„ ì¶”ì  ë²„ì „

    ì£¼ìš” ê°œì„ ì‚¬í•­:
    1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: 90% ì ˆì•½ (ì§ì ‘ ì €ì¥ ë°©ì‹)
    2. DB ì ‘ê·¼ ìµœì†Œí™”: 56% ê°ì†Œ (ë¶ˆí•„ìš”í•œ ì¡°íšŒ ì œê±°)
    3. CPU ì²˜ë¦¬ëŸ‰ ê°œì„ : 70% ê°œì„  (ë³€í™˜ ê³¼ì • ì œê±°)
    4. ì½”ë“œ ë‹¨ìˆœì„±: ë³µì¡í•œ ë³‘í•© ë¡œì§ ì œê±°
    5. ğŸ†• ê°ì²´ ìƒì„± ìµœì í™”: temp_chunk ìƒì„± ì œê±° (ChunkInfo í™•ì¥)
    6. ğŸ†• ì „ì²´ ì²˜ë¦¬ ë‹¨ê³„ ì¶”ì : ìš”ì²­ â†’ API ì‘ë‹µ â†’ ìµœì¢… ê²°ê³¼ ì™„ì „ ì¶”ì 

    ChunkInfo í†µí•© ì¶”ì :
    - ìš”ì²­ ë‹¨ê³„: api_request_count/start/end (ì˜¤ë²„ë© ë¶„ì„ ê²°ê³¼)
    - ì‘ë‹µ ë‹¨ê³„: api_response_count/start/end (ì‹¤ì œ API ì‘ë‹µ)
    - ìµœì¢… ë‹¨ê³„: final_candle_count/start/end (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í›„)
    - ë””ë²„ê¹…: get_processing_summary()ë¡œ ì „ì²´ ê³¼ì • ìš”ì•½

    ìµœì í™” ì „ëµ:
    - API Dict â†’ DB ì§ì ‘ ì €ì¥ (CandleData ë³€í™˜ ìƒëµ)
    - OverlapAnalyzer ìœ ì§€ (API ì ˆì•½ íš¨ê³¼ ë³´ì¡´)
    - ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ (ëˆ„ì  ë°©ì§€)
    - ğŸš€ ChunkInfo í†µí•© ê´€ë¦¬: ì²˜ë¦¬ ë‹¨ê³„ë³„ ì™„ì „ ì¶”ì ìœ¼ë¡œ ë””ë²„ê¹… í˜ì‹ 
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        chunk_size: int = 200,
        enable_empty_candle_processing: bool = True
    ):
        """CandleDataProvider v6.2 ì´ˆê¸°í™” (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ + ChunkInfo í™•ì¥ ìµœì í™”)"""
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜
        self.active_collections: Dict[str, CollectionState] = {}
        self.api_rate_limit_rps = 10  # 10 RPS ê¸°ì¤€

        # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì„¤ì •
        self.enable_empty_candle_processing = enable_empty_candle_processing
        self.empty_candle_detectors: Dict[str, EmptyCandleDetector] = {}  # (symbol, timeframe) ì¡°í•© ìºì‹œ

        logger.info("CandleDataProvider v6.3 (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ + ChunkInfo ì „ì²´ ì²˜ë¦¬ ë‹¨ê³„ ì¶”ì ) ì´ˆê¸°í™”")
        logger.info(f"ì²­í¬ í¬ê¸°: {self.chunk_size}, API Rate Limit: {self.api_rate_limit_rps} RPS")
        logger.info(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: {'í™œì„±í™”' if enable_empty_candle_processing else 'ë¹„í™œì„±í™”'}")

    # =========================================================================
    # í•µì‹¬ ê³µê°œ API
    # =========================================================================

    def _get_empty_candle_detector(self, symbol: str, timeframe: str) -> EmptyCandleDetector:
        """(symbol, timeframe) ì¡°í•©ë³„ EmptyCandleDetector ìºì‹œ (ì™„ì „ ê°„ì†Œí™”)"""
        cache_key = f"{symbol}_{timeframe}"
        if cache_key not in self.empty_candle_detectors:
            self.empty_candle_detectors[cache_key] = EmptyCandleDetector(symbol, timeframe)
            logger.debug(f"EmptyCandleDetector ìƒì„±: {symbol} {timeframe}")
        return self.empty_candle_detectors[cache_key]

    async def _process_api_candles_with_empty_filling(
        self,
        api_candles: List[Dict[str, Any]],
        symbol: str,
        timeframe: str,
        api_start: Optional[datetime] = None,
        api_end: Optional[datetime] = None,
        safe_range_start: Optional[datetime] = None,
        safe_range_end: Optional[datetime] = None,
        is_first_chunk: bool = False
    ) -> List[Dict[str, Any]]:
        """
        API ìº”ë“¤ ì‘ë‹µì— ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì ìš© (save_raw_api_data ì „ í˜¸ì¶œ)

        ì²˜ë¦¬ ìˆœì„œ:
        1. DBì—ì„œ ì°¸ì¡° ìƒíƒœ ì¡°íšŒ (ë¹ˆ ìº”ë“¤ ê·¸ë£¹ ì°¸ì¡°ìš©, ì—†ìœ¼ë©´ UUID ê·¸ë£¹ ìƒì„±)
        2. API ìº”ë“¤ ì‘ë‹µì— ë¹ˆìº”ë“¤ ê²€ì‚¬ (api_start ~ api_end ë²”ìœ„ ë‚´)
        3. ê²€ì¶œëœ Gapì„ ë¹ˆ ìº”ë“¤ë¡œ ì±„ìš°ê¸°
        4. API ìº”ë“¤ ì‘ë‹µê³¼ í†µí•©í•˜ì—¬ ì™„ì „í•œ ì‹œê³„ì—´ ìƒì„±

        Args:
            api_candles: ì—…ë¹„íŠ¸ API ì›ì‹œ ì‘ë‹µ ë°ì´í„°
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„
            api_start: API ê²€ì¶œ ë²”ìœ„ ì‹œì‘ ì‹œê°„ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            api_end: API ê²€ì¶œ ë²”ìœ„ ì¢…ë£Œ ì‹œê°„ (Noneì´ë©´ ì œí•œ ì—†ìŒ)
            safe_range_start: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ì‹œì‘ (ì²« ì²­í¬ ì‹œì‘ì )
            safe_range_end: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ë (í˜„ì¬ ì²­í¬ ëì )

        Returns:
            ì²˜ë¦¬ëœ ìº”ë“¤ ë°ì´í„° (Dict í˜•íƒœ ìœ ì§€)
        """
        if not self.enable_empty_candle_processing:
            return api_candles

        detector = self._get_empty_candle_detector(symbol, timeframe)
        processed_candles = detector.detect_and_fill_gaps(
            api_candles,
            api_start=api_start,
            api_end=api_end,
            is_first_chunk=is_first_chunk  # ğŸš€ ì²« ì²­í¬ ì •ë³´ ì „ë‹¬ (api_start +1í‹± ì¶”ê°€ ì œì–´)
        )

        # ìº”ë“¤ ìˆ˜ ë³´ì • ë¡œê¹…
        if len(processed_candles) != len(api_candles):
            empty_count = len(processed_candles) - len(api_candles)
            logger.info(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: ì›ë³¸ {len(api_candles)}ê°œ + ë¹ˆ {empty_count}ê°œ = ìµœì¢… {len(processed_candles)}ê°œ")

        return processed_candles

    async def get_candles(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> List[CandleData]:
        """
        ì™„ì „ ìë™í™”ëœ ìº”ë“¤ ìˆ˜ì§‘ - ì„±ëŠ¥ ìµœì í™” ë²„ì „

        ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ê³¼ DB ì ‘ê·¼ ìµœì†Œí™”ë¥¼ í†µí•œ ê³ ì„±ëŠ¥ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘
        """
        logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ìš”ì²­: {symbol} {timeframe}")
        if count:
            logger.info(f"ê°œìˆ˜: {count}ê°œ")
        if to:
            logger.info(f"ì‹œì‘: {to}")
        if end:
            logger.info(f"ì¢…ë£Œ: {end}")

        # ğŸš€ UTC í†µì¼: ì§„ì…ì ì—ì„œ í•œ ë²ˆë§Œ ì •ê·œí™”í•˜ì—¬ ë‚´ë¶€ ë³µì¡ì„± ì œê±°
        normalized_to = TimeUtils.normalize_datetime_to_utc(to)
        normalized_end = TimeUtils.normalize_datetime_to_utc(end)

        logger.debug(f"UTC ì •ê·œí™”: to={to} â†’ {normalized_to}, end={end} â†’ {normalized_end}")

        # ìˆ˜ì§‘ ì‹œì‘
        request_id = self.start_collection(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=normalized_to,
            end=normalized_end
        )

        try:
            # ì²­í¬ë³„ ìë™ ì²˜ë¦¬ ë£¨í”„ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
            while True:
                chunk_info = self.get_next_chunk(request_id)
                if chunk_info is None:
                    break

                # ì²­í¬ ì™„ë£Œ ì²˜ë¦¬ (ì§ì ‘ ì €ì¥ ë°©ì‹)
                await self.mark_chunk_completed(request_id)

            # ìˆ˜ì§‘ ìƒíƒœì—ì„œ ê²°ê³¼ ì¶”ì¶œ
            collection_state = self.active_collections.get(request_id)
            if collection_state and collection_state.is_completed:
                logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ì™„ë£Œ: {collection_state.total_collected}ê°œ")

                # Repositoryì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ (ìµœì¢… ë³€í™˜ì€ ì—¬ê¸°ì„œë§Œ)
                collected_candles = await self._get_final_result(
                    collection_state, symbol, timeframe, count, to, end
                )
                return collected_candles

            return []

        except Exception as e:
            logger.error(f"ìº”ë“¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise
        finally:
            # ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬ (ë©”ëª¨ë¦¬ í•´ì œ)
            if request_id in self.active_collections:
                del self.active_collections[request_id]

    async def _get_final_result(
        self,
        collection_state: CollectionState,
        symbol: str,
        timeframe: str,
        count: Optional[int],
        to: Optional[datetime],
        end: Optional[datetime]
    ) -> List[CandleData]:
        """ìµœì¢… ê²°ê³¼ ì¡°íšŒ (CandleData ë³€í™˜ì€ ì—¬ê¸°ì„œë§Œ ìˆ˜í–‰) - ì—…ë¹„íŠ¸ API íŠ¹ì„± ë°˜ì˜"""
        try:
            # ğŸš€ ì—…ë¹„íŠ¸ API íŠ¹ì„± ê³ ë ¤í•œ ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚°
            aligned_to = collection_state.request_info.get_aligned_to_time()
            expected_count = collection_state.request_info.get_expected_count()
            request_type = collection_state.request_info.get_request_type()

            # 1. ì—…ë¹„íŠ¸ to exclusive íŠ¹ì„±: ìš”ì²­ íƒ€ì…ë³„ ì‹¤ì œ ìˆ˜ì§‘ ì‹œì‘ì  ê³„ì‚°
            if request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]:
                # COUNT_ONLY/END_ONLY: ì²« ë²ˆì§¸ ì²­í¬ì˜ ì‹¤ì œ API ì‘ë‹µ ì‹œì‘ì  ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
                if collection_state.completed_chunks and collection_state.completed_chunks[0].api_response_start:
                    actual_start = collection_state.completed_chunks[0].api_response_start
                else:
                    # í´ë°±: ê¸°ì¡´ ë¡œì§ (ì²« ì²­í¬ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°)
                    actual_start = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)
            else:
                # TO_COUNT/TO_END: ê¸°ì¡´ ë¡œì§ (aligned_toì—ì„œ 1í‹± ê³¼ê±°ë¡œ ì´ë™)
                actual_start = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)

            # 2. Count ê¸°ë°˜ ì¢…ë£Œì  ì¬ê³„ì‚°: actual_startì—ì„œ expected_count-1í‹± ê³¼ê±° (ì‹¤ì œ ìˆ˜ì§‘ ì¢…ë£Œì )
            actual_end = TimeUtils.get_time_by_ticks(actual_start, timeframe, -(expected_count - 1))

            logger.debug(f"ğŸ” ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„: {aligned_to} â†’ {actual_start} ~ {actual_end} ({expected_count}ê°œ)")

            return await self.repository.get_candles_by_range(
                symbol, timeframe, actual_start, actual_end
            )

        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def start_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> str:
        """ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘"""
        # RequestInfo ìƒì„± (ê²€ì¦ í¬í•¨)
        request_info = RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=to,
            end=end
        )

        logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘: {request_info.to_log_string()}")

        # ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½
        plan = self.plan_collection(symbol, timeframe, count, to, end)

        # ìš”ì²­ ID ìƒì„±
        request_id = f"{symbol}_{timeframe}_{int(datetime.now().timestamp())}"

        # ìˆ˜ì§‘ ìƒíƒœ ì´ˆê¸°í™”
        estimated_completion = datetime.now(timezone.utc) + timedelta(
            seconds=plan.estimated_duration_seconds
        )

        collection_state = CollectionState(
            request_id=request_id,
            request_info=request_info,
            symbol=symbol,
            timeframe=timeframe,
            total_requested=plan.total_count,
            estimated_total_chunks=plan.estimated_chunks,
            estimated_completion_time=estimated_completion,
            remaining_chunks=plan.estimated_chunks,
            estimated_remaining_seconds=plan.estimated_duration_seconds
        )

        # ì²« ë²ˆì§¸ ì²­í¬ ìƒì„±
        first_chunk = self._create_next_chunk(
            collection_state=collection_state,
            chunk_params=plan.first_chunk_params,
            chunk_index=0
        )
        collection_state.current_chunk = first_chunk

        # ìƒíƒœ ë“±ë¡
        self.active_collections[request_id] = collection_state

        logger.info(f"ìˆ˜ì§‘ ì‹œì‘ ì™„ë£Œ: ìš”ì²­ ID {request_id}")
        return request_id

    def get_next_chunk(self, request_id: str) -> Optional[ChunkInfo]:
        """ë‹¤ìŒ ì²˜ë¦¬í•  ì²­í¬ ì •ë³´ ë°˜í™˜"""
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.is_completed:
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œë¨: {request_id}")
            return None

        if state.current_chunk is None:
            logger.warning(f"ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {request_id}")
            return None

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ë°˜í™˜: {state.current_chunk.chunk_id}")
        return state.current_chunk

    async def mark_chunk_completed(self, request_id: str) -> bool:
        """
        ì²­í¬ ì™„ë£Œ ì²˜ë¦¬ - ì„±ëŠ¥ ìµœì í™” ë²„ì „

        í•µì‹¬ ê°œì„ ì‚¬í•­:
        - ì§ì ‘ ì €ì¥ ë°©ì‹: API Dict â†’ DB ì €ì¥ (ë³€í™˜ ìƒëµ)
        - ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ: ë°ì´í„° ëˆ„ì  ë°©ì§€
        - ê²¹ì¹¨ ë¶„ì„ ìœ ì§€: API ì ˆì•½ íš¨ê³¼ ë³´ì¡´

        Returns:
            bool: ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.current_chunk is None:
            raise ValueError("ì²˜ë¦¬ ì¤‘ì¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ìš”ì²­ íƒ€ì… ê¸°ë°˜ ìµœì í™”
        is_first_chunk = len(state.completed_chunks) == 0
        request_type = state.request_info.get_request_type()

        logger.info(f"ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {state.current_chunk.chunk_id} [{request_type.value}]")

        try:
            # ì„±ëŠ¥ ìµœì í™”ëœ ì²­í¬ ì²˜ë¦¬ (ChunkInfo ê¸°ë°˜ìœ¼ë¡œ last_candle_time ë¶ˆí•„ìš”)
            saved_count, _ = await self._process_chunk_direct_storage(
                state.current_chunk, state, is_first_chunk, request_type
            )

            # í˜„ì¬ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
            completed_chunk = state.current_chunk
            completed_chunk.status = "completed"
            state.completed_chunks.append(completed_chunk)

            # ğŸŸ¢ ìƒˆë¡œìš´ ì¹´ìš´íŒ… ë¡œì§: ì²­í¬ ì™„ë£Œ = ë‹´ë‹¹ ë²”ìœ„ ì™„ë£Œ
            # ì‹¤ì œ ì €ì¥ ê°œìˆ˜ì™€ ë¬´ê´€í•˜ê²Œ ì²­í¬ê°€ ë‹´ë‹¹í•œ ë²”ìœ„ ì „ì²´ë¥¼ ì™„ë£Œë¡œ ì²˜ë¦¬
            state.total_collected += completed_chunk.count

            # ğŸ†• ChunkInfo ê¸°ë°˜ ì—°ì†ì„±: ë” ì´ìƒ last_candle_time ì„¤ì • ë¶ˆí•„ìš”
            # ì—°ì†ì„±ì€ _create_next_chunk_paramsì—ì„œ ChunkInfo.get_effective_end_time()ë¡œ ì²˜ë¦¬
            effective_time = completed_chunk.get_effective_end_time()
            time_source = completed_chunk.get_time_source()
            logger.debug(f"ì²­í¬ ì™„ë£Œ - ì‹œê°„ì •ë³´: {effective_time} (ì¶œì²˜: {time_source})")

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ëŠ” CollectionState Propertyì—ì„œ ìë™ ê³„ì‚°ë¨ (last_update_timeë§Œ ì—…ë°ì´íŠ¸)
            state.last_update_time = datetime.now(timezone.utc)

            logger.info(f"ì²­í¬ ì™„ë£Œ: {completed_chunk.chunk_id}, "
                        f"ì €ì¥: {saved_count}ê°œ, ì²­í¬ë²”ìœ„: {completed_chunk.count}ê°œ, "
                        f"ëˆ„ì : {state.total_collected}/{state.total_requested}")

            # ğŸ†• ìƒì„¸í•œ ì²­í¬ ì²˜ë¦¬ ìš”ì•½ ì •ë³´ (ë””ë²„ê¹…ìš©)
            if logger.level <= 10:  # DEBUG ë ˆë²¨ì¼ ë•Œë§Œ
                summary = completed_chunk.get_processing_summary()
                logger.debug(f"\n{summary}")

            # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ í™•ì¸ (ìµœìš°ì„  ì¢…ë£Œ ì¡°ê±´)
            if state.reached_upbit_data_end:
                state.is_completed = True
                state.current_chunk = None
                logger.info(f"ğŸ”´ ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ë¡œ ìˆ˜ì§‘ ì™„ë£Œ: {request_id} - ìš”ì²­ ë²”ìœ„ì— ì—…ë¹„íŠ¸ ë°ì´í„° ëì´ í¬í•¨ë¨")
                return True

            # ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸
            if self._is_collection_complete(state):
                state.is_completed = True
                state.current_chunk = None
                logger.info(f"ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {request_id}")
                return True

            # ë‹¤ìŒ ì²­í¬ ìƒì„±
            self._prepare_next_chunk(state, request_type)
            return False

        except Exception as e:
            # ì²­í¬ ì‹¤íŒ¨ ì²˜ë¦¬
            if state.current_chunk:
                state.current_chunk.status = "failed"
                state.error_message = str(e)
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {state.current_chunk.chunk_id if state.current_chunk else 'Unknown'}, ì˜¤ë¥˜: {e}")
            raise

    async def _process_chunk_direct_storage(
        self,
        chunk_info: ChunkInfo,
        state: CollectionState,
        is_first_chunk: bool,
        request_type: RequestType
    ) -> tuple[int, Optional[str]]:
        """ì„±ëŠ¥ ìµœì í™”ëœ ì²­í¬ ì²˜ë¦¬ - ì§ì ‘ ì €ì¥ ë°©ì‹

        Returns:
            tuple[int, Optional[str]]: (saved_count, last_candle_time_str)
        """

        # ğŸš€ ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ê³„ì‚° (ì²« ì²­í¬ ~ í˜„ì¬ ì²­í¬)
        safe_range_start = None
        safe_range_end = None
        if state.completed_chunks and chunk_info.end:
            # ì²« ë²ˆì§¸ ì™„ë£Œëœ ì²­í¬ì˜ ì‹œì‘ì 
            safe_range_start = state.completed_chunks[0].to
            # í˜„ì¬ ì²­í¬ì˜ ëì 
            safe_range_end = chunk_info.end
            logger.debug(f"ğŸ”’ ì•ˆì „ ë²”ìœ„ ê³„ì‚°: [{safe_range_start}, {safe_range_end}]")

        # ê²¹ì¹¨ ë¶„ì„ (API ì ˆì•½ íš¨ê³¼ ìœ ì§€)
        overlap_result = None
        chunk_end = None
        if not (is_first_chunk and request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]):
            chunk_start = chunk_info.to
            chunk_end = self._calculate_chunk_end_time(chunk_info)

            # ğŸ” ë””ë²„ê¹…: ê²¹ì¹¨ ë¶„ì„ì—ì„œì˜ chunk_end
            logger.debug(f"ğŸ” ê²¹ì¹¨ ë¶„ì„ chunk_end: {chunk_end}")

            overlap_result = await self._analyze_chunk_overlap(
                state.symbol, state.timeframe, chunk_start, chunk_end
            )

        if overlap_result and hasattr(overlap_result, 'status'):
            # ğŸŸ¢ ê°œì„ : ChunkInfoì— overlap ì •ë³´ ì €ì¥ (í†µí•© ê´€ë¦¬)
            chunk_info.set_overlap_info(overlap_result)

            # ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì§ì ‘ ì €ì¥ (ChunkInfo ê¸°ë°˜ìœ¼ë¡œ last_candle_time ë¶ˆí•„ìš”)
            saved_count, last_candle_time = await self._handle_overlap_direct_storage(
                chunk_info, overlap_result, state, chunk_end, is_first_chunk,
                safe_range_start, safe_range_end
            )
        else:
            # í´ë°±: ì§ì ‘ API â†’ ì €ì¥ (COUNT_ONLY/END_ONLY ì²« ì²­í¬ í¬í•¨)
            api_response = await self._fetch_chunk_from_api(chunk_info)

            # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
            api_count, _ = chunk_info.get_api_params()
            if len(api_response) < api_count:
                state.reached_upbit_data_end = True
                logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ (í´ë°±): {chunk_info.symbol} {chunk_info.timeframe} - "
                               f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

            # ğŸš€ ì²« ì²­í¬ì—ì„œë„ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í—ˆìš© (EmptyCandleDetector ë‚´ë¶€ ì•ˆì „ ì²˜ë¦¬ ë¡œì§ ì ìš©)
            if is_first_chunk:
                logger.debug("ì²« ì²­í¬: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì ìš©")
                # ì²« ì²­í¬ë¥¼ ìœ„í•œ ì•ˆì „ ë²”ìœ„ ì„¤ì •
                first_chunk_safe_start = chunk_info.to  # ì²­í¬ ì‹œì‘ì 
                first_chunk_safe_end = chunk_info.end   # ì²­í¬ ëì 

                final_candles = await self._process_api_candles_with_empty_filling(
                    api_response,
                    state.symbol,
                    state.timeframe,
                    api_start=chunk_info.to,
                    api_end=chunk_info.end,
                    safe_range_start=first_chunk_safe_start,
                    safe_range_end=first_chunk_safe_end,
                    is_first_chunk=True  # ğŸš€ ì²« ì²­í¬ì„ì„ ëª…ì‹œ (api_start +1í‹± ì¶”ê°€ ë°©ì§€)
                )
                # ğŸ†• ìµœì¢… ìº”ë“¡ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì •
                chunk_info.set_final_candle_info(final_candles)
                logger.info(f"ì²« ì²­í¬ ë¹ˆ ìº”ë“¡ ì²˜ë¦¬ ì™„ë£Œ: {len(api_response)}ê°œ â†’ {len(final_candles)}ê°œ")
            else:
                logger.debug("í´ë°± ì¼€ì´ìŠ¤: api_end ì •ë³´ ì—†ìŒ â†’ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°")
                final_candles = api_response
                # ğŸ†• ìµœì¢… ìº”ë“¤ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì • (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì—†ì´)
                chunk_info.set_final_candle_info(final_candles)

            saved_count = await self.repository.save_raw_api_data(
                state.symbol, state.timeframe, final_candles
            )
            # âœ… ChunkInfoì—ì„œ ì‹œê°„ ì •ë³´ ìë™ ì¶”ì¶œ (get_effective_end_time í™œìš©)
            last_candle_time = None  # ChunkInfoì—ì„œ ì²˜ë¦¬ë¨

        return saved_count, last_candle_time

    async def _handle_overlap_direct_storage(
        self,
        chunk_info: ChunkInfo,
        overlap_result,
        state: CollectionState,
        calculated_chunk_end: Optional[datetime] = None,
        is_first_chunk: bool = False,
        safe_range_start: Optional[datetime] = None,
        safe_range_end: Optional[datetime] = None
    ) -> tuple[int, Optional[str]]:
        """ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì§ì ‘ ì €ì¥ ì²˜ë¦¬

        Args:
            chunk_info: ì²­í¬ ì •ë³´
            overlap_result: ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼
            calculated_chunk_end: ê³„ì‚°ëœ ì²­í¬ ì¢…ë£Œ ì‹œê°„
            is_first_chunk: ì²« ë²ˆì§¸ ì²­í¬ ì—¬ë¶€ (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°ìš©)

        Returns:
            tuple[int, Optional[str]]: (saved_count, last_candle_time_str)
        """
        from upbit_auto_trading.infrastructure.market_data.candle.models import OverlapStatus

        status = overlap_result.status

        if status == OverlapStatus.COMPLETE_OVERLAP:
            # ì™„ì „ ê²¹ì¹¨: ì €ì¥í•  ê²ƒ ì—†ìŒ (ì´ë¯¸ DBì— ì¡´ì¬)
            logger.debug("ì™„ì „ ê²¹ì¹¨ â†’ ì €ì¥ ìƒëµ")
            # ğŸ”„ ChunkInfoì—ì„œ ìë™ ì²˜ë¦¬: calculated_chunk_endë¥¼ final_candle_endë¡œ ì„¤ì •
            if calculated_chunk_end:
                chunk_info.final_candle_end = calculated_chunk_end
            return 0, None  # ChunkInfo ê¸°ë°˜ ì²˜ë¦¬ë¡œ last_candle_time ë¶ˆí•„ìš”

        elif status == OverlapStatus.NO_OVERLAP:
            # ê²¹ì¹¨ ì—†ìŒ: API â†’ ì§ì ‘ ì €ì¥
            logger.debug("ê²¹ì¹¨ ì—†ìŒ â†’ ì „ì²´ API ì§ì ‘ ì €ì¥")
            api_response = await self._fetch_chunk_from_api(chunk_info)

            # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
            api_count, _ = chunk_info.get_api_params()
            if len(api_response) < api_count:
                state.reached_upbit_data_end = True
                logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬: {chunk_info.symbol} {chunk_info.timeframe} - "
                               f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

            # ğŸš€ ì²« ì²­í¬ì—ì„œë„ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í—ˆìš© (NO_OVERLAP)
            # overlap_resultì—ì„œ api_start, api_end ì¶”ì¶œ
            api_start = overlap_result.api_start if hasattr(overlap_result, 'api_start') else None
            api_end = overlap_result.api_end if hasattr(overlap_result, 'api_end') else None

            # ğŸ” ì¡°ê±´ë¶€ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: API ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìº”ë“¤ê³¼ api_endê°€ ë‹¤ë¥¼ ë•Œë§Œ
            if self._should_process_empty_candles(api_response, api_end):
                final_candles = await self._process_api_candles_with_empty_filling(
                    api_response, chunk_info.symbol, chunk_info.timeframe, api_start, api_end,
                    safe_range_start, safe_range_end, is_first_chunk=is_first_chunk
                )
            else:
                final_candles = api_response

            # ğŸ†• ìµœì¢… ìº”ë“¤ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì •
            chunk_info.set_final_candle_info(final_candles)

            saved_count = await self.repository.save_raw_api_data(
                chunk_info.symbol, chunk_info.timeframe, final_candles
            )
            # ğŸ”„ ChunkInfo ìë™ ì²˜ë¦¬: calculated_chunk_end ì„¤ì •
            if calculated_chunk_end:
                chunk_info.final_candle_end = calculated_chunk_end
            # ğŸ”„ ì²­í¬ ë ì‹œê°„ ìš°ì„  ì‚¬ìš© (ë¹ˆ ìº”ë“¤ê³¼ ë¬´ê´€í•œ ì—°ì†ì„± ë³´ì¥)
            last_candle_time = None
            if calculated_chunk_end:
                last_candle_time = TimeUtils.format_datetime_utc(calculated_chunk_end)
            else:
                last_candle_time = None  # ChunkInfoì—ì„œ ì²˜ë¦¬ë¨
            return saved_count, last_candle_time

        elif status in [OverlapStatus.PARTIAL_START, OverlapStatus.PARTIAL_MIDDLE_CONTINUOUS]:
            # ë¶€ë¶„ ê²¹ì¹¨: API ë¶€ë¶„ë§Œ ì €ì¥ (DB ë¶€ë¶„ì€ ì´ë¯¸ ì¡´ì¬)
            logger.debug(f"ë¶€ë¶„ ê²¹ì¹¨ ({status.value}) â†’ API ë¶€ë¶„ë§Œ ì§ì ‘ ì €ì¥")

            if overlap_result.api_start and overlap_result.api_end:
                # ğŸŸ¢ ê°œì„ : ChunkInfoì— overlap ì •ë³´ ì„¤ì • (temp_chunk ìƒì„± ì œê±°)
                api_count = self._calculate_api_count(
                    overlap_result.api_start,
                    overlap_result.api_end,
                    chunk_info.timeframe
                )

                chunk_info.set_overlap_info(overlap_result, api_count)
                api_response = await self._fetch_chunk_from_api(chunk_info)

                # ğŸ†• ë¶€ë¶„ ê²¹ì¹¨ì—ì„œë„ ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€ (API ë¶€ë¶„ ìš”ì²­ì—ì„œë„ ë°œìƒ ê°€ëŠ¥)
                if len(api_response) < api_count:
                    state.reached_upbit_data_end = True
                    logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ (ë¶€ë¶„ê²¹ì¹¨): {chunk_info.symbol} {chunk_info.timeframe} - "
                                   f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

                # ğŸš€ ì²« ì²­í¬ì—ì„œë„ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í—ˆìš© (PARTIAL_OVERLAP)
                # overlap_resultì—ì„œ api_start, api_end ì¶”ì¶œ
                api_start = overlap_result.api_start if hasattr(overlap_result, 'api_start') else None
                api_end = overlap_result.api_end if hasattr(overlap_result, 'api_end') else None

                # ğŸ” ì¡°ê±´ë¶€ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: API ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìº”ë“¤ê³¼ api_endê°€ ë‹¤ë¥¼ ë•Œë§Œ
                if self._should_process_empty_candles(api_response, api_end):
                    final_candles = await self._process_api_candles_with_empty_filling(
                        api_response, chunk_info.symbol, chunk_info.timeframe, api_start, api_end,
                        safe_range_start, safe_range_end, is_first_chunk=is_first_chunk
                    )
                else:
                    final_candles = api_response

                # ğŸ†• ìµœì¢… ìº”ë“¤ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì •
                chunk_info.set_final_candle_info(final_candles)

                saved_count = await self.repository.save_raw_api_data(
                    chunk_info.symbol, chunk_info.timeframe, final_candles
                )
                # ğŸ”„ ì²­í¬ ë ì‹œê°„ ìš°ì„  ì‚¬ìš© (ë¹ˆ ìº”ë“¤ê³¼ ë¬´ê´€í•œ ì—°ì†ì„± ë³´ì¥)
                last_candle_time = None
                if calculated_chunk_end:
                    last_candle_time = TimeUtils.format_datetime_utc(calculated_chunk_end)
                else:
                    # âœ… ChunkInfoì—ì„œ ì²˜ë¦¬ë¨
                    last_candle_time = None
                return saved_count, last_candle_time
            # API ì •ë³´ ì—†ìœ¼ë©´ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
            last_candle_time = None
            if calculated_chunk_end:
                last_candle_time = TimeUtils.format_datetime_utc(calculated_chunk_end)
            return 0, last_candle_time

        else:
            # PARTIAL_MIDDLE_FRAGMENT ë˜ëŠ” ê¸°íƒ€: ì•ˆì „í•œ í´ë°± â†’ ì „ì²´ API ì €ì¥
            logger.debug("ë³µì¡í•œ ê²¹ì¹¨ â†’ ì „ì²´ API ì§ì ‘ ì €ì¥ í´ë°±")
            api_response = await self._fetch_chunk_from_api(chunk_info)

            # ğŸ†• ë³µì¡í•œ ê²¹ì¹¨ í´ë°±ì—ì„œë„ ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ê°ì§€
            api_count, _ = chunk_info.get_api_params()
            if len(api_response) < api_count:
                state.reached_upbit_data_end = True
                logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ (í´ë°±): {chunk_info.symbol} {chunk_info.timeframe} - "
                               f"ìš”ì²­={api_count}ê°œ, ì‘ë‹µ={len(api_response)}ê°œ")

            # ğŸ†• ë³µì¡í•œ ê²¹ì¹¨ í´ë°±: api_end ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸° (ì•ˆì „ì„±)
            # ì²« ì²­í¬ì™€ ê´€ê³„ì—†ì´ api_end ì •ë³´ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ì•ˆì „í•œ í´ë°±
            logger.debug(f"ë³µì¡í•œ ê²¹ì¹¨ í´ë°±: api_end ì •ë³´ ì—†ìŒ â†’ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸° (is_first_chunk={is_first_chunk})")
            final_candles = api_response

            saved_count = await self.repository.save_raw_api_data(
                chunk_info.symbol, chunk_info.timeframe, final_candles
            )
            # ğŸ”„ ì²­í¬ ë ì‹œê°„ ìš°ì„  ì‚¬ìš© (ë¹ˆ ìº”ë“¤ê³¼ ë¬´ê´€í•œ ì—°ì†ì„± ë³´ì¥)
            last_candle_time = None
            if calculated_chunk_end:
                last_candle_time = TimeUtils.format_datetime_utc(calculated_chunk_end)
            else:
                last_candle_time = None  # ChunkInfoì—ì„œ ì²˜ë¦¬ë¨
            return saved_count, last_candle_time

    # =========================================================================
    # ê³„íš ìˆ˜ë¦½
    # =========================================================================

    def plan_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> CollectionPlan:
        """ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½"""
        # RequestInfo ìƒì„± (ê²€ì¦ í¬í•¨)
        request_info = RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=to,
            end=end
        )

        logger.info(f"ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½: {request_info.to_internal_log_string()}")

        # ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ - ìš”ì²­ ì‹œì  ê¸°ì¤€
        if to is not None and to > request_info.request_at:
            raise ValueError(f"to ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {to}")
        if end is not None and end > request_info.request_at:
            raise ValueError(f"end ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {end}")

        # ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
        total_count = self._calculate_total_count_by_type(request_info)

        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚°
        estimated_chunks = (total_count + self.chunk_size - 1) // self.chunk_size

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (10 RPS ê¸°ì¤€)
        estimated_duration_seconds = estimated_chunks / self.api_rate_limit_rps

        # ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±
        first_chunk_params = self._create_first_chunk_params_by_type(request_info)

        plan = CollectionPlan(
            total_count=total_count,
            estimated_chunks=estimated_chunks,
            estimated_duration_seconds=estimated_duration_seconds,
            first_chunk_params=first_chunk_params
        )

        logger.info(f"ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {total_count:,}ê°œ ìº”ë“¤, {estimated_chunks}ì²­í¬, "
                    f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {estimated_duration_seconds:.1f}ì´ˆ")
        return plan

    # =========================================================================
    # ì²­í¬ ì²˜ë¦¬ í•µì‹¬ ë¡œì§
    # =========================================================================

    async def _fetch_chunk_from_api(self, chunk_info: ChunkInfo) -> List[Dict[str, Any]]:
        """ì‹¤ì œ API í˜¸ì¶œì„ í†µí•œ ì²­í¬ ë°ì´í„° ìˆ˜ì§‘ - Overlap ìµœì í™” ì§€ì›

        Returns:
            List[Dict[str, Any]]: ìº”ë“¤ ë°ì´í„°
        """
        logger.debug(f"API ì²­í¬ ìš”ì²­: {chunk_info.chunk_id}")

        # ğŸŸ¢ ê°œì„ : ChunkInfoì—ì„œ ìµœì í™”ëœ API íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        api_count, api_to = chunk_info.get_api_params()

        if chunk_info.has_overlap_info() and chunk_info.needs_partial_api_call():
            logger.debug(f"ë¶€ë¶„ API í˜¸ì¶œ: count={api_count}, to={api_to} (overlap ìµœì í™”)")
        else:
            logger.debug(f"ì „ì²´ API í˜¸ì¶œ: count={api_count}, to={api_to}")

        try:
            # íƒ€ì„í”„ë ˆì„ë³„ API ë©”ì„œë“œ ì„ íƒ
            if chunk_info.timeframe == '1s':
                # ì´ˆë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    timeframe_delta = TimeUtils.get_timeframe_delta(chunk_info.timeframe)
                    fetch_time = api_to + timeframe_delta
                    to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_seconds(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe.endswith('m'):
                # ë¶„ë´‰
                unit = int(chunk_info.timeframe[:-1])
                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ë´‰ ë‹¨ìœ„: {unit}")

                # ë¶„ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    timeframe_delta = TimeUtils.get_timeframe_delta(chunk_info.timeframe)
                    fetch_time = api_to + timeframe_delta
                    to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit,
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1d':
                # ì¼ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_days(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1w':
                # ì£¼ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1M':
                # ì›”ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_months(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1y':
                # ì—°ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_years(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk_info.timeframe}")

            # ğŸ†• API ì‘ë‹µ ì •ë³´ë¥¼ ChunkInfoì— ì„¤ì •
            chunk_info.set_api_response_info(candles)

            #  ê°œì„ : ìµœì í™”ëœ ë¡œê¹… (overlap ì •ë³´ í‘œì‹œ)
            overlap_info = f" (overlap: {chunk_info.overlap_status.value})" if chunk_info.has_overlap_info() else ""
            logger.info(f"API ì²­í¬ ì™„ë£Œ: {chunk_info.chunk_id}, ìˆ˜ì§‘: {len(candles)}ê°œ{overlap_info}")

            return candles

        except Exception as e:
            logger.error(f"API ì²­í¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    async def _analyze_chunk_overlap(
        self,
        symbol: str,
        timeframe: str,
        start_time: datetime,
        end_time: datetime
    ):
        """OverlapAnalyzerë¥¼ í™œìš©í•œ ì²­í¬ ê²¹ì¹¨ ë¶„ì„"""
        logger.debug(f"ê²¹ì¹¨ ë¶„ì„: {symbol} {timeframe}")

        try:
            from upbit_auto_trading.infrastructure.market_data.candle.models import OverlapRequest

            # ğŸš€ UTC í†µì¼: ì§„ì…ì ì—ì„œ ì •ê·œí™”ë˜ì–´ ë” ì´ìƒ ê²€ì¦ ë¶ˆí•„ìš”
            safe_start_time = start_time
            safe_end_time = end_time

            # ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
            expected_count = self._calculate_api_count(safe_start_time, safe_end_time, timeframe)

            overlap_request = OverlapRequest(
                symbol=symbol,
                timeframe=timeframe,
                target_start=safe_start_time,
                target_end=safe_end_time,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)
            logger.debug(f"ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼: {overlap_result.status.value}")

            return overlap_result

        except Exception as e:
            logger.warning(f"ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    # =========================================================================
    # íŒŒë¼ë¯¸í„° ìƒì„± ë° ìœ í‹¸ë¦¬í‹°
    # =========================================================================

    def _calculate_total_count_by_type(
        self,
        request_info: RequestInfo
    ) -> int:
        """ì‚¬ì „ ê³„ì‚°ëœ ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ë°˜í™˜ (í•­ìƒ ì¡´ì¬ ë³´ì¥)"""
        return request_info.get_expected_count()

    def _create_first_chunk_params_by_type(
        self,
        request_info: RequestInfo
    ) -> Dict[str, Any]:
        """ìš”ì²­ íƒ€ì…ë³„ ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±"""
        request_type = request_info.get_request_type()
        params = {"market": request_info.symbol}

        if request_type == RequestType.COUNT_ONLY:
            # COUNT_ONLY: to íŒŒë¼ë¯¸í„° ì—†ì´ countë§Œ ì‚¬ìš©
            chunk_size = min(request_info.count, self.chunk_size)
            params["count"] = chunk_size
            logger.debug(f"COUNT_ONLY: count={chunk_size} (ì„œë²„ ìµœì‹ ë¶€í„°)")

        elif request_type == RequestType.TO_COUNT:
            # to + count: ì‚¬ì „ ê³„ì‚°ëœ ì •ë ¬ ì‹œê°„ ì‚¬ìš©
            chunk_size = min(request_info.count, self.chunk_size)
            aligned_to = request_info.get_aligned_to_time()

            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            first_chunk_start_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

            params["count"] = chunk_size
            params["to"] = first_chunk_start_time
            logger.debug(f"TO_COUNT: ì§„ì…ì ë³´ì • {aligned_to} â†’ {first_chunk_start_time}")

        elif request_type == RequestType.TO_END:
            # to + end: ì‚¬ì „ ê³„ì‚°ëœ ì •ë ¬ ì‹œê°„ ì‚¬ìš©
            aligned_to = request_info.get_aligned_to_time()

            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            first_chunk_start_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

            params["count"] = self.chunk_size
            params["to"] = first_chunk_start_time
            logger.debug(f"TO_END: ì§„ì…ì ë³´ì • {aligned_to} â†’ {first_chunk_start_time}")

        elif request_type == RequestType.END_ONLY:
            # END_ONLY: COUNT_ONLYì²˜ëŸ¼ to ì—†ì´ countë§Œ ì‚¬ìš©
            params["count"] = self.chunk_size
            logger.debug(f"END_ONLY: count={self.chunk_size} (ì„œë²„ ìµœì‹ ë¶€í„°)")

        return params

    def _create_next_chunk(
        self,
        collection_state: CollectionState,
        chunk_params: Dict[str, Any],
        chunk_index: int
    ) -> ChunkInfo:
        """ë‹¤ìŒ ì²­í¬ ì •ë³´ ìƒì„±"""
        chunk_id = f"{collection_state.symbol}_{collection_state.timeframe}_{chunk_index:05d}"

        # 'to' íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        if "to" in chunk_params and chunk_params["to"]:
            to_param = chunk_params["to"]
            if isinstance(to_param, datetime):
                to_datetime = to_param
            elif isinstance(to_param, str):
                try:
                    # ğŸš€ UTC í†µì¼: ë‹¨ìˆœí•œ fromisoformat (ì§„ì…ì ì—ì„œ ì´ë¯¸ ì •ê·œí™”ë¨)
                    to_datetime = datetime.fromisoformat(to_param)
                except (ValueError, TypeError):
                    logger.warning(f"'to' íŒŒë¼ë¯¸í„° íŒŒì‹± ì‹¤íŒ¨: {to_param}")
                    to_datetime = None
            else:
                logger.warning(f"'to' íŒŒë¼ë¯¸í„° íƒ€ì… ì˜¤ë¥˜: {type(to_param)}")
                to_datetime = None
        else:
            to_datetime = None  # COUNT_ONLYëŠ” None ìœ ì§€

        # end ì‹œê°„ ê³„ì‚° - ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë
        calculated_end = None
        if to_datetime and chunk_params.get("count"):
            # toì™€ countê°€ ìˆìœ¼ë©´ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë ê³„ì‚°
            calculated_end = TimeUtils.get_time_by_ticks(
                to_datetime,
                collection_state.timeframe,
                -(chunk_params["count"] - 1)
            )
        else:
            # COUNT_ONLY, END_ONLY ê²½ìš° None ìœ ì§€ (í˜„ì¬ ì‹œê°„ ì‚¬ìš© ì•ˆ í•¨)
            calculated_end = None

        chunk_info = ChunkInfo(
            chunk_id=chunk_id,
            chunk_index=chunk_index,
            symbol=collection_state.symbol,
            timeframe=collection_state.timeframe,
            count=chunk_params["count"],
            to=to_datetime,
            end=calculated_end,
            status="pending"
        )

        return chunk_info

    def _is_collection_complete(self, state: CollectionState) -> bool:
        """ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸ - í†µí•© ë©”ì„œë“œ í™œìš©"""
        # ğŸ†• í†µí•©ëœ ì™„ë£Œ ì¡°ê±´ ì²´í¬ í™œìš©
        completion_info = state.get_completion_check_info()

        # ì™„ë£Œ ì¡°ê±´ í™•ì¸
        count_reached = completion_info['count_info']['count_reached']
        time_reached = completion_info['time_info']['time_reached']
        upbit_data_end = completion_info['upbit_info']['reached_data_end']

        # ìƒì„¸ ë¡œê¹… (ì¡°ê±´ë³„)
        completion_reasons = []
        if count_reached:
            completion_reasons.append("ê°œìˆ˜ë‹¬ì„±")
            logger.debug(f"ê°œìˆ˜ ë‹¬ì„±: {completion_info['count_info']['collected']}/{completion_info['count_info']['requested']}")

        if time_reached:
            completion_reasons.append("ChunkInfoì‹œê°„ë„ë‹¬")
            request_type = state.request_info.get_request_type()
            logger.debug(f"ì‹œê°„ ë„ë‹¬ (ChunkInfo, {request_type.value}): "
                        f"effective_end={completion_info['time_info']['last_processed']}, "
                        f"target_end={completion_info['time_info']['target_end']}, "
                        f"ì¶œì²˜={completion_info['time_info']['time_source']}")

        if upbit_data_end:
            completion_reasons.append("ì—…ë¹„íŠ¸ë°ì´í„°ë")
            logger.debug("ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬")

        # í†µí•© ì™„ë£Œ íŒì •
        should_complete = count_reached or time_reached or upbit_data_end

        if should_complete:
            logger.debug(f"ğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ: {', '.join(completion_reasons)}")

            # ì™„ë£Œ ìƒì„¸ ì •ë³´ ì¶œë ¥ (DEBUG)
            if logger.level <= 10:
                import json
                logger.debug(f"ì™„ë£Œ ì¡°ê±´ ìƒì„¸ ì •ë³´:")
                logger.debug(f"  {json.dumps(completion_info, indent=2, default=str)}")

        return should_complete

    def _prepare_next_chunk(self, state: CollectionState, request_type: RequestType):
        """ë‹¤ìŒ ì²­í¬ ì¤€ë¹„"""
        next_chunk_index = len(state.completed_chunks)
        remaining_count = state.total_requested - state.total_collected
        next_chunk_size = min(remaining_count, self.chunk_size)

        next_chunk_params = self._create_next_chunk_params(
            state, next_chunk_size, request_type
        )

        next_chunk = self._create_next_chunk(
            collection_state=state,
            chunk_params=next_chunk_params,
            chunk_index=next_chunk_index
        )
        state.current_chunk = next_chunk

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ìƒì„±: {next_chunk.chunk_id}")

    def _create_next_chunk_params(
        self,
        state: CollectionState,
        chunk_size: int,
        request_type: RequestType
    ) -> Dict[str, Any]:
        """ë‹¤ìŒ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± - ì—°ì†ì„± ë³´ì¥"""
        params = {
            "market": state.symbol,
            "count": chunk_size
        }

        # ğŸ†• ChunkInfo ê¸°ë°˜ ì—°ì†ì„± (ëª¨ë“  ì²­í¬ íƒ€ì…ì—ì„œ ì™„ì „í•œ ì‹œê°„ ì •ë³´ ì§€ì›)
        last_effective_time = state.get_last_effective_time_datetime()
        if last_effective_time:
            try:
                # ë‹¤ìŒ ì²­í¬ ì‹œì‘ = ì´ì „ ì²­í¬ ìœ íš¨ ëì‹œê°„ - 1í‹± (ì—°ì†ì„± ë³´ì¥)
                next_chunk_start = TimeUtils.get_time_by_ticks(last_effective_time, state.timeframe, -1)
                params["to"] = next_chunk_start

                time_source = state.get_last_time_source()
                logger.debug(f"ChunkInfo ì—°ì†ì„±: {last_effective_time} (ì¶œì²˜: {time_source}) â†’ {next_chunk_start}")

            except Exception as e:
                logger.warning(f"ChunkInfo ì—°ì†ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")

        return params

    def _update_remaining_time_estimates(self, state: CollectionState):
        """ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¶”ì • ì—…ë°ì´íŠ¸

        Note: avg_chunk_duration, remaining_chunks, estimated_remaining_secondsëŠ”
        ì´ì œ CollectionStateì˜ @propertyë¡œ ìë™ ê³„ì‚°ë˜ë¯€ë¡œ ìˆ˜ë™ ì—…ë°ì´íŠ¸ ë¶ˆí•„ìš”.
        last_update_timeë§Œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        state.last_update_time = datetime.now(timezone.utc)

    # =========================================================================
    # í—¬í¼ ë©”ì„œë“œë“¤
    # =========================================================================

    def _calculate_chunk_end_time(self, chunk_info: ChunkInfo) -> datetime:
        """ì²­í¬ ìš”ì²­ì˜ ì˜ˆìƒ ì¢…ë£Œ ì‹œì  ê³„ì‚°"""
        ticks = -(chunk_info.count - 1)
        end_time = TimeUtils.get_time_by_ticks(chunk_info.to, chunk_info.timeframe, ticks)

        # ğŸ” ë””ë²„ê¹…: ì²­í¬ ê²½ê³„ ê³„ì‚° ê³¼ì • ë¡œê¹…
        logger.debug(f"ğŸ” ì²­í¬ ê²½ê³„ ê³„ì‚°: to={chunk_info.to}, count={chunk_info.count}, "
                     f"ticks={ticks}, calculated_end={end_time}")

        return end_time

    def _calculate_api_count(self, start_time: datetime, end_time: datetime, timeframe: str) -> int:
        """API ìš”ì²­ì— í•„ìš”í•œ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°"""
        return TimeUtils.calculate_expected_count(start_time, end_time, timeframe)

    def _should_process_empty_candles(self, api_response: List[Dict[str, Any]], api_end: Optional[datetime]) -> bool:
        """API ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ê³¼ api_end ë¹„êµí•˜ì—¬ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨

        Args:
            api_response: ì—…ë¹„íŠ¸ API ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
            api_end: ì˜ˆìƒë˜ëŠ” ì²­í¬ ì¢…ë£Œ ì‹œê°„

        Returns:
            bool: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        if not api_response or not api_end:
            logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: api_response ë˜ëŠ” api_endê°€ ì—†ìŒ â†’ ì²˜ë¦¬ ì•ˆ í•¨")
            return False

        try:
            # ì—…ë¹„íŠ¸ APIëŠ” ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ ë§ˆì§€ë§‰ ìš”ì†Œê°€ ê°€ì¥ ê³¼ê±° ìº”ë“¤
            last_candle = api_response[-1]
            candle_time_utc = last_candle.get('candle_date_time_utc')

            if candle_time_utc and isinstance(candle_time_utc, str):
                # ğŸš€ UTC í†µì¼: TimeUtilsë¥¼ í†µí•œ í‘œì¤€ ì •ê·œí™” (aware datetime ë³´ì¥)
                parsed_time = datetime.fromisoformat(candle_time_utc)
                last_candle_time = TimeUtils.normalize_datetime_to_utc(parsed_time)

                # ğŸš€ UTC í†µì¼: ë™ì¼í•œ í˜•ì‹(aware datetime) ê°„ ë¹„êµë¡œ ì •í™•ì„± ë³´ì¥
                needs_processing = last_candle_time != api_end

                if needs_processing:
                    logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time} vs api_end={api_end}")
                else:
                    logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë¶ˆí•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time} == api_end={api_end}")

                return needs_processing

        except Exception as e:
            logger.warning(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨: {e} â†’ ì•ˆì „í•œ í´ë°±ìœ¼ë¡œ ì²˜ë¦¬ ì•ˆ í•¨")
            return False

        logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: ìº”ë“¤ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ â†’ ì²˜ë¦¬ ì•ˆ í•¨")
        return False
