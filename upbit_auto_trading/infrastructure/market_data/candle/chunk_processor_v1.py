"""
ChunkProcessor v1.1 - ìº”ë“¤ ì²­í¬ ì²˜ë¦¬ ì „ë¬¸ í´ë˜ìŠ¤ (Legacy ë¡œì§ í†µí•©)

Created: 2025-09-23
Updated: ì²« ì²­í¬ êµ¬ë¶„, ì•ˆì „í•œ ë²”ìœ„ ê³„ì‚°, ë¹ˆ ìº”ë“¤ ì²˜ë¦¬, ì—…ë¹„íŠ¸ ë°ì´í„° ë ê°ì§€ ì¶”ê°€
Purpose: CandleDataProviderì—ì„œ ì²­í¬ ì²˜ë¦¬ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ ë‹¨ì¼ ì±…ì„ ì›ì¹™ ì¤€ìˆ˜
Features: 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬, ì¡°ê¸° ì¢…ë£Œ ìµœì í™”, ì„±ëŠ¥ ì¶”ì , Legacy í˜¸í™˜ì„±
Architecture: DDD Infrastructure ê³„ì¸µ, ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´

ì±…ì„:
- ê°œë³„ ì²­í¬ì˜ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
- API í˜¸ì¶œ ìµœì í™” ë° ê²¹ì¹¨ ë¶„ì„
- ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë° ë°ì´í„° ì •ê·œí™” (ì²« ì²­í¬ ì²˜ë¦¬ í¬í•¨)
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—ëŸ¬ ì²˜ë¦¬
- ì—…ë¹„íŠ¸ ë°ì´í„° ë ê°ì§€ ë° CollectionState ì—…ë°ì´íŠ¸
"""

import time
from datetime import datetime, timezone
from typing import Optional, Dict, Any, Callable, List
from contextlib import contextmanager

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import OverlapAnalyzer
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import UpbitPublicClient
from upbit_auto_trading.domain.repositories.candle_repository_interface import CandleRepositoryInterface
from upbit_auto_trading.infrastructure.market_data.candle.models import (
    ChunkInfo,
    CollectionState,
    ExecutionPlan,
    OverlapAnalysis,
    ValidationResult,
    ApiResponse,
    ProcessedData,
    StorageResult,
    ChunkResult,

    create_skip_result,
    create_early_exit_result,
    create_success_result,
    create_error_result,
)


class PerformanceTracker:
    """ì²­í¬ ì²˜ë¦¬ ì„±ëŠ¥ ì¶”ì  í´ë˜ìŠ¤"""

    def __init__(self):
        self.metrics = {}
        self.logger = create_component_logger("PerformanceTracker")

    @contextmanager
    def measure_chunk_execution(self, chunk_id: str):
        """ì „ì²´ ì²­í¬ ì‹¤í–‰ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        try:
            yield
        finally:
            execution_time = time.time() - start_time
            self.metrics[f"{chunk_id}_total"] = execution_time
            self.logger.info(f"â±ï¸ ì²­í¬ ì‹¤í–‰ ì‹œê°„: {chunk_id} = {execution_time:.3f}ì´ˆ")

    @contextmanager
    def measure_phase(self, phase_name: str):
        """ê°œë³„ Phase ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        try:
            yield
        finally:
            phase_time = time.time() - start_time
            self.metrics[f"phase_{phase_name}"] = phase_time
            self.logger.debug(f"ğŸ“Š Phase ì‹œê°„: {phase_name} = {phase_time:.3f}ì´ˆ")

    def get_performance_report(self) -> Dict[str, float]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            import numpy as np
            np_mean = np.mean
        except ImportError:
            # numpyê°€ ì—†ìœ¼ë©´ ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©
            def np_mean(arr):
                return sum(arr) / len(arr) if arr else 0.0

        return {
            'avg_total_time': np_mean([v for k, v in self.metrics.items() if '_total' in k]) if self.metrics else 0.0,
            'avg_preparation_time': np_mean([
                v for k, v in self.metrics.items() if 'phase_preparation' in k
            ]) if self.metrics else 0.0,
            'avg_api_fetch_time': np_mean([
                v for k, v in self.metrics.items() if 'phase_api_fetch' in k
            ]) if self.metrics else 0.0,
            'avg_processing_time': np_mean([
                v for k, v in self.metrics.items() if 'phase_data_processing' in k
            ]) if self.metrics else 0.0,
            'avg_storage_time': np_mean([
                v for k, v in self.metrics.items() if 'phase_data_storage' in k
            ]) if self.metrics else 0.0,
        }


class ChunkProcessor:
    """
    ìº”ë“¤ ì²­í¬ ì²˜ë¦¬ ì „ë¬¸ í´ë˜ìŠ¤

    ì±…ì„:
    - ê°œë³„ ì²­í¬ì˜ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬
    - API í˜¸ì¶œ ìµœì í™” ë° ê²¹ì¹¨ ë¶„ì„
    - ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë° ë°ì´í„° ì •ê·œí™” (ì²« ì²­í¬ ì²˜ë¦¬ í¬í•¨)
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì—ëŸ¬ ì²˜ë¦¬
    - ì—…ë¹„íŠ¸ ë°ì´í„° ë ê°ì§€ ë° ìƒíƒœ ì—…ë°ì´íŠ¸

    4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸:
    1. Phase 1: ì¤€ë¹„ ë° ë¶„ì„ ë‹¨ê³„ (ê²¹ì¹¨ ë¶„ì„, ì‹¤í–‰ ê³„íš ìˆ˜ë¦½)
    2. Phase 2: ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„ (API í˜¸ì¶œ ìµœì í™”)
    3. Phase 3: ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„ (ë¹ˆ ìº”ë“¤ ì²˜ë¦¬, ì •ê·œí™”)
    4. Phase 4: ë°ì´í„° ì €ì¥ ë‹¨ê³„ (Repositoryë¥¼ í†µí•œ ì˜êµ¬ ì €ì¥)
    """

    def __init__(self,
                 overlap_analyzer: OverlapAnalyzer,
                 upbit_client: UpbitPublicClient,
                 repository: CandleRepositoryInterface,
                 empty_candle_detector_factory: Callable[[str, str], Any],
                 enable_empty_candle_processing: bool = True,
                 performance_tracker: Optional[PerformanceTracker] = None):
        """ChunkProcessor ì´ˆê¸°í™”

        Args:
            overlap_analyzer: ë°ì´í„° ê²¹ì¹¨ ë¶„ì„ì„ ìœ„í•œ ë¶„ì„ê¸°
            upbit_client: ì—…ë¹„íŠ¸ API í´ë¼ì´ì–¸íŠ¸
            repository: ìº”ë“¤ ë°ì´í„° ì €ì¥ì†Œ ì¸í„°í˜ì´ìŠ¤
            empty_candle_detector_factory: ë¹ˆ ìº”ë“¤ ê°ì§€ê¸° íŒ©í† ë¦¬ í•¨ìˆ˜
            enable_empty_candle_processing: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í™œì„±í™” ì—¬ë¶€
            performance_tracker: ì„±ëŠ¥ ì¶”ì ê¸° (ì„ íƒì )
        """
        # ì™¸ë¶€ ì˜ì¡´ì„± ì£¼ì… (í…ŒìŠ¤íŠ¸ ìš©ì´ì„±)
        self.overlap_analyzer = overlap_analyzer
        self.upbit_client = upbit_client
        self.repository = repository
        self.empty_candle_detector_factory = empty_candle_detector_factory
        self.enable_empty_candle_processing = enable_empty_candle_processing

        # ì„±ëŠ¥ ì¶”ì  (ì„ íƒì )
        self.performance_tracker = performance_tracker or PerformanceTracker()

        # ë¡œê¹…
        self.logger = create_component_logger("ChunkProcessor")

        # ë‚´ë¶€ ìƒíƒœ
        self._cache = {}  # ê³„ì‚° ê²°ê³¼ ìºì‹±ìš©

        empty_status = 'í™œì„±í™”' if enable_empty_candle_processing else 'ë¹„í™œì„±í™”'
        self.logger.info(f"ChunkProcessor v1.1 ì´ˆê¸°í™” ì™„ë£Œ (Legacy ë¡œì§ í†µí•©, ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: {empty_status})")

    async def execute_chunk_pipeline(self,
                                     chunk_info: ChunkInfo,
                                     collection_state: CollectionState) -> ChunkResult:
        """
        ğŸš€ ì²­í¬ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸ - ì „ì²´ íë¦„ì´ í•œëˆˆì— ë³´ì„

        Args:
            chunk_info: ì²˜ë¦¬í•  ì²­í¬ ì •ë³´
            collection_state: ì „ì²´ ìˆ˜ì§‘ ìƒíƒœ

        Returns:
            ChunkResult: ì²˜ë¦¬ ê²°ê³¼ (ì„±ê³µ/ì‹¤íŒ¨, ì €ì¥ ê°œìˆ˜, ë©”íƒ€ë°ì´í„°)
        """
        chunk_id = chunk_info.chunk_id
        self.logger.info(f"ğŸš€ ì²­í¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘: {chunk_id}")

        # ğŸ†• ì²« ì²­í¬ íŒë‹¨ ë¡œì§ ì¶”ê°€
        is_first_chunk = len(collection_state.completed_chunks) == 0

        # ğŸ†• ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ê³„ì‚° (ì²« ì²­í¬ ~ í˜„ì¬ ì²­í¬)
        safe_range_start = None
        safe_range_end = None
        if collection_state.completed_chunks and chunk_info.end:
            # ì²« ë²ˆì§¸ ì™„ë£Œëœ ì²­í¬ì˜ ì‹œì‘ì 
            safe_range_start = collection_state.completed_chunks[0].to
            # í˜„ì¬ ì²­í¬ì˜ ëì 
            safe_range_end = chunk_info.end
            self.logger.debug(f"ğŸ”’ ì•ˆì „ ë²”ìœ„ ê³„ì‚°: [{safe_range_start}, {safe_range_end}]")

        # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()

        with self.performance_tracker.measure_chunk_execution(chunk_id):
            try:
                # Phase 1: ğŸ“‹ ì¤€ë¹„ ë° ë¶„ì„ ë‹¨ê³„
                execution_plan = await self._phase1_prepare_execution(chunk_info)

                # ì¡°ê¸° ì¢…ë£Œ: ì™„ì „ ê²¹ì¹¨ì¸ ê²½ìš° API í˜¸ì¶œ ìƒëµ
                if execution_plan.should_skip_api_call:
                    return create_skip_result(execution_plan, chunk_info)

                # Phase 2: ğŸŒ ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
                api_response = await self._phase2_fetch_data(chunk_info, execution_plan)

                # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ í™•ì¸ (CollectionState ì—…ë°ì´íŠ¸)
                if api_response.has_upbit_data_end:
                    collection_state.reached_upbit_data_end = True
                    self.logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬: {chunk_info.symbol} {chunk_info.timeframe}")

                # ì¡°ê¸° ì¢…ë£Œ: ë¹ˆ ì‘ë‹µ ë˜ëŠ” ì—…ë¹„íŠ¸ ë°ì´í„° ë
                if api_response.requires_early_exit:
                    return create_early_exit_result(api_response, chunk_info)

                # Phase 3: âš™ï¸ ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„
                processed_data = await self._phase3_process_data(
                    api_response, chunk_info, is_first_chunk, safe_range_start, safe_range_end
                )

                # Phase 4: ğŸ’¾ ë°ì´í„° ì €ì¥ ë‹¨ê³„
                storage_result = await self._phase4_persist_data(processed_data, chunk_info)

                # âœ… ì„±ê³µ ê²°ê³¼ ìƒì„± (ì²˜ë¦¬ ì‹œê°„ ê³„ì‚° í¬í•¨)
                processing_time_ms = (time.time() - start_time) * 1000
                return create_success_result(storage_result, chunk_info.chunk_id, processing_time_ms)

            except Exception as e:
                self.logger.error(f"âŒ ì²­í¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {chunk_id}, ì˜¤ë¥˜: {e}")
                return create_error_result(e, chunk_info)

    # =========================================================================
    # Phase 1: ì¤€ë¹„ ë° ë¶„ì„ ë‹¨ê³„
    # =========================================================================

    async def _phase1_prepare_execution(self, chunk_info: ChunkInfo) -> ExecutionPlan:
        """
        ğŸ“‹ ì²­í¬ ì‹¤í–‰ ì¤€ë¹„ ë° ê²¹ì¹¨ ë¶„ì„

        ì±…ì„:
        - ê²¹ì¹¨ ìƒíƒœ ë¶„ì„
        - API í˜¸ì¶œ ì „ëµ ê²°ì •
        - ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
        """
        with self.performance_tracker.measure_phase('preparation'):
            self.logger.debug(f"ğŸ“‹ ì‹¤í–‰ ì¤€ë¹„: {chunk_info.chunk_id}")

            # ê²¹ì¹¨ ë¶„ì„ ìˆ˜í–‰
            overlap_analysis = await self._analyze_data_overlap(chunk_info)

            # ì‹¤í–‰ ê³„íš ìˆ˜ë¦½
            execution_plan = self._build_execution_plan(chunk_info, overlap_analysis)

            # ChunkInfo ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            if overlap_analysis and overlap_analysis.overlap_result:
                chunk_info.set_overlap_info(overlap_analysis.overlap_result)

            self.logger.debug(f"ğŸ“‹ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ: {execution_plan.strategy}")
            return execution_plan

    async def _analyze_data_overlap(self, chunk_info: ChunkInfo) -> Optional[OverlapAnalysis]:
        """
        ğŸ” ë°ì´í„° ê²¹ì¹¨ ìƒíƒœ ì •ë°€ ë¶„ì„

        í˜„ì¬ _analyze_chunk_overlap() ë©”ì„œë“œë¥¼ ê°œì„ :
        - ë” ëª…í™•í•œ ë©”ì„œë“œëª…
        - ë¶„ì„ ê²°ê³¼ë¥¼ êµ¬ì¡°í™”ëœ ê°ì²´ë¡œ ë°˜í™˜
        """
        if not chunk_info.to or not chunk_info.end:
            self.logger.debug("ê²¹ì¹¨ ë¶„ì„ ê±´ë„ˆë›°ê¸°: to ë˜ëŠ” end ì •ë³´ ì—†ìŒ")
            return None

        try:
            from upbit_auto_trading.infrastructure.market_data.candle.models import OverlapRequest

            # ì²­í¬ ì‹œê°„ ë²”ìœ„ ê³„ì‚°
            chunk_start = chunk_info.to
            chunk_end = self._calculate_chunk_end_time(chunk_info)

            self.logger.debug(f"ğŸ” ê²¹ì¹¨ ë¶„ì„ ë²”ìœ„: {chunk_start} ~ {chunk_end}")

            # ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
            expected_count = self._calculate_api_count(chunk_start, chunk_end, chunk_info.timeframe)

            # OverlapAnalyzerë¥¼ í†µí•œ ê²¹ì¹¨ ë¶„ì„
            overlap_request = OverlapRequest(
                symbol=chunk_info.symbol,
                timeframe=chunk_info.timeframe,
                target_start=chunk_start,
                target_end=chunk_end,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)

            return OverlapAnalysis(
                overlap_result=overlap_result,
                analysis_time=datetime.now(timezone.utc),
                optimization_applied=True,
                recommendations=[f"ë¶„ì„ ì „ëµ: {overlap_result.status.value}"]
            )

        except Exception as e:
            self.logger.warning(f"ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def _build_execution_plan(self, chunk_info: ChunkInfo,
                              overlap_analysis: Optional[OverlapAnalysis]) -> ExecutionPlan:
        """ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½"""

        # ê¸°ë³¸ API íŒŒë¼ë¯¸í„°
        base_params = {
            'count': chunk_info.count,
            'to': chunk_info.to
        }

        # ê²¹ì¹¨ ë¶„ì„ì´ ì—†ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° - ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        if not overlap_analysis or not overlap_analysis.overlap_result:
            return ExecutionPlan(
                strategy='full_fetch',
                should_skip_api_call=False,
                optimized_api_params=base_params,
                expected_data_range=(chunk_info.to, chunk_info.end) if chunk_info.end else (chunk_info.to, chunk_info.to)
            )

        # ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ì „ëµ ê²°ì •
        from upbit_auto_trading.infrastructure.market_data.candle.models import OverlapStatus

        overlap_result = overlap_analysis.overlap_result
        status = overlap_result.status

        if status == OverlapStatus.COMPLETE_OVERLAP:
            # ì™„ì „ ê²¹ì¹¨: API í˜¸ì¶œ ìƒëµ
            return ExecutionPlan(
                strategy='skip_complete_overlap',
                should_skip_api_call=True,
                optimized_api_params={},
                expected_data_range=(chunk_info.to, chunk_info.end) if chunk_info.end else (chunk_info.to, chunk_info.to)
            )

        elif status in [OverlapStatus.PARTIAL_START, OverlapStatus.PARTIAL_MIDDLE_CONTINUOUS]:
            # ë¶€ë¶„ ê²¹ì¹¨: ìµœì í™”ëœ API í˜¸ì¶œ
            if hasattr(overlap_result, 'api_start') and hasattr(overlap_result, 'api_end'):
                api_count = self._calculate_api_count(
                    overlap_result.api_start, overlap_result.api_end, chunk_info.timeframe
                )
                optimized_params = {
                    'count': api_count,
                    'to': overlap_result.api_start
                }
                return ExecutionPlan(
                    strategy='partial_fetch',
                    should_skip_api_call=False,
                    optimized_api_params=optimized_params,
                    expected_data_range=(overlap_result.api_start, overlap_result.api_end)
                )

        # ê²¹ì¹¨ ì—†ìŒ ë˜ëŠ” ê¸°íƒ€ ê²½ìš°: ì „ì²´ ê°€ì ¸ì˜¤ê¸°
        return ExecutionPlan(
            strategy='full_fetch',
            should_skip_api_call=False,
            optimized_api_params=base_params,
            expected_data_range=(chunk_info.to, chunk_info.end) if chunk_info.end else (chunk_info.to, chunk_info.to)
        )

    # =========================================================================
    # Phase 2: ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„
    # =========================================================================

    async def _phase2_fetch_data(self,
                                 chunk_info: ChunkInfo,
                                 execution_plan: ExecutionPlan) -> ApiResponse:
        """
        ğŸŒ ìµœì í™”ëœ API ë°ì´í„° ìˆ˜ì§‘

        ê°œì„ ì‚¬í•­:
        - ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ìµœì í™”ëœ API í˜¸ì¶œ
        - ì‘ë‹µ ë°ì´í„° ì¦‰ì‹œ ê²€ì¦
        - ì—…ë¹„íŠ¸ ë°ì´í„° ë ê°ì§€
        """
        with self.performance_tracker.measure_phase('api_fetch'):
            self.logger.debug(f"ğŸŒ API ë°ì´í„° ìˆ˜ì§‘: {chunk_info.chunk_id}")

            # ìµœì í™”ëœ API íŒŒë¼ë¯¸í„° ì‚¬ìš©
            api_params = execution_plan.get_optimized_api_params()

            # API í˜¸ì¶œ (ê¸°ì¡´ _fetch_chunk_from_api ë¡œì§ í™œìš©)
            raw_data = await self._call_upbit_api(chunk_info, api_params)

            # ì‘ë‹µ ì¦‰ì‹œ ê²€ì¦
            validation_result = self._validate_api_response(raw_data, chunk_info)

            # ChunkInfoì— API ì‘ë‹µ ì •ë³´ ì €ì¥
            chunk_info.set_api_response_info(raw_data)

            # ì—…ë¹„íŠ¸ ë°ì´í„° ë ê°ì§€ (ìš”ì²­í•œ ê°œìˆ˜ë³´ë‹¤ ì ê²Œ ë°›ì€ ê²½ìš°)
            expected_count = api_params.get('count', chunk_info.count)
            has_upbit_data_end = len(raw_data) < expected_count

            # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ ë¡œê¹…
            if has_upbit_data_end:
                self.logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬: {chunk_info.symbol} {chunk_info.timeframe} - "
                                    f"ìš”ì²­={expected_count}ê°œ, ì‘ë‹µ={len(raw_data)}ê°œ")

            # êµ¬ì¡°í™”ëœ ì‘ë‹µ ê°ì²´ ìƒì„±
            return ApiResponse(
                raw_data=raw_data,
                validation_result=validation_result,
                has_upbit_data_end=has_upbit_data_end,
                requires_early_exit=validation_result.has_critical_errors or has_upbit_data_end,
                response_metadata={
                    'expected_count': expected_count,
                    'actual_count': len(raw_data),
                    'api_params': api_params
                }
            )

    async def _call_upbit_api(self, chunk_info: ChunkInfo, api_params: Dict[str, Any]) -> List[Dict]:
        """
        ğŸ“¡ ì—…ë¹„íŠ¸ API í˜¸ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)

        í˜„ì¬ _fetch_chunk_from_api() ë©”ì„œë“œë¥¼ ë¶„ë¦¬:
        - ë” ëª…í™•í•œ ë©”ì„œë“œëª…
        - API í˜¸ì¶œ ë¡œì§ë§Œ ì§‘ì¤‘
        """
        self.logger.debug(f"API ì²­í¬ ìš”ì²­: {chunk_info.chunk_id}")

        # API íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        api_count = api_params.get('count', chunk_info.count)
        api_to = api_params.get('to', chunk_info.to)

        if chunk_info.has_overlap_info() and chunk_info.needs_partial_api_call():
            self.logger.debug(f"ë¶€ë¶„ API í˜¸ì¶œ: count={api_count}, to={api_to} (overlap ìµœì í™”)")
        else:
            self.logger.debug(f"ì „ì²´ API í˜¸ì¶œ: count={api_count}, to={api_to}")

        try:
            # ê¸°ì¡´ _fetch_chunk_from_apiì˜ íƒ€ì„í”„ë ˆì„ë³„ ë¶„ê¸° ë¡œì§ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if chunk_info.timeframe == '1s':
                # ì´ˆë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    timeframe_delta = TimeUtils.get_timeframe_delta(chunk_info.timeframe)
                    fetch_time = api_to + timeframe_delta
                    to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_seconds(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe.endswith('m'):
                # ë¶„ë´‰
                unit = int(chunk_info.timeframe[:-1])
                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ë´‰ ë‹¨ìœ„: {unit}")

                # ë¶„ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    timeframe_delta = TimeUtils.get_timeframe_delta(chunk_info.timeframe)
                    fetch_time = api_to + timeframe_delta
                    to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")

                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit,
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1d':
                # ì¼ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%d")

                candles = await self.upbit_client.get_candles_days(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1w':
                # ì£¼ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m-%d")

                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1M':
                # ì›”ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y-%m")

                candles = await self.upbit_client.get_candles_months(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1y':
                # ì—°ë´‰ API ì§€ì¶œì  ë³´ì •
                to_param = None
                if api_to:
                    fetch_start_time = TimeUtils.get_time_by_ticks(api_to, chunk_info.timeframe, 1)
                    to_param = fetch_start_time.strftime("%Y")

                candles = await self.upbit_client.get_candles_years(
                    market=chunk_info.symbol,
                    count=api_count,
                    to=to_param
                )

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk_info.timeframe}")

            # ê°œì„ : ìµœì í™”ëœ ë¡œê¹… (overlap ì •ë³´ í‘œì‹œ)
            overlap_info = f" (overlap: {chunk_info.overlap_status.value})" if chunk_info.has_overlap_info() else ""
            self.logger.info(f"API ì²­í¬ ì™„ë£Œ: {chunk_info.chunk_id}, ìˆ˜ì§‘: {len(candles)}ê°œ{overlap_info}")

            return candles

        except Exception as e:
            self.logger.error(f"API ì²­í¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    def _validate_api_response(self, raw_data: List[Dict], chunk_info: ChunkInfo) -> ValidationResult:
        """API ì‘ë‹µ ë°ì´í„° ì¦‰ì‹œ ê²€ì¦"""
        validation_messages = []
        has_critical_errors = False

        try:
            # ê¸°ë³¸ ê²€ì¦
            if not isinstance(raw_data, list):
                validation_messages.append("API ì‘ë‹µì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜")
                has_critical_errors = True

            # ë¹ˆ ì‘ë‹µ ì²´í¬ (ê²½ê³  ìˆ˜ì¤€)
            if len(raw_data) == 0:
                validation_messages.append("ë¹ˆ API ì‘ë‹µ")

            # ìº”ë“¤ ë°ì´í„° êµ¬ì¡° ê²€ì¦
            for i, candle in enumerate(raw_data[:5]):  # ì²˜ìŒ 5ê°œë§Œ ê²€ì¦
                if not isinstance(candle, dict):
                    validation_messages.append(f"ìº”ë“¤ {i}ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
                    has_critical_errors = True
                    break

                required_fields = ['candle_date_time_utc', 'opening_price', 'high_price', 'low_price', 'trade_price']
                for field in required_fields:
                    if field not in candle:
                        validation_messages.append(f"ìº”ë“¤ {i}ì— í•„ìˆ˜ í•„ë“œ {field} ëˆ„ë½")
                        has_critical_errors = True
                        break

        except Exception as e:
            validation_messages.append(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            has_critical_errors = True

        return ValidationResult(
            is_valid=not has_critical_errors,
            has_critical_errors=has_critical_errors,
            validation_messages=validation_messages,
            validation_time=datetime.now(timezone.utc)
        )

    # =========================================================================
    # Phase 3: ë°ì´í„° ì²˜ë¦¬ ë‹¨ê³„
    # =========================================================================

    async def _phase3_process_data(self,
                                   api_response: ApiResponse,
                                   chunk_info: ChunkInfo,
                                   is_first_chunk: bool,
                                   safe_range_start: Optional[datetime],
                                   safe_range_end: Optional[datetime]) -> ProcessedData:
        """
        âš™ï¸ ìº”ë“¤ ë°ì´í„° ì²˜ë¦¬ ë° ì •ê·œí™”

        ì±…ì„:
        - ë¹ˆ ìº”ë“¤ ê°ì§€ ë° ì±„ìš°ê¸°
        - ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦
        - ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ìƒì„±
        """
        with self.performance_tracker.measure_phase('data_processing'):
            self.logger.debug(f"âš™ï¸ ë°ì´í„° ì²˜ë¦¬: {chunk_info.chunk_id}")

            # ğŸ†• ì²« ì²­í¬ ì •ë³´ì™€ ì•ˆì „ ë²”ìœ„ ì‚¬ìš©
            self.logger.debug(f"ì²« ì²­í¬: {is_first_chunk}, ì•ˆì „ ë²”ìœ„: [{safe_range_start}, {safe_range_end}]")

            # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ (ì²« ì²­í¬ ì •ë³´ í¬í•¨)
            filled_data = await self._detect_and_fill_gaps(
                api_response.raw_data, chunk_info, is_first_chunk=is_first_chunk,
                safe_range_start=safe_range_start, safe_range_end=safe_range_end
            )

            # ë°ì´í„° ì •ê·œí™”
            normalized_data = self._normalize_candle_data(filled_data)

            # ìµœì¢… ê²€ì¦
            validation_result = self._validate_processed_data(normalized_data, chunk_info)

            # ChunkInfoì— ìµœì¢… ìº”ë“¤ ì •ë³´ ì €ì¥
            chunk_info.set_final_candle_info(normalized_data)

            return ProcessedData(
                candles=normalized_data,
                gap_filled_count=len(filled_data) - len(api_response.raw_data),
                processing_metadata=self._create_processing_metadata(chunk_info),
                validation_passed=validation_result.is_valid
            )

    # =========================================================================
    # Phase 4: ë°ì´í„° ì €ì¥ ë‹¨ê³„
    # =========================================================================

    async def _phase4_persist_data(self,
                                   processed_data: ProcessedData,
                                   chunk_info: ChunkInfo) -> StorageResult:
        """
        ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì˜êµ¬ ì €ì¥

        ì±…ì„:
        - Repositoryë¥¼ í†µí•œ ë°ì´í„° ì €ì¥
        - ì €ì¥ ê²°ê³¼ ê²€ì¦
        - ì €ì¥ ë©”íƒ€ë°ì´í„° ìƒì„±
        """
        with self.performance_tracker.measure_phase('data_storage'):
            self.logger.debug(f"ğŸ’¾ ë°ì´í„° ì €ì¥: {chunk_info.chunk_id}")

            # ì €ì¥ ì‹¤í–‰ (ê¸°ì¡´ ë¡œì§ ì´ì£¼)
            saved_count = await self.repository.save_raw_api_data(
                chunk_info.symbol,
                chunk_info.timeframe,
                processed_data.candles
            )

            # ì €ì¥ ê²°ê³¼ ê²€ì¦
            storage_validation = self._validate_storage_result(saved_count, processed_data)

            self.logger.info(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥")

            return StorageResult(
                saved_count=saved_count,
                expected_count=len(processed_data.candles),
                storage_time=datetime.now(timezone.utc),
                validation_passed=storage_validation.is_valid,
                metadata=self._create_storage_metadata(chunk_info, saved_count)
            )

    # =========================================================================
    # í—¬í¼ ë©”ì„œë“œë“¤
    # =========================================================================

    async def _detect_and_fill_gaps(self,
                                    raw_candles: List[Dict],
                                    chunk_info: ChunkInfo,
                                    is_first_chunk: bool = False,
                                    safe_range_start: Optional[datetime] = None,
                                    safe_range_end: Optional[datetime] = None) -> List[Dict]:
        """
        ğŸ” ë¹ˆ ìº”ë“¤ ê°ì§€ ë° ì±„ìš°ê¸° (legacy ë¡œì§ ì™„ì „ ì´ì£¼)

        Args:
            raw_candles: API ì›ì‹œ ì‘ë‹µ ë°ì´í„°
            chunk_info: ì²­í¬ ì •ë³´
            is_first_chunk: ì²« ë²ˆì§¸ ì²­í¬ ì—¬ë¶€ (api_start +1í‹± ì¶”ê°€ ì œì–´)
            safe_range_start: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ì‹œì‘
            safe_range_end: ì•ˆì „í•œ ì°¸ì¡° ë²”ìœ„ ë

        Returns:
            ì²˜ë¦¬ëœ ìº”ë“¤ ë°ì´í„°
        """
        # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ê°€ ë¹„í™œì„±í™”ëœ ê²½ìš°
        if not self.enable_empty_candle_processing:
            self.logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë¹„í™œì„±í™”ë¨ â†’ ì›ë³¸ ë°ì´í„° ë°˜í™˜")
            return raw_candles

        self.logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì‹œì‘: ì²«ì²­í¬={is_first_chunk}, api_start={api_start}, api_end={api_end}")

        try:
            detector = self.empty_candle_detector_factory(chunk_info.symbol, chunk_info.timeframe)

            # API ë²”ìœ„ ì •ë³´ ì¶”ì¶œ (overlap ë¶„ì„ ê²°ê³¼ì—ì„œ)
            api_start = None
            api_end = None

            if chunk_info.has_overlap_info():
                overlap_result = getattr(chunk_info, 'overlap_result', None)
                if overlap_result and hasattr(overlap_result, 'api_start'):
                    api_start = overlap_result.api_start
                if overlap_result and hasattr(overlap_result, 'api_end'):
                    api_end = overlap_result.api_end

            # api_start/api_endê°€ ì—†ìœ¼ë©´ ì²­í¬ ì •ë³´ì—ì„œ ì¶”ì •
            if not api_start:
                api_start = chunk_info.to
            if not api_end:
                api_end = chunk_info.end

            # ë¹ˆ ìº”ë“¡ ì²˜ë¦¬ ì‹œì‘ ë¡œê¹… (ë³€ìˆ˜ ì •ì˜ í›„)
            self.logger.debug(f"ë¹ˆ ìº”ë“¡ ì²˜ë¦¬ ì‹œì‘: ì²«ì²­í¬={is_first_chunk}, api_start={api_start}, api_end={api_end}")

            # ğŸš€ Legacy ë¡œì§ê³¼ ë™ì¼í•œ ì²˜ë¦¬: ì²« ì²­í¬ëŠ” ë¬´ì¡°ê±´, ë‚˜ë¨¸ì§€ëŠ” ì¡°ê±´ë¶€
            if is_first_chunk:
                # ì²« ì²­í¬: api_endê°€ Noneì´ì–´ë„ ë¬´ì¡°ê±´ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ìˆ˜í–‰
                self.logger.info(f"ğŸš€ ì²« ì²­í¬ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì‹œì‘: api_start={api_start}, api_end={api_end} (api_endê°€ Noneì´ì–´ë„ ì²˜ë¦¬)")

                processed_candles = detector.detect_and_fill_gaps(
                    raw_candles,
                    api_start=api_start,
                    api_end=api_end,
                    is_first_chunk=True  # ğŸš€ ì²« ì²­í¬ ì •ë³´ ì „ë‹¬ (api_start +1í‹± ì¶”ê°€ ì œì–´)
                )

                # ìº”ë“¤ ìˆ˜ ë³´ì • ë¡œê¹…
                if len(processed_candles) != len(raw_candles):
                    empty_count = len(processed_candles) - len(raw_candles)
                    self.logger.info(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: ì›ë³¸ {len(raw_candles)}ê°œ + ë¹ˆ {empty_count}ê°œ = ìµœì¢… {len(processed_candles)}ê°œ")

                return processed_candles

            else:
                # ë‚˜ë¨¸ì§€ ì²­í¬: ì¡°ê±´ë¶€ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ (Legacyì™€ ë™ì¼)
                should_process = self._should_process_empty_candles(raw_candles, api_end)
                self.logger.debug(f"ğŸ” ë‚˜ë¨¸ì§€ ì²­í¬ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: {should_process} (api_end={api_end})")

                if should_process:
                    self.logger.info(f"âœ… ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤ê³¼ api_end ë¶ˆì¼ì¹˜")

                    processed_candles = detector.detect_and_fill_gaps(
                        raw_candles,
                        api_start=api_start,
                        api_end=api_end,
                        is_first_chunk=False
                    )

                    # ìº”ë“¤ ìˆ˜ ë³´ì • ë¡œê¹…
                    if len(processed_candles) != len(raw_candles):
                        empty_count = len(processed_candles) - len(raw_candles)
                        self.logger.info(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: ì›ë³¸ {len(raw_candles)}ê°œ + ë¹ˆ {empty_count}ê°œ = ìµœì¢… {len(processed_candles)}ê°œ")

                    return processed_candles
                else:
                    self.logger.info(f"âŒ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°: ë§ˆì§€ë§‰ìº”ë“¤ê³¼ api_end ì¼ì¹˜ ë˜ëŠ” ì¡°ê±´ ë¶ˆì¶©ì¡± (api_end={api_end})")
                    return raw_candles

        except Exception as e:
            self.logger.warning(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}, ì›ë³¸ ë°ì´í„° ë°˜í™˜")
            return raw_candles

    def _should_process_empty_candles(self, api_response: List[Dict[str, Any]], api_end: Optional[datetime]) -> bool:
        """API ì‘ë‹µì˜ ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ê³¼ api_end ë¹„êµí•˜ì—¬ ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš” ì—¬ë¶€ íŒë‹¨

        Args:
            api_response: ì—…ë¹„íŠ¸ API ì‘ë‹µ ë¦¬ìŠ¤íŠ¸
            api_end: ì˜ˆìƒë˜ëŠ” ì²­í¬ ì¢…ë£Œ ì‹œê°„

        Returns:
            bool: ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ê°€ í•„ìš”í•˜ë©´ True, ì•„ë‹ˆë©´ False
        """
        if not api_response or not api_end:
            self.logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: api_response ë˜ëŠ” api_endê°€ ì—†ìŒ â†’ ì²˜ë¦¬ ì•ˆ í•¨")
            return False

        try:
            # ì—…ë¹„íŠ¸ APIëŠ” ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ ë§ˆì§€ë§‰ ìš”ì†Œê°€ ê°€ì¥ ê³¼ê±° ìº”ë“¤
            last_candle = api_response[-1]
            candle_time_utc = last_candle.get('candle_date_time_utc')

            if candle_time_utc and isinstance(candle_time_utc, str):
                # UTC í†µì¼: TimeUtilsë¥¼ í†µí•œ í‘œì¤€ ì •ê·œí™” (aware datetime ë³´ì¥)
                parsed_time = datetime.fromisoformat(candle_time_utc)
                last_candle_time = TimeUtils.normalize_datetime_to_utc(parsed_time)

                # UTC í†µì¼: ë™ì¼í•œ í˜•ì‹(aware datetime) ê°„ ë¹„êµë¡œ ì •í™•ì„± ë³´ì¥
                needs_processing = last_candle_time != api_end

                if needs_processing:
                    self.logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ í•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time} vs api_end={api_end}")
                else:
                    self.logger.debug(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë¶ˆí•„ìš”: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time} == api_end={api_end}")

                return needs_processing

        except Exception as e:
            self.logger.warning(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨: {e} â†’ ì•ˆì „í•œ í´ë°±ìœ¼ë¡œ ì²˜ë¦¬ ì•ˆ í•¨")
            return False

        self.logger.debug("ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì¡°ê±´ í™•ì¸: ìº”ë“¤ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨ â†’ ì²˜ë¦¬ ì•ˆ í•¨")
        return False

    def _normalize_candle_data(self, candles: List[Dict]) -> List[Dict]:
        """ìº”ë“¤ ë°ì´í„° ì •ê·œí™”"""
        try:
            normalized = []
            for candle in candles:
                # ê¸°ë³¸ ì •ê·œí™”: í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° íƒ€ì… ë³€í™˜
                normalized_candle = {
                    'candle_date_time_utc': candle.get('candle_date_time_utc'),
                    'candle_date_time_kst': candle.get('candle_date_time_kst'),
                    'opening_price': float(candle.get('opening_price', 0)),
                    'high_price': float(candle.get('high_price', 0)),
                    'low_price': float(candle.get('low_price', 0)),
                    'trade_price': float(candle.get('trade_price', 0)),
                    'timestamp': candle.get('timestamp', 0),
                    'candle_acc_trade_price': float(candle.get('candle_acc_trade_price', 0)),
                    'candle_acc_trade_volume': float(candle.get('candle_acc_trade_volume', 0)),
                    'prev_closing_price': (float(candle.get('prev_closing_price', 0))
                                           if candle.get('prev_closing_price') else None),
                    'change_price': float(candle.get('change_price', 0)) if candle.get('change_price') else None,
                    'change_rate': float(candle.get('change_rate', 0)) if candle.get('change_rate') else None,
                }

                # ì¶”ê°€ í•„ë“œë“¤ë„ ê·¸ëŒ€ë¡œ ìœ ì§€ (ë¹ˆ ìº”ë“¤ ê´€ë ¨ ë“±)
                for key, value in candle.items():
                    if key not in normalized_candle:
                        normalized_candle[key] = value

                normalized.append(normalized_candle)

            self.logger.debug(f"ë°ì´í„° ì •ê·œí™” ì™„ë£Œ: {len(normalized)}ê°œ ìº”ë“¤")
            return normalized

        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            return candles  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

    def _validate_processed_data(self, candles: List[Dict], chunk_info: ChunkInfo) -> ValidationResult:
        """ì²˜ë¦¬ëœ ë°ì´í„° ìµœì¢… ê²€ì¦"""
        validation_messages = []
        has_critical_errors = False

        try:
            # ê¸°ë³¸ ë°ì´í„° ì¡´ì¬ í™•ì¸
            if not candles:
                validation_messages.append("ì²˜ë¦¬ëœ ìº”ë“¤ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ")
                has_critical_errors = True

            # ë°ì´í„° êµ¬ì¡° ê²€ì¦
            for i, candle in enumerate(candles[:3]):  # ì²˜ìŒ 3ê°œë§Œ ê²€ì¦
                if not isinstance(candle, dict):
                    validation_messages.append(f"ì²˜ë¦¬ëœ ìº”ë“¤ {i}ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜")
                    has_critical_errors = True
                    break

                # ê°€ê²© ë°ì´í„° ê²€ì¦
                price_fields = ['opening_price', 'high_price', 'low_price', 'trade_price']
                for field in price_fields:
                    value = candle.get(field)
                    if value is None or (isinstance(value, (int, float)) and value < 0):
                        validation_messages.append(f"ìº”ë“¤ {i}ì˜ {field} ê°’ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {value}")
                        # ê°€ê²©ì´ ìŒìˆ˜ì¸ ê²ƒì€ ê²½ê³  ìˆ˜ì¤€ (ë¹ˆ ìº”ë“¤ì¼ ìˆ˜ ìˆìŒ)

        except Exception as e:
            validation_messages.append(f"ì²˜ë¦¬ëœ ë°ì´í„° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            has_critical_errors = True

        return ValidationResult(
            is_valid=not has_critical_errors,
            has_critical_errors=has_critical_errors,
            validation_messages=validation_messages,
            validation_time=datetime.now(timezone.utc)
        )

    def _validate_storage_result(self, saved_count: int, processed_data: ProcessedData) -> ValidationResult:
        """ì €ì¥ ê²°ê³¼ ê²€ì¦"""
        validation_messages = []
        has_critical_errors = False

        try:
            expected_count = len(processed_data.candles)

            if saved_count != expected_count:
                validation_messages.append(f"ì €ì¥ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ={expected_count}, ì‹¤ì œ={saved_count}")
                # ì €ì¥ ê°œìˆ˜ê°€ ë‹¤ë¥¸ ê²ƒì€ ì¤‘ëŒ€ ì˜¤ë¥˜ë¡œ ì²˜ë¦¬
                has_critical_errors = True

            if saved_count < 0:
                validation_messages.append(f"ì˜ëª»ëœ ì €ì¥ ê°œìˆ˜: {saved_count}")
                has_critical_errors = True

        except Exception as e:
            validation_messages.append(f"ì €ì¥ ê²°ê³¼ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            has_critical_errors = True

        return ValidationResult(
            is_valid=not has_critical_errors,
            has_critical_errors=has_critical_errors,
            validation_messages=validation_messages,
            validation_time=datetime.now(timezone.utc)
        )

    def _create_processing_metadata(self, chunk_info: ChunkInfo) -> Dict[str, Any]:
        """ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        return {
            'processed_at': datetime.now(timezone.utc),
            'chunk_id': chunk_info.chunk_id,
            'symbol': chunk_info.symbol,
            'timeframe': chunk_info.timeframe,
            'processor_version': 'v1.1',
            'has_overlap_optimization': chunk_info.has_overlap_info(),
            'overlap_strategy': getattr(chunk_info, 'overlap_status', None)
        }

    def _create_storage_metadata(self, chunk_info: ChunkInfo, saved_count: int) -> Dict[str, Any]:
        """ì €ì¥ ë©”íƒ€ë°ì´í„° ìƒì„±"""
        return {
            'storage_method': 'repository',
            'saved_at': datetime.now(timezone.utc),
            'chunk_id': chunk_info.chunk_id,
            'saved_count': saved_count,
            'symbol': chunk_info.symbol,
            'timeframe': chunk_info.timeframe,
            'processor_version': 'v1.1'
        }

    def _calculate_chunk_end_time(self, chunk_info: ChunkInfo) -> datetime:
        """ì²­í¬ ìš”ì²­ì˜ ì˜ˆìƒ ì¢…ë£Œ ì‹œì  ê³„ì‚°"""
        ticks = -(chunk_info.count - 1)
        end_time = TimeUtils.get_time_by_ticks(chunk_info.to, chunk_info.timeframe, ticks)

        self.logger.debug(f"ğŸ” ì²­í¬ ê²½ê³„ ê³„ì‚°: to={chunk_info.to}, count={chunk_info.count}, "
                          f"ticks={ticks}, calculated_end={end_time}")

        return end_time

    def _calculate_api_count(self, start_time: datetime, end_time: datetime, timeframe: str) -> int:
        """API ìš”ì²­ì— í•„ìš”í•œ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°"""
        return TimeUtils.calculate_expected_count(start_time, end_time, timeframe)

    def _invalidate_cache(self):
        """í•„ìš”ì‹œ ìºì‹œ ë¬´íš¨í™”"""
        self._cache.clear()
        self.logger.debug("ìºì‹œ ë¬´íš¨í™” ì™„ë£Œ")

    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½ ì •ë³´ ë°˜í™˜"""
        return self.performance_tracker.get_performance_report()
