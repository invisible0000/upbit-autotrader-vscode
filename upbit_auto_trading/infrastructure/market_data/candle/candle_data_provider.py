"""
ğŸ“‹ CandleDataProvider v4.0 - Infrastructure Service ë©”ì¸ êµ¬í˜„
DDD Infrastructure Layerì—ì„œ ì„œë¸Œì‹œìŠ¤í…œì— ìº”ë“¤ ë°ì´í„° ì œê³µí•˜ëŠ” ë‹¨ì¼ ì§„ì…ì 

Created: 2025-01-08
Purpose: 5ê°€ì§€ íŒŒë¼ë¯¸í„° ì¡°í•© ì§€ì›, 200ê°œ ì²­í¬ ë¶„í• , DB/API í˜¼í•© ìµœì í™”
Features: inclusive_start ì‹œê°„ ì²˜ë¦¬, ìºì‹œ í™œìš©, ê²¹ì¹¨ ë¶„ì„ ê¸°ë°˜ API ìš”ì²­ ìµœì í™”
"""

import time
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Tuple

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.database.database_manager import DatabaseManager
from upbit_auto_trading.infrastructure.repositories.sqlite_candle_repository import SqliteCandleRepository
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import UpbitPublicClient
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils
from upbit_auto_trading.infrastructure.market_data.candle.models import (
    CandleData, CandleDataResponse, CandleChunk, CollectionResult,
    create_success_response, create_error_response
)

logger = create_component_logger("CandleDataProvider")


class CandleDataProvider:
    """
    ìº”ë“¤ ë°ì´í„° Infrastructure Service - ì„œë¸Œì‹œìŠ¤í…œë“¤ì˜ ë‹¨ì¼ ì§„ì…ì 

    ì£¼ìš” ê¸°ëŠ¥:
    - 5ê°€ì§€ íŒŒë¼ë¯¸í„° ì¡°í•© ì§€ì› (count, start_time, end_time)
    - inclusive_start ì—…ë¹„íŠ¸ API ì‹œê°„ ì²˜ë¦¬
    - 200ê°œ ì²­í¬ ìë™ ë¶„í•  ë° ìˆœì°¨ ìˆ˜ì§‘
    - DB/API í˜¼í•© ìµœì í™” (OverlapAnalyzer ì—°ë™)
    - ìºì‹œ í™œìš© (60ì´ˆ TTL)
    - ëŒ€ëŸ‰ ìš”ì²­ì‹œ target_end_time ë„ë‹¬ ì‹œ ìë™ ì¤‘ë‹¨

    DDD ì›ì¹™:
    - Infrastructure Serviceë¡œ ì„œë¸Œì‹œìŠ¤í…œë“¤ì´ ì§ì ‘ importí•˜ì—¬ ì‚¬ìš©
    - ë³µì¡í•œ ì²­í¬ ì²˜ë¦¬ì™€ ìµœì í™”ë¥¼ ì„œë¸Œì‹œìŠ¤í…œì—ì„œ ê°ì¶¤
    - Domain ë¡œì§ì€ í¬í•¨í•˜ì§€ ì•Šê³  ìˆœìˆ˜ ë°ì´í„° ì œê³µë§Œ ë‹´ë‹¹
    """

    def __init__(self,
                 db_manager: Optional[DatabaseManager] = None,
                 upbit_client: Optional[UpbitPublicClient] = None):
        """
        CandleDataProvider ì´ˆê¸°í™”

        Args:
            db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Noneì´ë©´ ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±)
            upbit_client: ì—…ë¹„íŠ¸ API í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ê¸°ë³¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±)
        """
        # Infrastructure ë¡œê¹… ì´ˆê¸°í™”
        logger.info("CandleDataProvider v4.0 ì´ˆê¸°í™” ì‹œì‘...")

        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •
        self.db_manager = db_manager
        if self.db_manager is None:
            logger.debug("ê¸°ë³¸ DatabaseManager ìƒì„± ì¤‘...")
            # ê¸°ë³¸ DatabaseManagerëŠ” ì¶”í›„ ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ê°œì„  ì˜ˆì •
            from upbit_auto_trading.infrastructure.database.database_manager import DatabaseManager
            from upbit_auto_trading.infrastructure.configuration import get_path_service

            # ì „ì—­ ê²½ë¡œ ì„œë¹„ìŠ¤ì—ì„œ í‘œì¤€ DB ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
            path_service = get_path_service()
            db_paths = {
                'settings': str(path_service.get_database_path('settings')),
                'strategies': str(path_service.get_database_path('strategies')),
                'market_data': str(path_service.get_database_path('market_data'))
            }
            self.db_manager = DatabaseManager(db_paths)        # Repository ì´ˆê¸°í™”
        self.repository = SqliteCandleRepository(self.db_manager)

        # ì—…ë¹„íŠ¸ API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
        self.upbit_client = upbit_client
        self._client_owned = upbit_client is None  # í´ë¼ì´ì–¸íŠ¸ ì†Œìœ ê¶Œ ì¶”ì 

        # ìºì‹œ ì´ˆê¸°í™” (60ì´ˆ TTL, 100MB ì œí•œ)
        from upbit_auto_trading.infrastructure.market_data.candle.candle_cache import CandleCache
        self.cache = CandleCache(max_memory_mb=100, default_ttl_seconds=60)
        logger.debug("CandleCache ì´ˆê¸°í™” ì™„ë£Œ - 100MB, 60ì´ˆ TTL")

        # ê²¹ì¹¨ ë¶„ì„ê¸° ì´ˆê¸°í™” (DB/API í˜¼í•© ìµœì í™”)
        from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import OverlapAnalyzer
        self.overlap_analyzer = OverlapAnalyzer(self.repository, TimeUtils)

        # ì„±ëŠ¥ í†µê³„ (DB/API í˜¼í•© ìµœì í™” ì¶”ê°€)
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'api_requests': 0,
            'db_queries': 0,
            'chunks_processed': 0,
            'average_response_time_ms': 0.0,
            # ìƒˆë¡œìš´ í˜¼í•© ìµœì í™” í†µê³„
            'mixed_optimizations': 0,        # DB/API í˜¼í•© ìµœì í™” íšŸìˆ˜
            'db_only_hits': 0,               # DB ì „ìš© ì¡°íšŒ íšŸìˆ˜
            'overlap_analysis_count': 0,     # ê²¹ì¹¨ ë¶„ì„ ì‹¤í–‰ íšŸìˆ˜
            'api_requests_saved': 0          # ìµœì í™”ë¡œ ì ˆì•½ëœ API ìš”ì²­ ìˆ˜
        }

        logger.info("âœ… CandleDataProvider v4.0 ì´ˆê¸°í™” ì™„ë£Œ")

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self._ensure_upbit_client()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close()

    async def _ensure_upbit_client(self):
        """ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ í™•ë³´"""
        if self.upbit_client is None:
            logger.debug("ê¸°ë³¸ UpbitPublicClient ìƒì„± ì¤‘...")
            from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
                create_upbit_public_client_async
            )
            self.upbit_client = await create_upbit_public_client_async()
            self._client_owned = True

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._client_owned and self.upbit_client:
            await self.upbit_client.close()
            self.upbit_client = None
            logger.debug("ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")

    # ================================================================
    # ì„œë¸Œì‹œìŠ¤í…œ ì§„ì…ì  - ë©”ì¸ API
    # ================================================================

    async def get_candles(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        inclusive_start: bool = True
    ) -> CandleDataResponse:
        """
        ìº”ë“¤ ë°ì´í„° ì¡°íšŒ - 5ê°€ì§€ íŒŒë¼ë¯¸í„° ì¡°í•© ì§€ì›

        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m', '5m', '1h', '1d')
            count: ì¡°íšŒí•  ìº”ë“¤ ê°œìˆ˜ (ìµœëŒ€ ì œí•œ ì—†ìŒ, ìë™ ì²­í¬ ë¶„í• )
            start_time: ì‹œì‘ ì‹œê°„ (ì‚¬ìš©ì ì œê³µì‹œ inclusive_start ì ìš©)
            end_time: ì¢…ë£Œ ì‹œê°„ (í•­ìƒ í¬í•¨, ì¡°ì • ë¶ˆí•„ìš”)
            inclusive_start: ì‚¬ìš©ì ì œê³µ start_time í¬í•¨ ì²˜ë¦¬ ì—¬ë¶€
                           True: start_time í¬í•¨í•˜ë„ë¡ ì¡°ì • (ê¸°ë³¸, ì§ê´€ì )
                           False: API ë„¤ì´í‹°ë¸Œ ë°°ì œ ë°©ì‹ (ê³ ê¸‰ ì‚¬ìš©ììš©)

        Returns:
            CandleDataResponse: ìº”ë“¤ ë°ì´í„° ì‘ë‹µ
                - success: ì„±ê³µ ì—¬ë¶€
                - candles: ìº”ë“¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ìˆœ ì •ë ¬)
                - total_count: ì´ ê°œìˆ˜
                - data_source: ë°ì´í„° ì†ŒìŠ¤ ("cache", "db", "api", "mixed")
                - response_time_ms: ì‘ë‹µ ì‹œê°„

        Raises:
            ValueError: ì˜ëª»ëœ íŒŒë¼ë¯¸í„° ì¡°í•© ë˜ëŠ” ë¯¸ë˜ ì‹œê°„ ìš”ì²­
            Exception: API ì˜¤ë¥˜ ë˜ëŠ” ì‹œìŠ¤í…œ ì˜¤ë¥˜

        Examples:
            # 1. countë§Œ ì§€ì • (ìµœê·¼ 200ê°œ)
            response = await provider.get_candles("KRW-BTC", "1m", count=200)

            # 2. start_time + count (í¬í•¨ ëª¨ë“œ)
            response = await provider.get_candles(
                "KRW-BTC", "1m",
                start_time=datetime.now() - timedelta(hours=10),
                count=600,
                inclusive_start=True  # 10ì‹œê°„ ì „ë¶€í„° í¬í•¨
            )

            # 3. start_time + end_time (ì‹œê°„ ë²”ìœ„)
            response = await provider.get_candles(
                "KRW-BTC", "1m",
                start_time=datetime.now() - timedelta(hours=5),
                end_time=datetime.now() - timedelta(hours=1),
                inclusive_start=True  # 5ì‹œê°„ ì „ë¶€í„° í¬í•¨
            )

            # 4. end_timeë§Œ ì§€ì •
            response = await provider.get_candles(
                "KRW-BTC", "1m",
                end_time=datetime.now() - timedelta(hours=2)
            )

            # 5. ê¸°ë³¸ê°’ (ìµœê·¼ 200ê°œ)
            response = await provider.get_candles("KRW-BTC", "1m")
        """
        request_start_time = time.perf_counter()
        self.stats['total_requests'] += 1

        try:
            logger.info(f"ğŸš€ ìº”ë“¤ ë°ì´í„° ìš”ì²­: {symbol} {timeframe}, "
                        f"count={count}, start_time={start_time}, end_time={end_time}, "
                        f"inclusive_start={inclusive_start}")

            # 1. ìš”ì²­ ê²€ì¦ ë° í‘œì¤€í™”
            validated_params = await self._validate_and_standardize_request(
                symbol, timeframe, count, start_time, end_time, inclusive_start
            )
            final_start_time, final_end_time, final_count = validated_params

            # ì—…ë¹„íŠ¸ ë°©í–¥ (latest â†’ past): end_timeì´ latest, start_timeì´ past
            logger.debug(f"ğŸ“‹ í‘œì¤€í™”ëœ ìš”ì²­ (ì—…ë¹„íŠ¸ ë°©í–¥): latest={final_end_time} â†’ past={final_start_time}, count={final_count}")

            # 2. ìºì‹œ ìš°ì„  í™•ì¸ (ì™„ì „ ë°ì´í„° ì¡´ì¬ì‹œ ì¦‰ì‹œ ë°˜í™˜)
            cache_result = await self._check_cache_complete_range(symbol, timeframe, final_start_time, final_count)
            if cache_result:
                response_time = (time.perf_counter() - request_start_time) * 1000
                self.stats['cache_hits'] += 1
                logger.info(f"ğŸ’¨ ìºì‹œ íˆíŠ¸! ì¦‰ì‹œ ë°˜í™˜: {len(cache_result)}ê°œ ìº”ë“¤, {response_time:.2f}ms")
                return create_success_response(cache_result, "cache", response_time)
            # if cache_result:
            #     return self._create_cache_response(cache_result, time.perf_counter() - request_start_time)

            # 3. ëŒ€ëŸ‰ ìš”ì²­ì‹œ 200ê°œ ì²­í¬ë¡œ ë¶„í• 
            chunks = self._split_into_chunks(symbol, timeframe, final_count, final_start_time, final_end_time)
            logger.debug(f"ğŸ“¦ ì²­í¬ ë¶„í• : {len(chunks)}ê°œ ì²­í¬ (200ê°œì”©)")

            # 4. ì²­í¬ë“¤ì„ ìˆœì°¨ ìˆ˜ì§‘ (target_end_time ë„ë‹¬ì‹œ ì¤‘ë‹¨)
            collected_candles, data_sources = await self._collect_chunks_sequentially(chunks, final_end_time)

            # 5. ìµœì¢… ì‘ë‹µ ì¡°í•©
            response = await self._assemble_response(
                collected_candles,
                data_sources,
                request_start_time,
                len(chunks)
            )

            logger.info(f"âœ… ìº”ë“¤ ë°ì´í„° ìš”ì²­ ì™„ë£Œ: {symbol} {timeframe}, "
                        f"{response.total_count}ê°œ ìˆ˜ì§‘, {response.response_time_ms:.1f}ms")

            return response

        except Exception as e:
            error_time = (time.perf_counter() - request_start_time) * 1000
            logger.error(f"âŒ ìº”ë“¤ ë°ì´í„° ìš”ì²­ ì‹¤íŒ¨: {symbol} {timeframe}, {e}")
            return create_error_response(str(e), error_time)

    # ================================================================
    # ìš”ì²­ ê²€ì¦ ë° í‘œì¤€í™”
    # ================================================================

    async def _validate_and_standardize_request(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int],
        start_time: Optional[datetime],
        end_time: Optional[datetime],
        inclusive_start: bool
    ) -> Tuple[datetime, datetime, int]:
        """
        ìš”ì²­ íŒŒë¼ë¯¸í„° ê²€ì¦ ë° í‘œì¤€í™”

        Returns:
            Tuple[datetime, datetime, int]: (final_start_time, final_end_time, final_count)
        """
        # ê¸°ë³¸ ê²€ì¦
        if not symbol or not timeframe:
            raise ValueError("symbolê³¼ timeframeì€ í•„ìˆ˜ì…ë‹ˆë‹¤")

        # count + end_time ë™ì‹œ ì‚¬ìš© ê¸ˆì§€
        if count is not None and end_time is not None:
            raise ValueError("countì™€ end_timeì€ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # TimeUtilsë¡œ ìµœì¢… ì‹œê°„ ë²”ìœ„ ê³„ì‚°
        final_start_time, final_end_time, final_count = TimeUtils.determine_target_end_time(
            count=count,
            start_time=start_time,
            end_time=end_time,
            timeframe=timeframe
        )

        # ë¯¸ë˜ ì‹œê°„ ê²€ì¦
        now = datetime.now(timezone.utc)
        if final_start_time > now:
            raise ValueError(f"ì‹œì‘ ì‹œê°„ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {final_start_time}")
        if final_end_time > now:
            raise ValueError(f"ì¢…ë£Œ ì‹œê°„ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {final_end_time}")

        # ì‚¬ìš©ì ì œê³µ start_timeì— ëŒ€í•œ inclusive_start ì²˜ë¦¬
        user_provided_start = start_time is not None and (count is not None or end_time is not None)
        adjusted_start_time = self._adjust_start_time_for_api(
            final_start_time, timeframe, inclusive_start, user_provided_start
        )

        return adjusted_start_time, final_end_time, final_count

    def _adjust_start_time_for_api(
        self,
        start_time: datetime,
        timeframe: str,
        inclusive_start: bool,
        user_provided_start: bool
    ) -> datetime:
        """
        ì—…ë¹„íŠ¸ API ì‹œê°„ ì²˜ë¦¬ - ì‚¬ìš©ì ì œê³µ start_timeì—ë§Œ ì¡°ì • ì ìš©

        Args:
            start_time: ì‹œì‘ ì‹œê°„
            timeframe: íƒ€ì„í”„ë ˆì„
            inclusive_start: í¬í•¨ ëª¨ë“œ ì—¬ë¶€
            user_provided_start: ì‚¬ìš©ìê°€ ì§ì ‘ ì œê³µí•œ start_time ì—¬ë¶€

        Returns:
            datetime: ì¡°ì •ëœ ì‹œì‘ ì‹œê°„
        """
        if not user_provided_start or not inclusive_start:
            # ì¡°ì • ë¶ˆí•„ìš” ì¼€ì´ìŠ¤:
            # 1. ì‹œìŠ¤í…œ ìë™ ìƒì„± start_time (ì¼€ì´ìŠ¤ 1,4,5)
            # 2. ì‚¬ìš©ìê°€ ë°°ì œ ëª¨ë“œ ì„ íƒ (inclusive_start=False)
            return start_time

        # ì‚¬ìš©ì ì œê³µ start_time + inclusive_start=True: í¬í•¨í•˜ë„ë¡ ì¡°ì •
        # ì—…ë¹„íŠ¸ APIëŠ” start_timeì„ ë°°ì œí•˜ë¯€ë¡œ, ì‹œê°„ìƒ ê³¼ê±°ë¡œ ì¡°ì •í•˜ì—¬ í¬í•¨ ë³´ì¥
        adjusted_start = TimeUtils.get_before_candle_time(start_time, timeframe)
        logger.debug(f"ğŸ¯ ì‚¬ìš©ì start_time í¬í•¨ ì¡°ì •: {start_time} â†’ {adjusted_start} (timeframe: {timeframe})")
        return adjusted_start

    # ================================================================
    # ìºì‹œ ê´€ë ¨ ë©”ì„œë“œ
    # ================================================================

    async def _check_cache_complete_range(self, symbol: str, timeframe: str,
                                          start_time: datetime, count: int) -> Optional[List[CandleData]]:
        """
        ìºì‹œì—ì„œ ì™„ì „í•œ ë²”ìœ„ì˜ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë°˜í™˜

        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            start_time: ì‹œì‘ ì‹œê°„
            count: ìš”ì²­ ê°œìˆ˜

        Returns:
            ìºì‹œëœ ì™„ì „í•œ ë°ì´í„° ë˜ëŠ” None
        """
        try:
            if not self.cache:
                return None

            # ì™„ì „ ë²”ìœ„ í™•ì¸
            if self.cache.has_complete_range(symbol, timeframe, start_time, count):
                cached_data = self.cache.get_cached_chunk(symbol, timeframe, start_time, count)
                if cached_data and len(cached_data) >= count:
                    logger.debug(f"ğŸ’¾ ìºì‹œ ì™„ì „ íˆíŠ¸: {symbol} {timeframe} ({count}ê°œ)")
                    return cached_data[:count]  # ì •í™•í•œ ê°œìˆ˜ë§Œ ë°˜í™˜

            return None

        except Exception as e:
            logger.warning(f"ìºì‹œ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    async def _store_chunk_in_cache(self, symbol: str, timeframe: str,
                                    start_time: datetime, candles: List[CandleData]) -> None:
        """
        ìˆ˜ì§‘í•œ ì²­í¬ë¥¼ ìºì‹œì— ì €ì¥

        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            start_time: ì‹œì‘ ì‹œê°„
            candles: ìº”ë“¤ ë°ì´í„°
        """
        try:
            if not self.cache or not candles:
                return

            success = self.cache.store_chunk(symbol, timeframe, start_time, candles)
            if success:
                logger.debug(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {symbol} {timeframe} ({len(candles)}ê°œ)")
            else:
                logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {symbol} {timeframe}")

        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        if not self.cache:
            return {"cache_enabled": False}

        cache_info = self.cache.get_cache_info()
        cache_info["cache_enabled"] = True
        return cache_info

    # ================================================================
    # ì²­í¬ ë¶„í•  ë° ìˆœì°¨ ìˆ˜ì§‘
    # ================================================================

    def _split_into_chunks(
        self,
        symbol: str,
        timeframe: str,
        count: int,
        start_time: datetime,
        end_time: datetime
    ) -> List[CandleChunk]:
        """
        ì „ì²´ ìš”ì²­ì„ 200ê°œ ì²­í¬ë¡œ ë¶„í•  - TimeUtils ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©

        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            count: ì´ ìº”ë“¤ ê°œìˆ˜
            start_time: ì‹œì‘ ì‹œê°„
            end_time: ì¢…ë£Œ ì‹œê°„

        Returns:
            List[CandleChunk]: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # TimeUtilsì˜ ê³ ê¸‰ ê¸°ëŠ¥ í™œìš©í•˜ì—¬ ì •í™•í•œ ì²­í¬ ë¶„í• 
        time_chunks = TimeUtils.calculate_chunk_boundaries(
            start_time=start_time,
            end_time=end_time,
            timeframe=timeframe,
            chunk_size=200
        )

        # TimeChunkë¥¼ CandleChunkë¡œ ë³€í™˜
        candle_chunks = []
        for idx, time_chunk in enumerate(time_chunks):
            candle_chunk = CandleChunk(
                symbol=symbol,
                timeframe=timeframe,
                start_time=time_chunk.start_time,
                count=time_chunk.expected_count,
                chunk_index=idx
            )
            candle_chunks.append(candle_chunk)

        logger.debug(f"ï¿½ TimeUtils ê¸°ë°˜ ì²­í¬ ë¶„í•  ì™„ë£Œ: {len(candle_chunks)}ê°œ ì²­í¬, "
                     f"ì˜ˆìƒ ì´ {sum(c.count for c in candle_chunks)}ê°œ ìº”ë“¤")
        return candle_chunks

    async def _collect_chunks_sequentially(
        self,
        chunks: List[CandleChunk],
        target_end_time: datetime
    ) -> Tuple[List[CandleData], List[str]]:
        """
        ì²­í¬ë“¤ì„ ìˆœì„œëŒ€ë¡œ í•˜ë‚˜ì”© ìˆ˜ì§‘ - ë°ì´í„° ì†ŒìŠ¤ ì¶”ì  ê°œì„ 

        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
            target_end_time: ëª©í‘œ ì¢…ë£Œ ì‹œê°„ (ì´ ì‹œì  ë„ë‹¬ì‹œ ìˆ˜ì§‘ ì¤‘ë‹¨)

        Returns:
            Tuple[List[CandleData], List[str]]: (ìˆ˜ì§‘ëœ ìº”ë“¤ ë°ì´í„°, ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸)
        """
        all_collected_candles = []
        data_sources = []  # ê° ì²­í¬ë³„ ë°ì´í„° ì†ŒìŠ¤ ì¶”ì 
        connected_end = None  # ì—°ì†ëœ ë°ì´í„°ì˜ ëì  ì¶”ì 

        for chunk_idx, chunk in enumerate(chunks):
            try:
                logger.debug(f"ğŸ“¦ ì²­í¬ {chunk_idx + 1}/{len(chunks)} ìˆ˜ì§‘ ì‹œì‘: "
                             f"{chunk.symbol} {chunk.timeframe}, {chunk.count}ê°œ")

                # ë‹¨ì¼ ì²­í¬ ìˆ˜ì§‘
                collection_result = await self._collect_single_chunk(chunk, connected_end)

                if collection_result.collected_candles:
                    all_collected_candles.extend(collection_result.collected_candles)
                    data_sources.append(collection_result.data_source)

                    # ìˆ˜ì§‘ëœ ìº”ë“¤ì˜ ë§ˆì§€ë§‰ ì‹œê°„ìœ¼ë¡œ connected_end ì—…ë°ì´íŠ¸
                    last_candle = collection_result.collected_candles[-1]
                    logger.debug(f"ğŸ” last_candle.candle_date_time_utc ì›ë³¸: '{last_candle.candle_date_time_utc}'")

                    # UTC ì‹œê°„ëŒ€ ì •ë³´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
                    if 'Z' in last_candle.candle_date_time_utc:
                        # Zê°€ ìˆëŠ” ê²½ìš°: Zë¥¼ +00:00ìœ¼ë¡œ ë³€í™˜
                        utc_time_str = last_candle.candle_date_time_utc.replace('Z', '+00:00')
                        connected_end = datetime.fromisoformat(utc_time_str)
                    else:
                        # Zê°€ ì—†ëŠ” ê²½ìš°: UTC ì‹œê°„ëŒ€ë¥¼ ì§ì ‘ ì„¤ì •
                        naive_time = datetime.fromisoformat(last_candle.candle_date_time_utc)
                        connected_end = naive_time.replace(tzinfo=timezone.utc)

                    logger.debug(f"ğŸ” ìµœì¢… connected_end: {connected_end} (tzinfo: {connected_end.tzinfo})")

                    logger.debug(f"âœ… ì²­í¬ {chunk_idx + 1} ìˆ˜ì§‘ ì™„ë£Œ: {len(collection_result.collected_candles)}ê°œ, "
                                 f"ì†ŒìŠ¤={collection_result.data_source}, connected_end={connected_end}")
                else:
                    logger.warning(f"âš ï¸ ì²­í¬ {chunk_idx + 1} ìˆ˜ì§‘ ê²°ê³¼ ì—†ìŒ")
                    data_sources.append("empty")

                # target_end_time ë„ë‹¬ ê²€ì‚¬ (ì‹œê°„ëŒ€ ì •ë³´ ë§ì¶¤)
                if connected_end and target_end_time:
                    # target_end_timeì´ ì‹œê°„ëŒ€ ì •ë³´ê°€ ì—†ìœ¼ë©´ UTCë¡œ ê°„ì£¼
                    if target_end_time.tzinfo is None:
                        target_end_time_utc = target_end_time.replace(tzinfo=timezone.utc)
                    else:
                        target_end_time_utc = target_end_time

                    if self._is_collection_complete(connected_end, target_end_time_utc):
                        logger.info(f"ğŸ¯ target_end_time ë„ë‹¬ë¡œ ìˆ˜ì§‘ ì™„ë£Œ: {connected_end} >= {target_end_time_utc}")
                        break

                self.stats['chunks_processed'] += 1

            except Exception as e:
                logger.error(f"âŒ ì²­í¬ {chunk_idx + 1} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                data_sources.append("error")
                # ì²­í¬ ì‹¤íŒ¨ì‹œì—ë„ ê³„ì† ì§„í–‰ (ë¶€ë¶„ ì„±ê³µ í—ˆìš©)
                continue

        logger.info(f"ğŸ“Š ì „ì²´ ì²­í¬ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_collected_candles)}ê°œ ìº”ë“¤")
        return all_collected_candles, data_sources

    async def _collect_single_chunk(
        self,
        chunk: CandleChunk,
        connected_end: Optional[datetime]
    ) -> CollectionResult:
        """
        ë‹¨ì¼ ì²­í¬ ìˆ˜ì§‘ ë¡œì§ - OverlapAnalyzer í™œìš© DB/API í˜¼í•© ìµœì í™”

        Args:
            chunk: ìˆ˜ì§‘í•  ì²­í¬
            connected_end: ì´ì „ ì²­í¬ì—ì„œ ì—°ì†ëœ ë°ì´í„°ì˜ ëì 

        Returns:
            CollectionResult: ì²­í¬ ìˆ˜ì§‘ ê²°ê³¼
        """
        collection_start_time = time.perf_counter()

        try:
            # OverlapAnalyzerë¡œ ê²¹ì¹¨ ë¶„ì„í•˜ì—¬ ìµœì í™”ëœ ìˆ˜ì§‘ ì „ëµ ê²°ì •
            self.stats['overlap_analysis_count'] += 1
            overlap_result = await self.overlap_analyzer.analyze_overlap(
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                target_start=chunk.start_time,
                target_count=chunk.count
            )

            # ê²¹ì¹¨ ìƒíƒœì— ë”°ë¥¸ ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘
            if overlap_result.status.value == "complete_overlap":
                # ì™„ì „ ê²¹ì¹¨: DBì—ì„œë§Œ ì¡°íšŒ (API ìš”ì²­ ì—†ìŒ)
                collected_candles = await self._collect_from_db_only(chunk)
                data_source = "db"
                api_requests = 0
                self.stats['db_only_hits'] += 1
                self.stats['api_requests_saved'] += 1  # API ìš”ì²­ 1íšŒ ì ˆì•½
                logger.debug(f"âœ… ì™„ì „ ê²¹ì¹¨ DB ì¡°íšŒ: {chunk.symbol} {chunk.timeframe}, {len(collected_candles)}ê°œ")

            elif overlap_result.status.value == "partial_overlap" and overlap_result.connected_end:
                # ë¶€ë¶„ ê²¹ì¹¨: DB + API í˜¼í•© ìµœì í™”
                collected_candles = await self._collect_mixed_optimized(chunk, overlap_result.connected_end)
                data_source = "mixed"
                api_requests = 1  # ìµœì í™”ëœ API ìš”ì²­ 1íšŒ
                self.stats['mixed_optimizations'] += 1
                logger.debug(f"ğŸ”„ í˜¼í•© ìµœì í™”: {chunk.symbol} {chunk.timeframe}, connected_end={overlap_result.connected_end}")

            else:
                # ê²¹ì¹¨ ì—†ìŒ: APIì—ì„œë§Œ ìˆ˜ì§‘ (ê¸°ì¡´ ë¡œì§)
                collected_candles = await self._collect_from_api_only(chunk)
                data_source = "api"
                api_requests = 1
                logger.debug(f"ğŸ†• API ì „ìš© ìˆ˜ì§‘: {chunk.symbol} {chunk.timeframe}, {len(collected_candles)}ê°œ")

            # Repositoryì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€ - INSERT OR IGNORE)
            if collected_candles:
                await self.repository.save_candle_chunk(chunk.symbol, chunk.timeframe, collected_candles)
                self.stats['db_queries'] += 1

                # ìºì‹œì— ì €ì¥
                await self._store_chunk_in_cache(chunk.symbol, chunk.timeframe, chunk.start_time, collected_candles)

            collection_time_ms = (time.perf_counter() - collection_start_time) * 1000

            return CollectionResult(
                chunk=chunk,
                collected_candles=collected_candles,
                data_source=data_source,
                api_requests_made=api_requests,
                collection_time_ms=collection_time_ms
            )

        except Exception as e:
            logger.error(f"âŒ ì²­í¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {chunk.symbol} {chunk.timeframe}, {e}")
            raise

    async def _collect_from_api_only(self, chunk: CandleChunk) -> List[CandleData]:
        """
        APIì—ì„œë§Œ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ êµ¬í˜„)

        Args:
            chunk: ìˆ˜ì§‘í•  ì²­í¬

        Returns:
            List[CandleData]: ìˆ˜ì§‘ëœ ìº”ë“¤ ë°ì´í„°
        """
        await self._ensure_upbit_client()

        # íƒ€ì„í”„ë ˆì„ë³„ API ì—”ë“œí¬ì¸íŠ¸ ë§¤í•‘
        api_response = await self._call_upbit_api(chunk)

        if not api_response:
            logger.warning(f"âš ï¸ API ì‘ë‹µ ì—†ìŒ: {chunk.symbol} {chunk.timeframe}")
            return []

        # API ì‘ë‹µì„ CandleData ëª¨ë¸ë¡œ ë³€í™˜
        candles = []
        for api_candle in api_response:
            try:
                candle = CandleData.from_upbit_api(api_candle, chunk.timeframe)
                candles.append(candle)
            except Exception as e:
                logger.warning(f"âš ï¸ ìº”ë“¤ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue

        self.stats['api_requests'] += 1
        logger.debug(f"ğŸ“¡ API ìˆ˜ì§‘ ì™„ë£Œ: {chunk.symbol} {chunk.timeframe}, {len(candles)}ê°œ")

        return candles

    async def _collect_from_db_only(self, chunk: CandleChunk) -> List[CandleData]:
        """
        DBì—ì„œë§Œ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘ (ì™„ì „ ê²¹ì¹¨ì‹œ)

        Args:
            chunk: ìˆ˜ì§‘í•  ì²­í¬

        Returns:
            List[CandleData]: DBì—ì„œ ì¡°íšŒëœ ìº”ë“¤ ë°ì´í„°
        """
        # ì²­í¬ ë²”ìœ„ì˜ ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
        timeframe_seconds = TimeUtils.get_timeframe_seconds(chunk.timeframe)
        chunk_end_time = chunk.start_time + timedelta(seconds=timeframe_seconds * (chunk.count - 1))

        # Repositoryì—ì„œ ë²”ìœ„ ì¡°íšŒ
        db_candles = await self.repository.get_candles_by_range(
            symbol=chunk.symbol,
            timeframe=chunk.timeframe,
            start_time=chunk.start_time,
            end_time=chunk_end_time
        )

        self.stats['db_queries'] += 1
        logger.debug(f"ğŸ’¾ DB ì¡°íšŒ ì™„ë£Œ: {chunk.symbol} {chunk.timeframe}, {len(db_candles)}ê°œ")

        return db_candles

    async def _collect_mixed_optimized(self, chunk: CandleChunk, connected_end: datetime) -> List[CandleData]:
        """
        DB/API í˜¼í•© ìµœì í™” ìˆ˜ì§‘ (ë¶€ë¶„ ê²¹ì¹¨ì‹œ)

        Args:
            chunk: ìˆ˜ì§‘í•  ì²­í¬
            connected_end: ì—°ì†ëœ ë°ì´í„°ì˜ ëì  (OverlapAnalyzer ì œê³µ)

        Returns:
            List[CandleData]: DB + API í˜¼í•© ìˆ˜ì§‘ëœ ìº”ë“¤ ë°ì´í„°
        """
        # ì‹œê°„ëŒ€ ì •ë³´ ë³´ì • - connected_end UTC í™•ë³´
        if connected_end.tzinfo is None:
            connected_end = connected_end.replace(tzinfo=timezone.utc)
        elif connected_end.tzinfo != timezone.utc:
            connected_end = connected_end.astimezone(timezone.utc)

        # 1. DBì—ì„œ ê¸°ì¡´ ë°ì´í„° ì¡°íšŒ (connected_endê¹Œì§€)
        db_candles = await self.repository.get_candles_by_range(
            symbol=chunk.symbol,
            timeframe=chunk.timeframe,
            start_time=chunk.start_time,
            end_time=connected_end
        )

        # 2. TimeUtilsë¡œ API ìš”ì²­ ì‹œì‘ì  ìµœì í™”
        api_start = TimeUtils.adjust_start_from_connection(
            connected_end=connected_end,
            timeframe=chunk.timeframe,
            count=200  # ê¸°ë³¸ ì²­í¬ í¬ê¸°
        )

        # 3. ì²­í¬ ë²”ìœ„ ì¢…ë£Œ ì‹œê°„ ê³„ì‚° (ì‹œê°„ëŒ€ ë³´ì •)
        timeframe_seconds = TimeUtils.get_timeframe_seconds(chunk.timeframe)
        chunk_end_time = chunk.start_time + timedelta(seconds=timeframe_seconds * (chunk.count - 1))

        # chunk_end_time ì‹œê°„ëŒ€ ë³´ì •
        if chunk_end_time.tzinfo is None:
            chunk_end_time = chunk_end_time.replace(tzinfo=timezone.utc)
        elif chunk_end_time.tzinfo != timezone.utc:
            chunk_end_time = chunk_end_time.astimezone(timezone.utc)

        # 4. APIì—ì„œ ì‹ ê·œ ë°ì´í„° ìˆ˜ì§‘ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
        api_candles = []
        if api_start <= chunk_end_time:
            # ìµœì í™”ëœ API ìš”ì²­ìš© ì²­í¬ ìƒì„±
            api_count = int((chunk_end_time - api_start).total_seconds() // timeframe_seconds) + 1
            api_chunk = CandleChunk(
                symbol=chunk.symbol,
                timeframe=chunk.timeframe,
                start_time=api_start,
                count=min(api_count, 200),  # ìµœëŒ€ 200ê°œë¡œ ì œí•œ
                chunk_index=chunk.chunk_index
            )

            api_candles = await self._collect_from_api_only(api_chunk)

        # 5. DB + API ë°ì´í„° í†µí•©
        all_candles = db_candles + api_candles

        # 6. ì¤‘ë³µ ì œê±° ë° ì‹œê°„ìˆœ ì •ë ¬
        unique_candles = {}
        for candle in all_candles:
            key = f"{candle.symbol}_{candle.timeframe}_{candle.candle_date_time_utc}"
            if key not in unique_candles:
                unique_candles[key] = candle

        sorted_candles = sorted(
            unique_candles.values(),
            key=lambda c: c.candle_date_time_utc
        )

        self.stats['db_queries'] += 1
        logger.debug(f"ğŸ”„ í˜¼í•© ìµœì í™” ì™„ë£Œ: {chunk.symbol} {chunk.timeframe}, "
                     f"DB={len(db_candles)}ê°œ + API={len(api_candles)}ê°œ = ì´ {len(sorted_candles)}ê°œ")

        return sorted_candles

    async def _call_upbit_api(self, chunk: CandleChunk) -> List[dict]:
        """
        ì—…ë¹„íŠ¸ API í˜¸ì¶œ (íƒ€ì„í”„ë ˆì„ë³„ ì—”ë“œí¬ì¸íŠ¸ ë§¤í•‘)

        Args:
            chunk: ì²­í¬ ì •ë³´

        Returns:
            List[dict]: ì—…ë¹„íŠ¸ API ì‘ë‹µ ë°ì´í„°
        """
        symbol = chunk.symbol
        count = chunk.count

        # upbit_clientê°€ Noneì´ ì•„ë‹˜ì„ íƒ€ì… ì²´í‚¹ìœ¼ë¡œ ë³´ì¥
        if self.upbit_client is None:
            raise RuntimeError("ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. _ensure_upbit_client()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")

        # ì²­í¬ì˜ ì‹œì‘ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ to_time ê³„ì‚°
        to_time = self._calculate_chunk_end_time(chunk)

        # ì—…ë¹„íŠ¸ ë°©í–¥: to_timeì´ latest(ì‹œì‘ì ), startê°€ ê³¼ê±° ë°©í–¥ ì°¸ê³ ìš©
        logger.debug(f"ğŸ“¡ API í˜¸ì¶œ (ì—…ë¹„íŠ¸ ë°©í–¥): {symbol} {chunk.timeframe}, count={count}, "
                    f"latest(to)={to_time}, past_ref={chunk.start_time}")

        try:
            if chunk.timeframe == '1s':
                if to_time:
                    return await self.upbit_client.get_candles_seconds(symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_seconds(symbol, count=count)
            elif chunk.timeframe.endswith('m'):
                # ë¶„ë´‰: 1m, 3m, 5m, 15m, 30m, 60m, 240m
                unit = int(chunk.timeframe[:-1])
                if to_time:
                    return await self.upbit_client.get_candles_minutes(unit, symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_minutes(unit, symbol, count=count)
            elif chunk.timeframe.endswith('h'):
                # ì‹œê°„ë´‰ì„ ë¶„ë´‰ìœ¼ë¡œ ë³€í™˜: 1h=60m, 4h=240m
                if chunk.timeframe == '1h':
                    unit = 60
                elif chunk.timeframe == '4h':
                    unit = 240
                else:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì‹œê°„ë´‰: {chunk.timeframe}")
                if to_time:
                    return await self.upbit_client.get_candles_minutes(unit, symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_minutes(unit, symbol, count=count)
            elif chunk.timeframe == '1d':
                if to_time:
                    return await self.upbit_client.get_candles_days(symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_days(symbol, count=count)
            elif chunk.timeframe == '1w':
                if to_time:
                    return await self.upbit_client.get_candles_weeks(symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_weeks(symbol, count=count)
            elif chunk.timeframe == '1M':
                if to_time:
                    return await self.upbit_client.get_candles_months(symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_months(symbol, count=count)
            elif chunk.timeframe == '1y':
                if to_time:
                    return await self.upbit_client.get_candles_years(symbol, count=count, to=to_time)
                else:
                    return await self.upbit_client.get_candles_years(symbol, count=count)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk.timeframe}")

        except Exception as e:
            logger.error(f"âŒ ì—…ë¹„íŠ¸ API í˜¸ì¶œ ì‹¤íŒ¨: {symbol} {chunk.timeframe}, {e}")
            raise

    def _is_collection_complete(self, current_end: datetime, target_end: datetime) -> bool:
        """
        í˜„ì¬ ìˆ˜ì§‘ì´ ëª©í‘œ ì¢…ë£Œ ì‹œê°„ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸

        Args:
            current_end: í˜„ì¬ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ë ì‹œê°„
            target_end: ëª©í‘œ ì¢…ë£Œ ì‹œê°„

        Returns:
            bool: ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€
        """
        return current_end >= target_end

    def _calculate_chunk_end_time(self, chunk: CandleChunk) -> Optional[str]:
        """
        ì²­í¬ì˜ ì •í™•í•œ ì¢…ë£Œ ì‹œê°„ì„ ê³„ì‚°í•˜ì—¬ ì—…ë¹„íŠ¸ API 'to' íŒŒë¼ë¯¸í„° í˜•ì‹ìœ¼ë¡œ ë°˜í™˜

        ì—…ë¹„íŠ¸ APIëŠ” 'to' íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •í•œ ì‹œê° **ì´ì „** ìº”ë“¤ì„ ì¡°íšŒí•˜ë¯€ë¡œ,
        chunk.start_time + (count * timeframe_duration)ë¡œ ì •í™•í•œ ë²”ìœ„ ê³„ì‚°

        Args:
            chunk: ì²­í¬ ì •ë³´

        Returns:
            str: ISO 8601 UTC í˜•ì‹ì˜ ì¢…ë£Œ ì‹œê°„ (ì˜ˆ: "2025-01-08T10:30:00Z")
        """
        try:
            # íƒ€ì„í”„ë ˆì„ë³„ 1ê°œ ìº”ë“¤ì˜ ì‹œê°„ ê°„ê²©(ì´ˆ) ê³„ì‚°
            timeframe_seconds = TimeUtils.get_timeframe_seconds(chunk.timeframe)

            # ì²­í¬ì˜ ì¢…ë£Œ ì‹œê°„ = ì‹œì‘ ì‹œê°„ + (ìº”ë“¤ ê°œìˆ˜ * íƒ€ì„í”„ë ˆì„ ê°„ê²©)
            chunk_end_time = chunk.start_time + timedelta(seconds=timeframe_seconds * chunk.count)

            # UTC ì‹œê°„ëŒ€ ë³´ì •
            if chunk_end_time.tzinfo is None:
                chunk_end_time = chunk_end_time.replace(tzinfo=timezone.utc)
            elif chunk_end_time.tzinfo != timezone.utc:
                chunk_end_time = chunk_end_time.astimezone(timezone.utc)

            # ISO 8601 UTC í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì—…ë¹„íŠ¸ API í˜¸í™˜)
            return chunk_end_time.strftime("%Y-%m-%dT%H:%M:%SZ")

        except Exception as e:
            logger.warning(f"ì²­í¬ ì¢…ë£Œ ì‹œê°„ ê³„ì‚° ì‹¤íŒ¨: {e}, chunk: {chunk.symbol} {chunk.timeframe}")
            # ê³„ì‚° ì‹¤íŒ¨ì‹œ None ë°˜í™˜í•˜ì—¬ ìµœì‹  ë°ì´í„° ì¡°íšŒë¡œ í´ë°±
            return None

    # ================================================================
    # ì‘ë‹µ ì¡°í•© ë° í†µê³„
    # ================================================================

    async def _assemble_response(
        self,
        collected_candles: List[CandleData],
        data_sources: List[str],
        request_start_time: float,
        chunks_count: int
    ) -> CandleDataResponse:
        """
        ìˆ˜ì§‘ëœ ëª¨ë“  ì²­í¬ë¥¼ í•˜ë‚˜ì˜ ì‘ë‹µìœ¼ë¡œ ì¡°í•© - ë°ì´í„° ì†ŒìŠ¤ ì¶”ì  ê°œì„ 

        Args:
            collected_candles: ìˆ˜ì§‘ëœ ëª¨ë“  ìº”ë“¤ ë°ì´í„°
            data_sources: ê° ì²­í¬ë³„ ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸
            request_start_time: ìš”ì²­ ì‹œì‘ ì‹œê°„
            chunks_count: ì²˜ë¦¬ëœ ì²­í¬ ê°œìˆ˜

        Returns:
            CandleDataResponse: ìµœì¢… ì‘ë‹µ
        """
        # ì¤‘ë³µ ì œê±° (UTC ì‹œê°„ ê¸°ì¤€)
        unique_candles = {}
        for candle in collected_candles:
            key = f"{candle.symbol}_{candle.timeframe}_{candle.candle_date_time_utc}"
            if key not in unique_candles:
                unique_candles[key] = candle

        # ì‹œê°„ìˆœ ì •ë ¬ (ê³¼ê±° â†’ ìµœì‹ )
        sorted_candles = sorted(
            unique_candles.values(),
            key=lambda c: c.candle_date_time_utc
        )

        # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
        response_time_ms = (time.perf_counter() - request_start_time) * 1000

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        if self.stats['average_response_time_ms'] == 0.0:
            self.stats['average_response_time_ms'] = response_time_ms
        else:
            # ì§€ìˆ˜ ì´ë™ í‰ê·  (Î±=0.1)
            self.stats['average_response_time_ms'] = (
                0.9 * self.stats['average_response_time_ms'] + 0.1 * response_time_ms
            )

        # ë°ì´í„° ì†ŒìŠ¤ ê²°ì • (ë°ì´í„° ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜)
        unique_sources = set(data_sources)
        if len(unique_sources) > 1:
            data_source = "mixed"  # ì—¬ëŸ¬ ì†ŒìŠ¤ í˜¼í•©
        elif "db" in unique_sources:
            data_source = "db"     # DB ìœ„ì£¼
        elif "api" in unique_sources:
            data_source = "api"    # API ìœ„ì£¼
        else:
            data_source = "api"    # ê¸°ë³¸ê°’

        return create_success_response(
            candles=sorted_candles,
            data_source=data_source,
            response_time_ms=response_time_ms
        )

    # ================================================================
    # í†µê³„ ë° ìƒíƒœ ì¡°íšŒ
    # ================================================================

    def get_stats(self) -> dict:
        """ì„œë¹„ìŠ¤ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        stats = self.stats.copy()

        # ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ í†µê³„ ì¶”ê°€
        if self.upbit_client:
            stats['upbit_client'] = self.upbit_client.get_stats()

        return stats

    def get_supported_timeframes(self) -> List[str]:
        """ì§€ì›í•˜ëŠ” íƒ€ì„í”„ë ˆì„ ëª©ë¡"""
        return ['1s', '1m', '3m', '5m', '15m', '30m', '1h', '4h', '1d', '1w', '1M', '1y']

    # ================================================================
    # í¸ì˜ ë©”ì„œë“œë“¤
    # ================================================================

    async def get_latest_candles(self, symbol: str, timeframe: str, count: int = 200) -> CandleDataResponse:
        """ìµœì‹  ìº”ë“¤ ì¡°íšŒ (í¸ì˜ ë©”ì„œë“œ)"""
        return await self.get_candles(symbol, timeframe, count=count)

    async def get_candles_since(
        self,
        symbol: str,
        timeframe: str,
        since: datetime,
        inclusive: bool = True
    ) -> CandleDataResponse:
        """íŠ¹ì • ì‹œì  ì´í›„ ìº”ë“¤ ì¡°íšŒ (í¸ì˜ ë©”ì„œë“œ)"""
        return await self.get_candles(
            symbol, timeframe,
            start_time=since,
            inclusive_start=inclusive
        )

    async def get_candles_range(
        self,
        symbol: str,
        timeframe: str,
        start: datetime,
        end: datetime,
        inclusive_start: bool = True
    ) -> CandleDataResponse:
        """ì‹œê°„ ë²”ìœ„ ìº”ë“¤ ì¡°íšŒ (í¸ì˜ ë©”ì„œë“œ)"""
        return await self.get_candles(
            symbol, timeframe,
            start_time=start,
            end_time=end,
            inclusive_start=inclusive_start
        )


# ================================================================
# í¸ì˜ íŒ©í† ë¦¬ í•¨ìˆ˜
# ================================================================

def create_candle_data_provider(
    db_manager: Optional[DatabaseManager] = None,
    upbit_client: Optional[UpbitPublicClient] = None
) -> CandleDataProvider:
    """
    CandleDataProvider ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í¸ì˜ í•¨ìˆ˜)

    Args:
        db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Noneì´ë©´ ê¸°ë³¸ ìƒì„±)
        upbit_client: ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ê¸°ë³¸ ìƒì„±)

    Returns:
        CandleDataProvider: ì„¤ì •ëœ í”„ë¡œë°”ì´ë” ì¸ìŠ¤í„´ìŠ¤

    Examples:
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ìƒì„±
        provider = create_candle_data_provider()

        # ì»¤ìŠ¤í…€ í´ë¼ì´ì–¸íŠ¸ë¡œ ìƒì„±
        custom_client = create_upbit_public_client()
        provider = create_candle_data_provider(upbit_client=custom_client)
    """
    return CandleDataProvider(db_manager=db_manager, upbit_client=upbit_client)


async def create_candle_data_provider_async(
    db_manager: Optional[DatabaseManager] = None,
    upbit_client: Optional[UpbitPublicClient] = None
) -> CandleDataProvider:
    """
    CandleDataProvider ë¹„ë™ê¸° ìƒì„± ë° ì´ˆê¸°í™” (í¸ì˜ í•¨ìˆ˜)

    Args:
        db_manager: ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € (Noneì´ë©´ ê¸°ë³¸ ìƒì„±)
        upbit_client: ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ (Noneì´ë©´ ê¸°ë³¸ ìƒì„±)

    Returns:
        CandleDataProvider: ì´ˆê¸°í™”ëœ í”„ë¡œë°”ì´ë” ì¸ìŠ¤í„´ìŠ¤

    Note:
        ì—…ë¹„íŠ¸ í´ë¼ì´ì–¸íŠ¸ë¥¼ ë¯¸ë¦¬ ì´ˆê¸°í™”í•˜ì—¬ ì²« ë²ˆì§¸ ìš”ì²­ ì‹œ ì§€ì—°ì„ ì¤„ì…ë‹ˆë‹¤.
    """
    provider = CandleDataProvider(db_manager=db_manager, upbit_client=upbit_client)
    await provider._ensure_upbit_client()
    return provider
