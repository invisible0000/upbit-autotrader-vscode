"""
ğŸ“‹ CandleDataProvider v4.1 - RequestInfo ê°•í™” ë° ìš”ì²­ ìƒíƒœ ê´€ë¦¬ ê°œì„ 
RequestType Enumê³¼ ê°•í™”ëœ RequestInfo ëª¨ë¸ì„ í†µí•œ ëª…í™•í•œ ìš”ì²­ íƒ€ì… ë¶„ë¥˜

Created: 2025-09-12 (Based on v4.0)
Purpose: ìš”ì²­ íƒ€ì… í˜¼ë™ ë°©ì§€, ì‹œê°„ ì •ë ¬ ë¬¸ì œ í•´ê²°, ì²« ì²­í¬ OverlapAnalyzer ìµœì í™”
Features: RequestType Enum, request_at í•„ë“œ, íƒ€ì…ë³„ ë¶„ê¸° ë¡œì§, ì½”ë“œ ì‹œì¸ì„± í–¥ìƒ
Architecture: ê¸°ì¡´ v4.0 + RequestInfo ëª¨ë¸ ê°•í™” + ëª…í™•í•œ ìƒíƒœ ê´€ë¦¬
"""

from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any
from enum import Enum
from dataclasses import dataclass, field

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils
from upbit_auto_trading.infrastructure.market_data.candle.candle_models import (
    ChunkInfo, CandleData
)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)

logger = create_component_logger("CandleDataProvider")


class RequestType(Enum):
    """ìš”ì²­ íƒ€ì… ë¶„ë¥˜ - ì‹œê°„ ì •ë ¬ ë° OverlapAnalyzer ìµœì í™”ìš©"""
    COUNT_ONLY = "count_only"      # countë§Œ, to=None (ì²« ì²­í¬ OverlapAnalyzer ê±´ë„ˆëœ€)
    TO_COUNT = "to_count"          # to + count (toë§Œ ì •ë ¬, OverlapAnalyzer ì‚¬ìš©)
    TO_END = "to_end"              # to + end (toë§Œ ì •ë ¬, OverlapAnalyzer ì‚¬ìš©)
    END_ONLY = "end_only"          # endë§Œ, COUNT_ONLYì²˜ëŸ¼ ë™ì‘ (ë™ì  count ê³„ì‚°)


@dataclass(frozen=True)
class RequestInfo:
    """
    ê°•í™”ëœ ìš”ì²­ ì •ë³´ ëª¨ë¸ - ì‹œê°„ ì •ë ¬ ë° íƒ€ì… ë¶„ë¥˜ ì§€ì›

    ë³€ê²½ì :
    - request_at í•„ë“œ ì¶”ê°€: ìš”ì²­ ë°œìƒ ì‹œì  ê¸°ë¡
    - get_request_type(): ëª…í™•í•œ íƒ€ì… ë¶„ë¥˜
    - should_align_time(): ì‹œê°„ ì •ë ¬ í•„ìš” ì—¬ë¶€
    - should_skip_overlap_analysis_for_first_chunk(): ì²« ì²­í¬ OverlapAnalyzer ê±´ë„ˆë›¸ì§€
    """
    # === í•„ìˆ˜ íŒŒë¼ë¯¸í„° ===
    symbol: str                           # ê±°ë˜ ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
    timeframe: str                        # íƒ€ì„í”„ë ˆì„ ('1m', '5m', '1h' ë“±)

    # === ì„ íƒì  íŒŒë¼ë¯¸í„° ===
    count: Optional[int] = None           # ìš”ì²­ ìº”ë“¤ ê°œìˆ˜
    to: Optional[datetime] = None         # ì‹œì‘ì  - ìµœì‹  ìº”ë“¤ ì‹œê°„
    end: Optional[datetime] = None        # ì¢…ë£Œì  - ê°€ì¥ ê³¼ê±° ìº”ë“¤ ì‹œê°„

    # === ìƒˆ í•„ë“œ ===
    request_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

    def __post_init__(self):
        """ìš”ì²­ ì •ë³´ ê²€ì¦"""
        # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ê²€ì¦
        if not self.symbol:
            raise ValueError("ì‹¬ë³¼ì€ í•„ìˆ˜ì…ë‹ˆë‹¤")
        if not self.timeframe:
            raise ValueError("íƒ€ì„í”„ë ˆì„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤")

        # count ë²”ìœ„ ê²€ì¦
        if self.count is not None and self.count < 1:
            raise ValueError(f"countëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {self.count}")

        # ì‹œê°„ ìˆœì„œ ê²€ì¦ (to > end ì´ì–´ì•¼ í•¨)
        if self.to is not None and self.end is not None and self.to <= self.end:
            raise ValueError("toëŠ” endë³´ë‹¤ ì´ì „ ì‹œì ì´ì–´ì•¼ í•©ë‹ˆë‹¤")

        # countì™€ end ë™ì‹œ ì‚¬ìš© ë°©ì§€
        if self.count is not None and self.end is not None:
            raise ValueError("countì™€ endëŠ” ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        # ìµœì†Œ íŒŒë¼ë¯¸í„° ì¡°í•© í™•ì¸
        has_count = self.count is not None
        has_to = self.to is not None
        has_end = self.end is not None

        valid_combinations = [
            has_count and not has_end,  # countë§Œ ë˜ëŠ” count+to
            has_to and has_end and not has_count,  # to+end
            has_end and not has_count and not has_to  # endë§Œ
        ]

        if not any(valid_combinations):
            raise ValueError("count ë˜ëŠ” to+end ë˜ëŠ” endë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")

    def get_request_type(self) -> RequestType:
        """ìš”ì²­ íƒ€ì… ìë™ ë¶„ë¥˜ - í˜¼ë™ ë¶ˆê°€ëŠ¥í•œ ëª…í™•í•œ ë¶„ë¥˜"""
        has_count = self.count is not None
        has_to = self.to is not None
        has_end = self.end is not None

        if has_count and not has_to and not has_end:
            return RequestType.COUNT_ONLY
        elif has_to and has_count and not has_end:
            return RequestType.TO_COUNT
        elif has_to and has_end and not has_count:
            return RequestType.TO_END
        elif has_end and not has_to and not has_count:
            return RequestType.END_ONLY
        else:
            # ì´ë¡ ì ìœ¼ë¡œ __post_init__ì—ì„œ ê±¸ëŸ¬ì§€ì§€ë§Œ ì•ˆì „ì¥ì¹˜
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒë¼ë¯¸í„° ì¡°í•©")

    def should_align_time(self) -> bool:
        """ì‹œê°„ ì •ë ¬ í•„ìš” ì—¬ë¶€ - TO_COUNT, TO_ENDë§Œ true"""
        request_type = self.get_request_type()
        return request_type in [RequestType.TO_COUNT, RequestType.TO_END]

    def needs_current_time_fallback(self) -> bool:
        """í˜„ì¬ ì‹œê°„ í´ë°± í•„ìš” ì—¬ë¶€ - END_ONLYë§Œ true"""
        return self.get_request_type() == RequestType.END_ONLY

    def should_skip_overlap_analysis_for_first_chunk(self) -> bool:
        """ì²« ì²­í¬ OverlapAnalyzer ê±´ë„ˆë›¸ì§€ - COUNT_ONLYì™€ END_ONLYë§Œ true"""
        request_type = self.get_request_type()
        return request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]

    def get_aligned_to_time(self) -> Optional[datetime]:
        """ì •ë ¬ëœ to ì‹œê°„ ë°˜í™˜ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)"""
        if not self.should_align_time() or self.to is None:
            return self.to

        # TimeUtilsë¥¼ ì‚¬ìš©í•œ ì‹œê°„ ì •ë ¬
        return TimeUtils.align_to_candle_boundary(self.to, self.timeframe)

    def to_log_string(self) -> str:
        """ë¡œê¹…ìš© ë¬¸ìì—´ (ë””ë²„ê¹… í¸ì˜ì„±)"""
        request_type = self.get_request_type()
        return (f"RequestInfo[{request_type.value}]: {self.symbol} {self.timeframe}, "
                f"count={self.count}, to={self.to}, end={self.end}, "
                f"request_at={self.request_at}")


class ChunkStatus(Enum):
    """ì²­í¬ ì²˜ë¦¬ ìƒíƒœ"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class CollectionState:
    """ìº”ë“¤ ìˆ˜ì§‘ ìƒíƒœ ê´€ë¦¬ - RequestInfo í¬í•¨"""
    request_id: str
    request_info: RequestInfo  # â† ìƒˆë¡œ ì¶”ê°€: ì›ë³¸ ìš”ì²­ ì •ë³´ ìœ ì§€
    symbol: str
    timeframe: str
    total_requested: int
    total_collected: int = 0
    completed_chunks: List[ChunkInfo] = field(default_factory=list)
    current_chunk: Optional[ChunkInfo] = None
    last_candle_time: Optional[str] = None  # ë§ˆì§€ë§‰ ìˆ˜ì§‘ëœ ìº”ë“¤ ì‹œê°„ (ì—°ì†ì„±ìš©)
    estimated_total_chunks: int = 0
    estimated_completion_time: Optional[datetime] = None
    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    is_completed: bool = False
    error_message: Optional[str] = None
    target_end: Optional[datetime] = None  # end íŒŒë¼ë¯¸í„° ëª©í‘œ ì‹œì 

    # ë‚¨ì€ ì‹œê°„ ì¶”ì  í•„ë“œë“¤
    last_update_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    avg_chunk_duration: float = 0.0  # í‰ê·  ì²­í¬ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    remaining_chunks: int = 0  # ë‚¨ì€ ì²­í¬ ìˆ˜
    estimated_remaining_seconds: float = 0.0  # ì‹¤ì‹œê°„ ê³„ì‚°ëœ ë‚¨ì€ ì‹œê°„


@dataclass
class CollectionPlan:
    """ìˆ˜ì§‘ ê³„íš (ìµœì†Œ ì •ë³´ë§Œ)"""
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float
    first_chunk_params: Dict[str, Any]  # ì²« ë²ˆì§¸ ì²­í¬ ìš”ì²­ íŒŒë¼ë¯¸í„°


class CandleDataProvider:
    """
    ìº”ë“¤ ë°ì´í„° ì œê³µì v4.1 - RequestInfo ê°•í™” ë° ìš”ì²­ ìƒíƒœ ê´€ë¦¬ ê°œì„ 

    ì£¼ìš” ë³€ê²½ì :
    1. RequestInfo ëª¨ë¸ ê°•í™”: request_at í•„ë“œ, íƒ€ì… ë¶„ë¥˜ ë©”ì„œë“œë“¤
    2. ìš”ì²­ íƒ€ì…ë³„ ëª…í™•í•œ ë¶„ê¸° ë¡œì§
    3. COUNT_ONLY ì²« ì²­í¬ OverlapAnalyzer ê±´ë„ˆë›°ê¸°
    4. ì‹œê°„ ì •ë ¬ ë¬¸ì œ ì›ì²œ ì°¨ë‹¨
    5. ì½”ë“œ ì‹œì¸ì„± ë° ë””ë²„ê¹… ê°œì„ 

    í•´ê²°ë˜ëŠ” ë¬¸ì œ:
    - COUNT_ONLY ìš”ì²­ì— to íŒŒë¼ë¯¸í„° ìë™ ì¶”ê°€ ë¬¸ì œ
    - OverlapAnalyzer ì‹œê°„ ì •ë ¬ ë¶ˆì¼ì¹˜ ë¬¸ì œ
    - ìš”ì²­ íƒ€ì… íŒë‹¨ ë¡œì§ ì¤‘ë³µ ë° í˜¼ë™
    - ë””ë²„ê¹… ì‹œ ìš”ì²­ ìƒíƒœ íŒŒì•… ì–´ë ¤ì›€
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        chunk_size: int = 200
    ):
        """CandleDataProvider v4.1 ì´ˆê¸°í™” - DI íŒ¨í„´ ì ìš©"""
        # ì˜ì¡´ì„± ì£¼ì…
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer

        # ê¸°ì¡´ ì„¤ì •
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜
        self.active_collections: Dict[str, CollectionState] = {}
        self.api_rate_limit_rps = 10  # 10 RPS ê¸°ì¤€

        logger.info("CandleDataProvider v4.1 (RequestInfo ê°•í™” + ìš”ì²­ ìƒíƒœ ê´€ë¦¬) ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ì²­í¬ í¬ê¸°: {self.chunk_size}, API Rate Limit: {self.api_rate_limit_rps} RPS")
        logger.info(f"ì˜ì¡´ì„±: Repository={type(repository).__name__}, Client={type(upbit_client).__name__}")
        logger.info(f"ì˜ì¡´ì„±: OverlapAnalyzer={type(overlap_analyzer).__name__}")

    def plan_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> CollectionPlan:
        """ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½ - RequestInfo ê¸°ë°˜"""
        # RequestInfo ìƒì„± (ê²€ì¦ í¬í•¨)
        request_info = RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=to,
            end=end
        )

        logger.info(f"ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½: {request_info.to_log_string()}")

        # ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦
        current_time = datetime.now(timezone.utc)
        if to is not None and to > current_time:
            raise ValueError(f"to ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {to}")
        if end is not None and end > current_time:
            raise ValueError(f"end ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {end}")

        # ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° (íƒ€ì…ë³„ ì²˜ë¦¬)
        total_count = self._calculate_total_count_by_type(request_info, current_time)

        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚°
        estimated_chunks = (total_count + self.chunk_size - 1) // self.chunk_size

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (10 RPS ê¸°ì¤€)
        estimated_duration_seconds = estimated_chunks / self.api_rate_limit_rps

        # ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± (íƒ€ì…ë³„ ë¶„ê¸°)
        first_chunk_params = self._create_first_chunk_params_by_type(request_info, current_time)

        plan = CollectionPlan(
            total_count=total_count,
            estimated_chunks=estimated_chunks,
            estimated_duration_seconds=estimated_duration_seconds,
            first_chunk_params=first_chunk_params
        )

        logger.info(f"âœ… ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {total_count:,}ê°œ ìº”ë“¤, {estimated_chunks}ì²­í¬, "
                    f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {estimated_duration_seconds:.1f}ì´ˆ")
        return plan

    def start_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> str:
        """ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘ - RequestInfo ê¸°ë°˜ ìƒíƒœ ì¶”ì """
        # RequestInfo ìƒì„±
        request_info = RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=to,
            end=end
        )

        logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘: {request_info.to_log_string()}")

        # ê³„íš ìˆ˜ë¦½
        plan = self.plan_collection(symbol, timeframe, count, to, end)

        # ìš”ì²­ ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        request_id = f"{symbol}_{timeframe}_{int(datetime.now().timestamp())}"

        # ìˆ˜ì§‘ ìƒíƒœ ì´ˆê¸°í™” (RequestInfo í¬í•¨)
        estimated_completion = datetime.now(timezone.utc) + timedelta(
            seconds=plan.estimated_duration_seconds
        )

        collection_state = CollectionState(
            request_id=request_id,
            request_info=request_info,  # â† ìƒˆë¡œ ì¶”ê°€: ì›ë³¸ ìš”ì²­ ì •ë³´ ë³´ê´€
            symbol=symbol,
            timeframe=timeframe,
            total_requested=plan.total_count,
            estimated_total_chunks=plan.estimated_chunks,
            estimated_completion_time=estimated_completion,
            remaining_chunks=plan.estimated_chunks,
            estimated_remaining_seconds=plan.estimated_duration_seconds,
            target_end=end
        )

        # ì²« ë²ˆì§¸ ì²­í¬ ìƒì„±
        first_chunk = self._create_next_chunk(
            collection_state=collection_state,
            chunk_params=plan.first_chunk_params,
            chunk_index=0
        )
        collection_state.current_chunk = first_chunk

        # ìƒíƒœ ë“±ë¡
        self.active_collections[request_id] = collection_state

        logger.info(f"âœ… ìˆ˜ì§‘ ì‹œì‘: ìš”ì²­ ID {request_id}, ì˜ˆìƒ ì™„ë£Œ: {estimated_completion}")
        return request_id

    def get_next_chunk(self, request_id: str) -> Optional[ChunkInfo]:
        """ë‹¤ìŒ ì²˜ë¦¬í•  ì²­í¬ ì •ë³´ ë°˜í™˜"""
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.is_completed:
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œë¨: {request_id}")
            return None

        if state.current_chunk is None:
            logger.warning(f"ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {request_id}")
            return None

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ë°˜í™˜: {state.current_chunk.chunk_id}")
        return state.current_chunk

    async def mark_chunk_completed(self, request_id: str) -> bool:
        """
        ì²­í¬ ì™„ë£Œ ì²˜ë¦¬ - RequestInfo ê¸°ë°˜ ìµœì í™” ì ìš©

        ì£¼ìš” ë³€ê²½ì :
        1. COUNT_ONLY ì²« ì²­í¬ëŠ” OverlapAnalyzer ê±´ë„ˆëœ€
        2. ìš”ì²­ íƒ€ì…ë³„ ëª…í™•í•œ ë¡œê·¸ ì¶œë ¥
        3. ì‹œê°„ ì •ë ¬ ë¬¸ì œ ì›ì²œ ì°¨ë‹¨
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.current_chunk is None:
            raise ValueError("ì²˜ë¦¬ ì¤‘ì¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ========================================
        # ğŸš€ í•µì‹¬ ê°œì„ ì : ìš”ì²­ íƒ€ì… ê¸°ë°˜ ìµœì í™”
        # ========================================
        is_first_chunk = len(state.completed_chunks) == 0
        request_type = state.request_info.get_request_type()

        logger.info(f"ğŸš€ ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {state.current_chunk.chunk_id} "
                    f"[{request_type.value}, ì²«ì²­í¬={is_first_chunk}]")

        try:
            # 1. COUNT_ONLYì™€ END_ONLY ì²« ì²­í¬ëŠ” OverlapAnalyzer ê±´ë„ˆëœ€ (to íŒŒë¼ë¯¸í„° ì—†ìŒ)
            if is_first_chunk and request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]:
                logger.info(f"ğŸ¯ {request_type.value} ì²« ì²­í¬: OverlapAnalyzer ê±´ë„ˆëœ€ (to íŒŒë¼ë¯¸í„° ì—†ìŒ)")

                # ì§ì ‘ API í˜¸ì¶œ (to íŒŒë¼ë¯¸í„° ì—†ìŒ ë³´ì¥)
                api_response = await self._fetch_chunk_from_api(state.current_chunk)
                candle_data_list = self._convert_upbit_response_to_candles(
                    api_response,
                    state.timeframe
                )
                api_calls_made = True

            else:
                # 2. ì¼ë°˜ì ì¸ ê²½ìš°: OverlapAnalyzer ì‚¬ìš©
                logger.debug(f"ğŸ” OverlapAnalyzer ì ìš©: {request_type.value}")

                chunk_start = state.current_chunk.to  # API ìš”ì²­ì˜ ì‹œì‘ì 
                chunk_end = self._calculate_chunk_end_time(state.current_chunk)  # ì˜ˆìƒ ì¢…ë£Œì 

                overlap_result = await self._analyze_chunk_overlap(
                    state.symbol,
                    state.timeframe,
                    chunk_start,
                    chunk_end
                )

                # ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ
                if overlap_result and hasattr(overlap_result, 'status'):
                    candle_data_list, api_calls_made = await self._collect_data_by_overlap_strategy(
                        state.current_chunk, overlap_result
                    )
                    logger.info(f"ğŸ” ê²¹ì¹¨ ë¶„ì„ ì ìš©: {overlap_result.status.value}, API í˜¸ì¶œ: {'ì˜ˆ' if api_calls_made else 'ì•„ë‹ˆì˜¤'}")
                else:
                    # ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨ ì‹œ í´ë°±: ê¸°ì¡´ API í˜¸ì¶œ ë°©ì‹
                    logger.warning("ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨ â†’ API í˜¸ì¶œ í´ë°±")
                    api_response = await self._fetch_chunk_from_api(state.current_chunk)
                    candle_data_list = self._convert_upbit_response_to_candles(
                        api_response,
                        state.timeframe
                    )
                    api_calls_made = True

            # 3. Repositoryë¥¼ í†µí•œ DB ì €ì¥ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            if candle_data_list:
                saved_count = await self._save_candles_to_repository(
                    state.symbol,
                    state.timeframe,
                    candle_data_list
                )
            else:
                saved_count = 0
                logger.debug("ì €ì¥í•  ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ì™„ì „ ê²¹ì¹¨)")

            # 4. í˜„ì¬ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
            completed_chunk = state.current_chunk
            completed_chunk.status = "completed"
            state.completed_chunks.append(completed_chunk)
            state.total_collected += len(candle_data_list)

            # 5. ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ì²­í¬ ì—°ì†ì„±ìš©)
            if candle_data_list:
                # CandleData ê°ì²´ ë˜ëŠ” dictì—ì„œ ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ ì¶”ì¶œ
                last_candle = candle_data_list[-1]
                if hasattr(last_candle, 'candle_date_time_utc'):
                    state.last_candle_time = last_candle.candle_date_time_utc
                elif isinstance(last_candle, dict) and 'candle_date_time_utc' in last_candle:
                    state.last_candle_time = last_candle['candle_date_time_utc']
                else:
                    logger.warning("ë§ˆì§€ë§‰ ìº”ë“¤ì—ì„œ ì‹œê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 6. ë‚¨ì€ ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
            self._update_remaining_time_estimates(state)

            logger.info(f"âœ… ì²­í¬ ì™„ë£Œ: {completed_chunk.chunk_id}, "
                        f"ìˆ˜ì§‘: {len(candle_data_list)}ê°œ, ì €ì¥: {saved_count}ê°œ, "
                        f"ëˆ„ì : {state.total_collected}/{state.total_requested}, "
                        f"ë‚¨ì€ì‹œê°„: {state.estimated_remaining_seconds:.1f}ì´ˆ")

            # 7. ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸ (ê°œìˆ˜ + ì‹œê°„ ì¡°ê±´)
            count_reached = state.c >= state.total_requested

            # end ì‹œì  ë„ë‹¬ í™•ì¸
            end_time_reached = False
            if state.target_end and candle_data_list:
                try:
                    # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ìº”ë“¤ì˜ ì‹œê°„ì„ datetimeìœ¼ë¡œ ë³€í™˜
                    last_candle = candle_data_list[-1]
                    last_candle_time_str = None

                    if hasattr(last_candle, 'candle_date_time_utc'):
                        last_candle_time_str = last_candle.candle_date_time_utc
                    elif isinstance(last_candle, dict) and 'candle_date_time_utc' in last_candle:
                        last_candle_time_str = last_candle['candle_date_time_utc']

                    if last_candle_time_str:
                        # ISO format ì²˜ë¦¬ (Z suffix ì œê±°)
                        if last_candle_time_str.endswith('Z'):
                            last_candle_time_str = last_candle_time_str[:-1] + '+00:00'
                        last_candle_time = datetime.fromisoformat(last_candle_time_str)

                        # ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ì´ target_endë³´ë‹¤ ê³¼ê±°ë©´ ì •ì§€
                        end_time_reached = last_candle_time <= state.target_end

                        logger.debug(
                            f"ì‹œê°„ ì¡°ê±´ í™•ì¸: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time}, "
                            f"ëª©í‘œì¢…ë£Œ={state.target_end}, ë„ë‹¬={end_time_reached}"
                        )
                except Exception as e:
                    logger.warning(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")

            if count_reached or end_time_reached:
                completion_reason = "ê°œìˆ˜ ë‹¬ì„±" if count_reached else "end ì‹œì  ë„ë‹¬"
                state.is_completed = True
                state.current_chunk = None
                logger.info(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ ({completion_reason}): {request_id}, {state.total_collected}ê°œ")
                return True

            # 8. ë‹¤ìŒ ì²­í¬ ìƒì„± (ì—°ì†ì„± ë³´ì¥ ê°œì„ )
            next_chunk_index = len(state.completed_chunks)
            remaining_count = state.total_requested - state.total_collected
            next_chunk_size = min(remaining_count, self.chunk_size)

            # ========================================
            # ğŸ”§ ê°œì„ ì : ì—°ì†ì„± ë³´ì¥ ë¡œì§ ê°œì„ 
            # ========================================
            next_chunk_params = self._create_next_chunk_params(
                state, next_chunk_size, request_type
            )

            # ë‹¤ìŒ ì²­í¬ ìƒì„±
            next_chunk = self._create_next_chunk(
                collection_state=state,
                chunk_params=next_chunk_params,
                chunk_index=next_chunk_index
            )
            state.current_chunk = next_chunk

            logger.debug(f"â¡ï¸ ë‹¤ìŒ ì²­í¬ ìƒì„±: {next_chunk.chunk_id}, ì”ì—¬: {remaining_count}ê°œ")
            return False

        except Exception as e:
            # ì²­í¬ ì‹¤íŒ¨ ì²˜ë¦¬
            if state.current_chunk:
                state.current_chunk.status = "failed"
                state.error_message = str(e)

            logger.error(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {state.current_chunk.chunk_id if state.current_chunk else 'Unknown'}, ì˜¤ë¥˜: {e}")
            raise

    # ========================================
    # ğŸ†• ìƒˆë¡œìš´ íƒ€ì…ë³„ ì²˜ë¦¬ ë©”ì„œë“œë“¤
    # ========================================

    def _calculate_total_count_by_type(
        self,
        request_info: RequestInfo,
        current_time: datetime
    ) -> int:
        """ìš”ì²­ íƒ€ì…ë³„ ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°"""
        request_type = request_info.get_request_type()

        if request_type in [RequestType.COUNT_ONLY, RequestType.TO_COUNT]:
            # countê°€ ì§ì ‘ ì œê³µëœ ê²½ìš°
            return request_info.count

        elif request_type in [RequestType.TO_END, RequestType.END_ONLY]:
            # endê°€ ì œê³µëœ ê²½ìš°: ê¸°ê°„ ê³„ì‚°
            start_time = request_info.to if request_info.to is not None else current_time
            end_time = request_info.end

            normalized_start = TimeUtils.align_to_candle_boundary(start_time, request_info.timeframe)
            normalized_end = TimeUtils.align_to_candle_boundary(end_time, request_info.timeframe)

            if normalized_start <= normalized_end:
                raise ValueError("ì‹œì‘ ì‹œì ì´ ì¢…ë£Œ ì‹œì ë³´ë‹¤ ì´ì „ì…ë‹ˆë‹¤")

            return TimeUtils.calculate_expected_count(
                start_time=normalized_start,
                end_time=normalized_end,
                timeframe=request_info.timeframe
            )

        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìš”ì²­ íƒ€ì…: {request_type}")

    def _create_first_chunk_params_by_type(
        self,
        request_info: RequestInfo,
        current_time: datetime
    ) -> Dict[str, Any]:
        """ìš”ì²­ íƒ€ì…ë³„ ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±"""
        request_type = request_info.get_request_type()
        params = {"market": request_info.symbol}

        if request_type == RequestType.COUNT_ONLY:
            # ì§„ì§œ count-only: to íŒŒë¼ë¯¸í„° ì—†ìŒ
            chunk_size = min(request_info.count, self.chunk_size)
            params["count"] = chunk_size
            logger.debug(f"COUNT_ONLY: to íŒŒë¼ë¯¸í„° ì—†ì´ count={chunk_size}")

        elif request_type == RequestType.TO_COUNT:
            # to + count: toë§Œ ì •ë ¬
            chunk_size = min(request_info.count, self.chunk_size)
            aligned_to = request_info.get_aligned_to_time()
            params["count"] = chunk_size
            params["to"] = aligned_to.strftime("%Y-%m-%dT%H:%M:%S")
            logger.debug(f"TO_COUNT: ì •ë ¬ëœ to={aligned_to}, count={chunk_size}")

        elif request_type == RequestType.TO_END:
            # to + end: toë§Œ ì •ë ¬
            aligned_to = request_info.get_aligned_to_time()
            params["count"] = self.chunk_size
            params["to"] = aligned_to.strftime("%Y-%m-%dT%H:%M:%S")
            logger.debug(f"TO_END: ì •ë ¬ëœ to={aligned_to}, count={self.chunk_size}")

        elif request_type == RequestType.END_ONLY:
            # endë§Œ: COUNT_ONLYì²˜ëŸ¼ ë™ì‘ (to ì—†ì´, ì—…ë¹„íŠ¸ ì„œë²„ ìµœì‹ ë¶€í„°)
            params["count"] = self.chunk_size
            logger.debug(f"END_ONLY: COUNT_ONLYì²˜ëŸ¼ to ì—†ì´, count={self.chunk_size} (ì´ ê°œìˆ˜ëŠ” ë™ì  ê³„ì‚°ë¨)")

        return params

    def _create_next_chunk_params(
        self,
        state: CollectionState,
        chunk_size: int,
        request_type: RequestType
    ) -> Dict[str, Any]:
        """ë‹¤ìŒ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± - ì—°ì†ì„± ë³´ì¥ ê°œì„ """
        params = {
            "market": state.symbol,
            "count": chunk_size
        }

        # ========================================
        # ğŸ”§ í•µì‹¬ ê°œì„ : ì—°ì†ì„± ë³´ì¥ ë¡œì§
        # ========================================
        if state.last_candle_time:
            if request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]:
                # COUNT_ONLYì™€ END_ONLYëŠ” ë‘ ë²ˆì§¸ ì²­í¬ë¶€í„° ë§ˆì§€ë§‰ ì‹œê°„ ì‚¬ìš© (ì—°ì†ì„± ë³´ì¥)
                params["to"] = state.last_candle_time
                logger.debug(f"{request_type.value} í›„ì† ì²­í¬: to={state.last_candle_time}")
            else:
                # TO_COUNT, TO_ENDëŠ” 1í‹± ì´ì „ ì‹œê°„ ì‚¬ìš© (ê²¹ì¹¨ ë°©ì§€)
                try:
                    # ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ì—ì„œ 1í‹± ì´ì „ìœ¼ë¡œ ì¡°ì •
                    last_time = datetime.fromisoformat(state.last_candle_time.replace('Z', '+00:00'))
                    timeframe_delta = TimeUtils.get_timeframe_delta(state.timeframe)
                    adjusted_to = last_time - timeframe_delta
                    params["to"] = adjusted_to.strftime("%Y-%m-%dT%H:%M:%S")
                    logger.debug(f"{request_type.value} í›„ì† ì²­í¬: ì¡°ì •ëœ to={adjusted_to}")
                except Exception as e:
                    logger.warning(f"ì‹œê°„ ì¡°ì • ì‹¤íŒ¨: {e}, ì›ë³¸ ì‹œê°„ ì‚¬ìš©")
                    params["to"] = state.last_candle_time

        return params

    # ========================================
    # ê¸°ì¡´ ë©”ì„œë“œë“¤ (ìµœì†Œ ë³€ê²½)
    # ========================================

    async def _fetch_chunk_from_api(self, chunk_info: ChunkInfo) -> List[Dict[str, Any]]:
        """ì‹¤ì œ API í˜¸ì¶œì„ í†µí•œ ì²­í¬ ë°ì´í„° ìˆ˜ì§‘"""
        logger.debug(f"API ì²­í¬ ìš”ì²­ ì‹œì‘: {chunk_info.chunk_id}")

        try:
            # íƒ€ì„í”„ë ˆì„ë³„ API ë©”ì„œë“œ ì„ íƒ
            if chunk_info.timeframe.endswith('m'):
                # ë¶„ë´‰ (1m, 3m, 5m, 10m, 15m, 30m, 60m, 240m)
                unit = int(chunk_info.timeframe[:-1])
                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ë´‰ ë‹¨ìœ„: {unit}")

                to_param = chunk_info.to.strftime("%Y-%m-%dT%H:%M:%S") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit,
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1d':
                # ì¼ë´‰
                to_param = chunk_info.to.strftime("%Y-%m-%d") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_days(
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1w':
                # ì£¼ë´‰
                to_param = chunk_info.to.strftime("%Y-%m-%d") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1M':
                # ì›”ë´‰
                to_param = chunk_info.to.strftime("%Y-%m") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_months(
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk_info.timeframe}")

            logger.info(f"âœ… API ì²­í¬ ì™„ë£Œ: {chunk_info.chunk_id}, ìˆ˜ì§‘: {len(candles)}ê°œ")
            return candles

        except Exception as e:
            logger.error(f"âŒ API ì²­í¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    def _convert_upbit_response_to_candles(
        self,
        api_response: List[Dict[str, Any]],
        timeframe: str
    ) -> List[CandleData]:
        """ì—…ë¹„íŠ¸ API ì‘ë‹µì„ CandleData ê°ì²´ë¡œ ë³€í™˜"""
        logger.debug(f"API ì‘ë‹µ ë³€í™˜ ì‹œì‘: {len(api_response)}ê°œ ìº”ë“¤, íƒ€ì„í”„ë ˆì„: {timeframe}")

        try:
            converted_candles = []
            for candle_dict in api_response:
                candle_data = CandleData.from_upbit_api(candle_dict, timeframe)
                converted_candles.append(candle_data)

            logger.debug(f"âœ… API ì‘ë‹µ ë³€í™˜ ì™„ë£Œ: {len(converted_candles)}ê°œ")
            return converted_candles

        except Exception as e:
            logger.error(f"âŒ API ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {e}")
            logger.error(f"ë¬¸ì œëœ ë°ì´í„° ìƒ˜í”Œ: {api_response[:1] if api_response else 'Empty'}")
            raise

    async def _save_candles_to_repository(
        self,
        symbol: str,
        timeframe: str,
        candles: List[CandleData]
    ) -> int:
        """Repositoryë¥¼ í†µí•œ ìº”ë“¤ ë°ì´í„° DB ì €ì¥"""
        if not candles:
            logger.warning("ì €ì¥í•  ìº”ë“¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0

        logger.debug(f"DB ì €ì¥ ì‹œì‘: {symbol} {timeframe}, {len(candles)}ê°œ")

        try:
            saved_count = await self.repository.save_candle_chunk(symbol, timeframe, candles)
            logger.debug(f"âœ… DB ì €ì¥ ì™„ë£Œ: {saved_count}/{len(candles)}ê°œ ì €ì¥ë¨")
            return saved_count

        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {symbol} {timeframe}, {len(candles)}ê°œ, ì˜¤ë¥˜: {e}")
            raise

    async def _analyze_chunk_overlap(
        self,
        symbol: str,
        timeframe: str,
        start_time: datetime,
        end_time: datetime
    ):
        """OverlapAnalyzerë¥¼ í™œìš©í•œ ì²­í¬ ê²¹ì¹¨ ë¶„ì„"""
        logger.debug(f"ê²¹ì¹¨ ë¶„ì„: {symbol} {timeframe}, {start_time} ~ {end_time}")

        try:
            # OverlapAnalyzer v5.0 í™œìš©
            from upbit_auto_trading.infrastructure.market_data.candle.candle_models import OverlapRequest

            # Timezone ì•ˆì „ ì²˜ë¦¬: ëª¨ë“  datetimeì„ UTCë¡œ í†µì¼
            safe_start_time = self._ensure_utc_timezone(start_time)
            safe_end_time = self._ensure_utc_timezone(end_time)

            # ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
            expected_count = self._calculate_api_count(safe_start_time, safe_end_time, timeframe)

            overlap_request = OverlapRequest(
                symbol=symbol,
                timeframe=timeframe,
                target_start=safe_start_time,
                target_end=safe_end_time,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)
            logger.debug(f"ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼: {overlap_result.status.value} (ì˜ˆìƒ ê°œìˆ˜: {expected_count})")

            return overlap_result

        except Exception as e:
            logger.warning(f"ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨: {e}, API í˜¸ì¶œë¡œ ì§„í–‰")
            return None

    def get_collection_status(self, request_id: str) -> Dict[str, Any]:
        """ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ - RequestInfo ì •ë³´ í¬í•¨"""
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        progress_percentage = (state.total_collected / state.total_requested) * 100
        elapsed_time = datetime.now(timezone.utc) - state.start_time
        completed_chunks = len(state.completed_chunks)

        # ========================================
        # ğŸ†• RequestInfo ì •ë³´ ì¶”ê°€
        # ========================================
        return {
            "request_id": request_id,
            "request_info": {
                "type": state.request_info.get_request_type().value,
                "symbol": state.request_info.symbol,
                "timeframe": state.request_info.timeframe,
                "count": state.request_info.count,
                "to": state.request_info.to.isoformat() if state.request_info.to else None,
                "end": state.request_info.end.isoformat() if state.request_info.end else None,
                "request_at": state.request_info.request_at.isoformat(),
                "should_align_time": state.request_info.should_align_time(),
                "skip_first_overlap_analysis": state.request_info.should_skip_overlap_analysis_for_first_chunk()
            },
            "progress": {
                "collected": state.total_collected,
                "requested": state.total_requested,
                "percentage": round(progress_percentage, 1)
            },
            "chunks": {
                "completed": completed_chunks,
                "estimated_total": state.estimated_total_chunks,
                "current": state.current_chunk.chunk_id if state.current_chunk else None
            },
            "timing": {
                "elapsed_seconds": elapsed_time.total_seconds(),
                "estimated_total_seconds": (
                    state.estimated_completion_time - state.start_time
                    if state.estimated_completion_time else None
                ),
                "estimated_remaining_seconds": state.estimated_remaining_seconds,
                "avg_chunk_duration": state.avg_chunk_duration,
                "remaining_chunks": state.remaining_chunks
            },
            "is_completed": state.is_completed,
            "error": state.error_message
        }

    # ========================================
    # ê¸°íƒ€ í—¬í¼ ë©”ì„œë“œë“¤ (ê¸°ì¡´ ìœ ì§€)
    # ========================================

    def resume_collection(self, request_id: str) -> Optional[ChunkInfo]:
        """ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ"""
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.is_completed:
            logger.info(f"ì´ë¯¸ ì™„ë£Œëœ ìˆ˜ì§‘: {request_id}")
            return None

        if state.current_chunk is not None:
            logger.info(f"ìˆ˜ì§‘ ì¬ê°œ: {request_id}, í˜„ì¬ ì²­í¬: {state.current_chunk.chunk_id}")
            return state.current_chunk

        # í˜„ì¬ ì²­í¬ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (ë§ˆì§€ë§‰ ìƒíƒœ ê¸°ì¤€)
        next_chunk_index = len(state.completed_chunks)
        remaining_count = state.total_requested - state.total_collected

        if remaining_count <= 0:
            state.is_completed = True
            return None

        next_chunk_size = min(remaining_count, self.chunk_size)
        request_type = state.request_info.get_request_type()

        next_chunk_params = self._create_next_chunk_params(
            state, next_chunk_size, request_type
        )

        next_chunk = self._create_next_chunk(
            collection_state=state,
            chunk_params=next_chunk_params,
            chunk_index=next_chunk_index
        )
        state.current_chunk = next_chunk

        logger.info(f"ìˆ˜ì§‘ ì¬ê°œ: ìƒˆ ì²­í¬ ìƒì„± {next_chunk.chunk_id}")
        return next_chunk

    def _create_next_chunk(
        self,
        collection_state: CollectionState,
        chunk_params: Dict[str, Any],
        chunk_index: int
    ) -> ChunkInfo:
        """ë‹¤ìŒ ì²­í¬ ì •ë³´ ìƒì„±"""
        chunk_id = f"{collection_state.symbol}_{collection_state.timeframe}_{chunk_index:05d}"

        # chunk_paramsì—ì„œ ì‹¤ì œ 'to' ê°’ ì¶”ì¶œ (ë¬¸ìì—´ â†’ datetime ë³€í™˜)
        current_time = datetime.now(timezone.utc)

        # 'to' íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜, ì—†ìœ¼ë©´ None ìœ ì§€
        if "to" in chunk_params and chunk_params["to"]:
            try:
                # ISO í˜•ì‹ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
                to_str = chunk_params["to"]
                if isinstance(to_str, str):
                    # ISO format ì²˜ë¦¬ (ì—…ë¹„íŠ¸ APIëŠ” 'YYYY-MM-DDTHH:MM:SS' í˜•ì‹)
                    to_datetime = datetime.fromisoformat(to_str.replace('Z', '+00:00'))
                else:
                    to_datetime = None
            except (ValueError, TypeError):
                logger.warning(f"'to' íŒŒë¼ë¯¸í„° íŒŒì‹± ì‹¤íŒ¨: {chunk_params.get('to')}, None ì‚¬ìš©")
                to_datetime = None
        else:
            to_datetime = None  # â† ì¤‘ìš”: COUNT_ONLYëŠ” None ìœ ì§€

        chunk_info = ChunkInfo(
            chunk_id=chunk_id,
            chunk_index=chunk_index,
            symbol=collection_state.symbol,
            timeframe=collection_state.timeframe,
            count=chunk_params["count"],
            to=to_datetime,  # COUNT_ONLY ì²« ì²­í¬ëŠ” None
            end=current_time,  # endëŠ” ì¶”í›„ API ì‘ë‹µ í›„ ì—…ë°ì´íŠ¸
            status="pending"
        )

        return chunk_info

    def _update_remaining_time_estimates(self, state: CollectionState):
        """ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¶”ì • ì—…ë°ì´íŠ¸"""
        current_time = datetime.now(timezone.utc)

        # ì™„ë£Œëœ ì²­í¬ ìˆ˜
        completed_chunks_count = len(state.completed_chunks)

        if completed_chunks_count == 0:
            # ì•„ì§ ì™„ë£Œëœ ì²­í¬ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ì¶”ì •ê°’ ì‚¬ìš©
            return

        # í‰ê·  ì²­í¬ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_elapsed = (current_time - state.start_time).total_seconds()
        state.avg_chunk_duration = total_elapsed / completed_chunks_count

        # ë‚¨ì€ ì²­í¬ ìˆ˜ ê³„ì‚°
        state.remaining_chunks = state.estimated_total_chunks - completed_chunks_count

        # ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¶”ì •
        if state.remaining_chunks > 0:
            state.estimated_remaining_seconds = state.remaining_chunks * state.avg_chunk_duration
        else:
            state.estimated_remaining_seconds = 0.0

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ì—…ë°ì´íŠ¸
        if state.estimated_remaining_seconds > 0:
            state.estimated_completion_time = current_time + timedelta(
                seconds=state.estimated_remaining_seconds
            )

        # ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ë¡
        state.last_update_time = current_time

        logger.debug(f"ë‚¨ì€ ì‹œê°„ ì—…ë°ì´íŠ¸: í‰ê·  ì²­í¬ ì‹œê°„ {state.avg_chunk_duration:.2f}ì´ˆ, "
                     f"ë‚¨ì€ ì²­í¬ {state.remaining_chunks}ê°œ, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ {state.estimated_remaining_seconds:.1f}ì´ˆ")

    def get_realtime_remaining_time(self, request_id: str) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì •ë³´ ì¡°íšŒ"""
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]
        current_time = datetime.now(timezone.utc)

        # ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¬ê³„ì‚° (ìµœì‹  ì •ë³´ ë°˜ì˜)
        if len(state.completed_chunks) > 0:
            self._update_remaining_time_estimates(state)

        return {
            "request_id": request_id,
            "current_time": current_time.isoformat(),
            "avg_chunk_duration": state.avg_chunk_duration,
            "remaining_chunks": state.remaining_chunks,
            "estimated_remaining_seconds": state.estimated_remaining_seconds,
            "estimated_completion_time": (
                state.estimated_completion_time.isoformat()
                if state.estimated_completion_time else None
            ),
            "progress_percentage": round(
                (state.total_collected / state.total_requested) * 100, 1
            ),
            "is_on_track": self._is_collection_on_track(state),
            "performance_info": self._get_performance_info(state)
        }

    def _is_collection_on_track(self, state: CollectionState) -> bool:
        """ìˆ˜ì§‘ì´ ì˜ˆì •ëŒ€ë¡œ ì§„í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸"""
        if len(state.completed_chunks) == 0:
            return True  # ì•„ì§ ì‹œì‘ ë‹¨ê³„

        # ì´ˆê¸° ì˜ˆìƒ ì‹œê°„ê³¼ í˜„ì¬ í‰ê·  ì‹œê°„ ë¹„êµ
        initial_expected_duration = state.estimated_total_chunks / self.api_rate_limit_rps

        # í˜„ì¬ í‰ê· ì´ ì´ˆê¸° ì˜ˆìƒì˜ 120% ì´ë‚´ë©´ ì •ìƒ
        return state.avg_chunk_duration <= initial_expected_duration * 1.2

    def _get_performance_info(self, state: CollectionState) -> Dict[str, Any]:
        """ìˆ˜ì§‘ ì„±ëŠ¥ ì •ë³´ ì œê³µ"""
        if len(state.completed_chunks) == 0:
            return {"status": "ì´ˆê¸° ë‹¨ê³„"}

        initial_expected_rps = self.api_rate_limit_rps
        current_rps = 1.0 / state.avg_chunk_duration if state.avg_chunk_duration > 0 else 0

        return {
            "expected_rps": initial_expected_rps,
            "current_rps": round(current_rps, 2),
            "efficiency_percentage": round((current_rps / initial_expected_rps) * 100, 1),
            "avg_chunk_duration": state.avg_chunk_duration,
            "total_chunks_completed": len(state.completed_chunks)
        }

    def cleanup_completed_collections(self, max_age_hours: int = 24):
        """ì™„ë£Œëœ ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬"""
        current_time = datetime.now(timezone.utc)
        cutoff_time = current_time - timedelta(hours=max_age_hours)

        completed_ids = []
        for request_id, state in self.active_collections.items():
            if state.is_completed and state.start_time < cutoff_time:
                completed_ids.append(request_id)

        for request_id in completed_ids:
            del self.active_collections[request_id]
            logger.debug(f"ì™„ë£Œëœ ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬: {request_id}")

        if completed_ids:
            logger.info(f"ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬ ì™„ë£Œ: {len(completed_ids)}ê°œ")

    def _calculate_chunk_end_time(self, chunk_info: ChunkInfo) -> datetime:
        """ì²­í¬ ìš”ì²­ì˜ ì˜ˆìƒ ì¢…ë£Œ ì‹œì  ê³„ì‚°"""
        # ì—…ë¹„íŠ¸ APIëŠ” ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ to ì‹œì ì—ì„œ countë§Œí¼ ì´ì „ìœ¼ë¡œ ê³„ì‚°
        timeframe_seconds = TimeUtils.get_timeframe_delta(chunk_info.timeframe).total_seconds()
        end_time = chunk_info.to - timedelta(seconds=(chunk_info.count - 1) * timeframe_seconds)

        logger.debug(f"ì²­í¬ ì¢…ë£Œ ì‹œê°„ ê³„ì‚°: {chunk_info.to} â†’ {end_time} (ë‚´ë¦¼ì°¨ìˆœ {chunk_info.count}ê°œ)")
        return end_time

    async def _collect_data_by_overlap_strategy(
        self,
        chunk_info: ChunkInfo,
        overlap_result
    ) -> tuple[list, bool]:
        """ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘"""
        from upbit_auto_trading.infrastructure.market_data.candle.candle_models import OverlapStatus

        status = overlap_result.status

        if status == OverlapStatus.COMPLETE_OVERLAP:
            # ì™„ì „ ê²¹ì¹¨: DBì—ì„œë§Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            logger.debug("ì™„ì „ ê²¹ì¹¨ ê°ì§€ â†’ DBì—ì„œ ë°ì´í„° ì¡°íšŒ")
            db_candles = await self.repository.get_candles_by_range(
                chunk_info.symbol,
                chunk_info.timeframe,
                overlap_result.db_start,
                overlap_result.db_end
            )
            return db_candles, False  # API í˜¸ì¶œ ì—†ìŒ

        elif status == OverlapStatus.NO_OVERLAP:
            # ê²¹ì¹¨ ì—†ìŒ: APIì—ì„œë§Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            logger.debug("ê²¹ì¹¨ ì—†ìŒ â†’ ì „ì²´ API ìš”ì²­")
            api_response = await self._fetch_chunk_from_api(chunk_info)
            candle_data_list = self._convert_upbit_response_to_candles(
                api_response, chunk_info.timeframe
            )
            return candle_data_list, True

        elif status in [OverlapStatus.PARTIAL_START, OverlapStatus.PARTIAL_MIDDLE_CONTINUOUS]:
            # ë¶€ë¶„ ê²¹ì¹¨: DB + API í˜¼í•© ì²˜ë¦¬
            logger.debug(f"ë¶€ë¶„ ê²¹ì¹¨ ({status.value}) â†’ DB + API í˜¼í•© ì²˜ë¦¬")

            # DB ë°ì´í„° ìˆ˜ì§‘
            db_candles = []
            if overlap_result.db_start and overlap_result.db_end:
                db_candles = await self.repository.get_candles_by_range(
                    chunk_info.symbol,
                    chunk_info.timeframe,
                    overlap_result.db_start,
                    overlap_result.db_end
                )

            # API ë°ì´í„° ìˆ˜ì§‘
            api_candles = []
            if overlap_result.api_start and overlap_result.api_end:
                # ì„ì‹œ ì²­í¬ ì •ë³´ë¡œ API í˜¸ì¶œ
                api_chunk = ChunkInfo(
                    chunk_id=f"{chunk_info.chunk_id}_api_partial",
                    chunk_index=chunk_info.chunk_index,
                    symbol=chunk_info.symbol,
                    timeframe=chunk_info.timeframe,
                    count=self._calculate_api_count(overlap_result.api_start, overlap_result.api_end, chunk_info.timeframe),
                    to=overlap_result.api_start,
                    end=overlap_result.api_end,
                    status="pending"
                )

                api_response = await self._fetch_chunk_from_api(api_chunk)
                api_candles = self._convert_upbit_response_to_candles(
                    api_response, chunk_info.timeframe
                )

            # ì‹œê°„ìˆœìœ¼ë¡œ ë³‘í•© (ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ê¸°ì¤€)
            combined_candles = self._merge_candles_by_time(db_candles, api_candles)
            return combined_candles, True

        else:
            # PARTIAL_MIDDLE_FRAGMENT ë˜ëŠ” ê¸°íƒ€: ì•ˆì „í•œ í´ë°± â†’ ì „ì²´ API ìš”ì²­
            logger.warning(f"ë³µì¡í•œ ê²¹ì¹¨ ìƒíƒœ ({status.value}) â†’ API í´ë°±")
            api_response = await self._fetch_chunk_from_api(chunk_info)
            candle_data_list = self._convert_upbit_response_to_candles(
                api_response, chunk_info.timeframe
            )
            return candle_data_list, True

    def _ensure_utc_timezone(self, dt: datetime) -> datetime:
        """DateTimeì´ timezoneì„ ê°€ì§€ì§€ ì•Šìœ¼ë©´ UTCë¡œ ì„¤ì •"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt

    def _calculate_api_count(self, start_time: datetime, end_time: datetime, timeframe: str) -> int:
        """API ìš”ì²­ì— í•„ìš”í•œ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° (ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ê¸°ì¤€)"""
        timeframe_seconds = TimeUtils.get_timeframe_delta(timeframe).total_seconds()
        time_diff = (start_time - end_time).total_seconds()  # ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ start > end
        return max(1, int(time_diff / timeframe_seconds) + 1)

    def _merge_candles_by_time(self, db_candles: list, api_candles: list) -> list:
        """ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìº”ë“¤ ë°ì´í„° ë³‘í•© (ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ìœ ì§€)"""
        # ëª¨ë“  ìº”ë“¤ì„ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±° í›„ ì‹œê°„ìˆœ ì •ë ¬
        all_candles = db_candles + api_candles

        # candle_date_time_utc ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ë”•ì…”ë„ˆë¦¬ í™œìš©)
        unique_candles = {}
        for candle in all_candles:
            if hasattr(candle, 'candle_date_time_utc'):
                key = candle.candle_date_time_utc
            elif isinstance(candle, dict):
                key = candle.get('candle_date_time_utc')
            else:
                continue

            if key not in unique_candles:
                unique_candles[key] = candle

        # ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹  â†’ ê³¼ê±°)
        sorted_candles = sorted(
            unique_candles.values(),
            key=lambda x: (
                x.candle_date_time_utc if hasattr(x, 'candle_date_time_utc')
                else x.get('candle_date_time_utc', '')
            ),
            reverse=True  # ë‚´ë¦¼ì°¨ìˆœ
        )

        logger.debug(f"ìº”ë“¤ ë³‘í•© ì™„ë£Œ: DB {len(db_candles)}ê°œ + API {len(api_candles)}ê°œ â†’ ë³‘í•© {len(sorted_candles)}ê°œ")
        return sorted_candles
