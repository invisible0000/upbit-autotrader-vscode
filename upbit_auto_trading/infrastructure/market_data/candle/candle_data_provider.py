"""
CandleDataProvider v7.0 - ChunkProcessor ì™„ì „ í†µí•© ë²„ì „

Created: 2025-09-23
Updated: ChunkProcessor í†µí•©ìœ¼ë¡œ ì²­í¬ ì²˜ë¦¬ ë¡œì§ ì™„ì „ ë¶„ë¦¬
Purpose: ChunkProcessorë¥¼ í™œìš©í•œ ê¹”ë”í•˜ê³  íš¨ìœ¨ì ì¸ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘
Features: 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸, ì¡°ê¸° ì¢…ë£Œ ìµœì í™”, ì„±ëŠ¥ ì¶”ì , ë¬´í•œ ë£¨í”„ ë°©ì§€
Architecture: DDD Infrastructure ê³„ì¸µ, ì˜ì¡´ì„± ì£¼        - ë¹ˆ ìº”ë“¡ ì²˜ë¦¬ ë° ì •ê·œí™” íŒ¨í„´

ì£¼ìš” ê°œì„ ì‚¬í•­:
1. ChunkProcessor ì™„ì „ í†µí•©: ì²­í¬ ì²˜ë¦¬ ë¡œì§ì„ ChunkProcessorë¡œ ìœ„ì„
2. ì²­í¬ ì§„í–‰ ë¡œì§ ìˆ˜ì •: ë¬´í•œ ë£¨í”„ ë°©ì§€ ë° ì˜¬ë°”ë¥¸ ì²­í¬ ì¸ë±ìŠ¤ ê´€ë¦¬
3. ê²°ê³¼ ì²˜ë¦¬ ê°œì„ : ChunkResult ê¸°ë°˜ ìƒíƒœ ì—…ë°ì´íŠ¸
4. ì„±ëŠ¥ ìµœì í™”: 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
"""

from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any
from dataclasses import dataclass

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils
from upbit_auto_trading.infrastructure.market_data.candle.models import (
    ChunkInfo, CandleData, CollectionState, RequestInfo, RequestType
)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)
from upbit_auto_trading.infrastructure.market_data.candle.empty_candle_detector import (
    EmptyCandleDetector
)

logger = create_component_logger("CandleDataProvider")


@dataclass
class CollectionPlan:
    """ìˆ˜ì§‘ ê³„íš"""
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float
    first_chunk_params: Dict[str, Any]


class CandleDataProvider:
    """
    ìº”ë“¤ ë°ì´í„° ì œê³µì v7.0 - ChunkProcessor ì™„ì „ í†µí•© ë²„ì „

    ì£¼ìš” ê°œì„ ì‚¬í•­:
    1. ChunkProcessor ì™„ì „ í†µí•©: ì²­í¬ ì²˜ë¦¬ë¥¼ ì „ë¬¸ í´ë˜ìŠ¤ì— ìœ„ì„
    2. ì²­í¬ ì§„í–‰ ë¡œì§ ê°œì„ : ë¬´í•œ ë£¨í”„ ë°©ì§€ ë° ì˜¬ë°”ë¥¸ ìƒíƒœ ê´€ë¦¬
    3. ê²°ê³¼ ê¸°ë°˜ ì²˜ë¦¬: ChunkResultë¥¼ í†µí•œ ëª…í™•í•œ ìƒíƒœ ì „í™˜
    4. ì„±ëŠ¥ ìµœì í™”: 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”

    ì±…ì„:
    - ì „ì²´ ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ ì¡°ì • (Orchestrator ì—­í• )
    - Collection ìƒíƒœ ê´€ë¦¬
    - ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½
    - ìµœì¢… ê²°ê³¼ ì¡°íšŒ
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        chunk_size: int = 200,
        enable_empty_candle_processing: bool = True
    ):
        """CandleDataProvider v7.0 ì´ˆê¸°í™” (ChunkProcessor ì™„ì „ í†µí•©)"""
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜
        self.active_collections: Dict[str, CollectionState] = {}
        self.api_rate_limit_rps = 10  # 10 RPS ê¸°ì¤€

        # ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ì„¤ì • (í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€)
        self.enable_empty_candle_processing = enable_empty_candle_processing
        self.empty_candle_detectors: Dict[str, EmptyCandleDetector] = {}

        # ğŸ†• ChunkProcessor ì˜ì¡´ì„± ì£¼ì…
        from upbit_auto_trading.infrastructure.market_data.candle.chunk_processor import ChunkProcessor

        self.chunk_processor = ChunkProcessor(
            overlap_analyzer=overlap_analyzer,
            upbit_client=upbit_client,
            repository=repository,
            empty_candle_detector_factory=self._get_empty_candle_detector
        )

        logger.info("CandleDataProvider v7.0 (ChunkProcessor ì™„ì „ í†µí•©) ì´ˆê¸°í™”")
        logger.info(f"ì²­í¬ í¬ê¸°: {self.chunk_size}, API Rate Limit: {self.api_rate_limit_rps} RPS")
        logger.info(f"ë¹ˆ ìº”ë“¤ ì²˜ë¦¬: {'í™œì„±í™”' if enable_empty_candle_processing else 'ë¹„í™œì„±í™”'}")

    def _get_empty_candle_detector(self, symbol: str, timeframe: str) -> EmptyCandleDetector:
        """(symbol, timeframe) ì¡°í•©ë³„ EmptyCandleDetector ìºì‹œ"""
        cache_key = f"{symbol}_{timeframe}"
        if cache_key not in self.empty_candle_detectors:
            self.empty_candle_detectors[cache_key] = EmptyCandleDetector(symbol, timeframe)
            logger.debug(f"EmptyCandleDetector ìƒì„±: {symbol} {timeframe}")
        return self.empty_candle_detectors[cache_key]

    # =========================================================================
    # í•µì‹¬ ê³µê°œ API
    # =========================================================================

    async def get_candles(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> List[CandleData]:
        """
        ì™„ì „ ìë™í™”ëœ ìº”ë“¤ ìˆ˜ì§‘ - ChunkProcessor í†µí•© ë²„ì „

        ChunkProcessorë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘
        """
        logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ìš”ì²­: {symbol} {timeframe}")
        if count:
            logger.info(f"ê°œìˆ˜: {count}ê°œ")
        if to:
            logger.info(f"ì‹œì‘: {to}")
        if end:
            logger.info(f"ì¢…ë£Œ: {end}")

        # ğŸš€ UTC í†µì¼: ì§„ì…ì ì—ì„œ í•œ ë²ˆë§Œ ì •ê·œí™”
        normalized_to = TimeUtils.normalize_datetime_to_utc(to)
        normalized_end = TimeUtils.normalize_datetime_to_utc(end)

        logger.debug(f"UTC ì •ê·œí™”: to={to} â†’ {normalized_to}, end={end} â†’ {normalized_end}")

        # ìˆ˜ì§‘ ì‹œì‘
        request_id = self.start_collection(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=normalized_to,
            end=normalized_end
        )

        try:
            # ì²­í¬ë³„ ìë™ ì²˜ë¦¬ ë£¨í”„ (ChunkProcessor í™œìš©)
            while True:
                chunk_info = self.get_next_chunk(request_id)
                if chunk_info is None:
                    break

                # ChunkProcessorë¥¼ í†µí•œ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
                is_collection_complete = await self.mark_chunk_completed(request_id)
                if is_collection_complete:
                    break

            # ìˆ˜ì§‘ ìƒíƒœì—ì„œ ê²°ê³¼ ì¶”ì¶œ
            collection_state = self.active_collections.get(request_id)
            if collection_state and collection_state.is_completed:
                logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ì™„ë£Œ: {collection_state.total_collected}ê°œ")

                # Repositoryì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° ì¡°íšŒ
                collected_candles = await self._get_final_result(
                    collection_state, symbol, timeframe, count, to, end
                )
                return collected_candles

            return []

        except Exception as e:
            logger.error(f"ìº”ë“¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise
        finally:
            # ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬ (ë©”ëª¨ë¦¬ í•´ì œ)
            if request_id in self.active_collections:
                del self.active_collections[request_id]

    def start_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> str:
        """ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘"""
        # RequestInfo ìƒì„± (ê²€ì¦ í¬í•¨)
        request_info = RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=to,
            end=end
        )

        logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘: {request_info.to_log_string()}")

        # ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½
        plan = self.plan_collection(symbol, timeframe, count, to, end)

        # ìš”ì²­ ID ìƒì„±
        request_id = f"{symbol}_{timeframe}_{int(datetime.now().timestamp())}"

        # ìˆ˜ì§‘ ìƒíƒœ ì´ˆê¸°í™”
        estimated_completion = datetime.now(timezone.utc) + timedelta(
            seconds=plan.estimated_duration_seconds
        )

        collection_state = CollectionState(
            request_id=request_id,
            request_info=request_info,
            symbol=symbol,
            timeframe=timeframe,
            total_requested=plan.total_count,
            estimated_total_chunks=plan.estimated_chunks,
            estimated_completion_time=estimated_completion,
            remaining_chunks=plan.estimated_chunks,
            estimated_remaining_seconds=plan.estimated_duration_seconds
        )

        # ì²« ë²ˆì§¸ ì²­í¬ ìƒì„±
        first_chunk = self._create_next_chunk(
            collection_state=collection_state,
            chunk_params=plan.first_chunk_params,
            chunk_index=0
        )
        collection_state.current_chunk = first_chunk

        # ìƒíƒœ ë“±ë¡
        self.active_collections[request_id] = collection_state

        logger.info(f"ìˆ˜ì§‘ ì‹œì‘ ì™„ë£Œ: ìš”ì²­ ID {request_id}")
        return request_id

    def get_next_chunk(self, request_id: str) -> Optional[ChunkInfo]:
        """ë‹¤ìŒ ì²˜ë¦¬í•  ì²­í¬ ì •ë³´ ë°˜í™˜"""
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.is_completed:
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œë¨: {request_id}")
            return None

        if state.current_chunk is None:
            logger.warning(f"ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {request_id}")
            return None

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ë°˜í™˜: {state.current_chunk.chunk_id}")
        return state.current_chunk

    async def mark_chunk_completed(self, request_id: str) -> bool:
        """
        âœ¨ ChunkProcessor í†µí•© ì²­í¬ ì™„ë£Œ ì²˜ë¦¬

        ChunkProcessorì˜ 4ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ì„ í™œìš©í•˜ì—¬:
        - ì¤€ë¹„ ë° ê²¹ì¹¨ ë¶„ì„
        - ìµœì í™”ëœ API ë°ì´í„° ìˆ˜ì§‘
        - ë¹ˆ ìº”ë“¤ ì²˜ë¦¬ ë° ì •ê·œí™”
        - Repositoryë¥¼ í†µí•œ ë°ì´í„° ì €ì¥        Returns:
            bool: ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]
        current_chunk = state.current_chunk

        if current_chunk is None:
            raise ValueError("ì²˜ë¦¬ ì¤‘ì¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.info(f"ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {current_chunk.chunk_id}")

        try:
            # ğŸš€ í•µì‹¬: ChunkProcessorì— ìœ„ì„
            chunk_result = await self.chunk_processor.execute_chunk_pipeline(
                current_chunk, state
            )

            # ê²°ê³¼ ì²˜ë¦¬ ë° ìƒíƒœ ì—…ë°ì´íŠ¸
            self._process_chunk_result(state, chunk_result)

            # ğŸ†• ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ í™•ì¸ (ìµœìš°ì„  ì¢…ë£Œ ì¡°ê±´)
            if hasattr(state, 'reached_upbit_data_end') and state.reached_upbit_data_end:
                state.is_completed = True
                state.current_chunk = None
                logger.info(f"ğŸ”´ ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ë¡œ ìˆ˜ì§‘ ì™„ë£Œ: {request_id}")
                return True

            # ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸
            if self._is_collection_complete(state):
                state.is_completed = True
                state.current_chunk = None
                logger.info(f"ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {request_id}")
                return True

            # ë‹¤ìŒ ì²­í¬ ì¤€ë¹„
            request_type = state.request_info.get_request_type()
            self._prepare_next_chunk(state, request_type)
            return False

        except Exception as e:
            # ì²­í¬ ì‹¤íŒ¨ ì²˜ë¦¬
            if state.current_chunk:
                state.current_chunk.status = "failed"
                state.error_message = str(e)
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {current_chunk.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    def _process_chunk_result(self, state: CollectionState, chunk_result) -> None:
        """ì²­í¬ ì²˜ë¦¬ ê²°ê³¼ë¥¼ CollectionStateì— ë°˜ì˜"""
        from upbit_auto_trading.infrastructure.market_data.candle.models import ChunkResultStatus

        if chunk_result.is_successful():
            # ì„±ê³µì ì¸ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
            completed_chunk = state.current_chunk
            completed_chunk.status = "completed"
            state.completed_chunks.append(completed_chunk)

            # ğŸ”„ ìˆ˜ì •ëœ ì¹´ìš´íŒ… ë¡œì§: ì‹¤ì œ ì €ì¥ëœ ê°œìˆ˜ ê¸°ì¤€
            # ê¸°ì¡´: ì²­í¬ ë‹´ë‹¹ ë²”ìœ„ ê¸°ì¤€ â†’ ìˆ˜ì •: ì‹¤ì œ ì €ì¥ ê°œìˆ˜ ê¸°ì¤€
            state.total_collected += chunk_result.saved_count

            # ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ í™•ì¸
            if chunk_result.status == ChunkResultStatus.EARLY_EXIT:
                state.reached_upbit_data_end = True
                logger.warning(f"ğŸ“Š ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬: {completed_chunk.symbol} {completed_chunk.timeframe}")

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            state.last_update_time = datetime.now(timezone.utc)

            logger.info(f"ì²­í¬ ì™„ë£Œ: {completed_chunk.chunk_id}, "
                        f"ì €ì¥: {chunk_result.saved_count}ê°œ, "
                        f"ëˆ„ì : {state.total_collected}/{state.total_requested}")

            # ìƒì„¸í•œ ì²­í¬ ì²˜ë¦¬ ìš”ì•½ (ë””ë²„ê¹…ìš©)
            if logger.level <= 10:  # DEBUG ë ˆë²¨ì¼ ë•Œë§Œ
                if hasattr(completed_chunk, 'get_processing_summary'):
                    summary = completed_chunk.get_processing_summary()
                    logger.debug(f"\n{summary}")

        else:
            # ì‹¤íŒ¨ ì²˜ë¦¬
            state.current_chunk.status = "failed"
            state.error_message = str(chunk_result.error)
            raise chunk_result.error

    # =========================================================================
    # ìˆ˜ì§‘ ê³„íš ë° ì²­í¬ ê´€ë¦¬
    # =========================================================================

    def plan_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> CollectionPlan:
        """ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½"""
        # RequestInfo ìƒì„± (ê²€ì¦ í¬í•¨)
        request_info = RequestInfo(
            symbol=symbol,
            timeframe=timeframe,
            count=count,
            to=to,
            end=end
        )

        logger.info(f"ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½: {request_info.to_internal_log_string()}")

        # ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦ - ìš”ì²­ ì‹œì  ê¸°ì¤€
        if to is not None and to > request_info.request_at:
            raise ValueError(f"to ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {to}")
        if end is not None and end > request_info.request_at:
            raise ValueError(f"end ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {end}")

        # ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
        total_count = self._calculate_total_count_by_type(request_info)

        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚°
        estimated_chunks = (total_count + self.chunk_size - 1) // self.chunk_size

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (10 RPS ê¸°ì¤€)
        estimated_duration_seconds = estimated_chunks / self.api_rate_limit_rps

        # ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±
        first_chunk_params = self._create_first_chunk_params_by_type(request_info)

        plan = CollectionPlan(
            total_count=total_count,
            estimated_chunks=estimated_chunks,
            estimated_duration_seconds=estimated_duration_seconds,
            first_chunk_params=first_chunk_params
        )

        logger.info(f"ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {total_count:,}ê°œ ìº”ë“¤, {estimated_chunks}ì²­í¬, "
                    f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {estimated_duration_seconds:.1f}ì´ˆ")
        return plan

    def _create_next_chunk(
        self,
        collection_state: CollectionState,
        chunk_params: Dict[str, Any],
        chunk_index: int
    ) -> ChunkInfo:
        """ë‹¤ìŒ ì²­í¬ ì •ë³´ ìƒì„±"""
        chunk_id = f"{collection_state.symbol}_{collection_state.timeframe}_{chunk_index:05d}"

        # 'to' íŒŒë¼ë¯¸í„° ì²˜ë¦¬
        if "to" in chunk_params and chunk_params["to"]:
            to_param = chunk_params["to"]
            if isinstance(to_param, datetime):
                to_datetime = to_param
            elif isinstance(to_param, str):
                try:
                    to_datetime = datetime.fromisoformat(to_param)
                except (ValueError, TypeError):
                    logger.warning(f"'to' íŒŒë¼ë¯¸í„° íŒŒì‹± ì‹¤íŒ¨: {to_param}")
                    to_datetime = None
            else:
                logger.warning(f"'to' íŒŒë¼ë¯¸í„° íƒ€ì… ì˜¤ë¥˜: {type(to_param)}")
                to_datetime = None
        else:
            to_datetime = None  # COUNT_ONLYëŠ” None ìœ ì§€

        # end ì‹œê°„ ê³„ì‚° - ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë
        calculated_end = None
        if to_datetime and chunk_params.get("count"):
            # toì™€ countê°€ ìˆìœ¼ë©´ ì‹¤ì œ ë°ì´í„° ë²”ìœ„ ë ê³„ì‚°
            calculated_end = TimeUtils.get_time_by_ticks(
                to_datetime,
                collection_state.timeframe,
                -(chunk_params["count"] - 1)
            )

        chunk_info = ChunkInfo(
            chunk_id=chunk_id,
            chunk_index=chunk_index,
            symbol=collection_state.symbol,
            timeframe=collection_state.timeframe,
            count=chunk_params["count"],
            to=to_datetime,
            end=calculated_end,
            status="pending"
        )

        return chunk_info

    def _is_collection_complete(self, state: CollectionState) -> bool:
        """ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€ í™•ì¸"""
        # ê¸°ë³¸ ê°œìˆ˜ ê¸°ì¤€ ì™„ë£Œ í™•ì¸
        count_reached = state.total_collected >= state.total_requested

        # ì—…ë¹„íŠ¸ ë°ì´í„° ë ë„ë‹¬ í™•ì¸
        upbit_data_end = getattr(state, 'reached_upbit_data_end', False)

        # ì‹œê°„ ê¸°ì¤€ ì™„ë£Œ í™•ì¸ (TO_END, END_ONLY íƒ€ì…)
        time_reached = False
        request_type = state.request_info.get_request_type()
        if request_type in [RequestType.TO_END, RequestType.END_ONLY] and state.completed_chunks:
            # ë§ˆì§€ë§‰ ì™„ë£Œëœ ì²­í¬ì˜ ì‹œê°„ ì •ë³´ í™•ì¸
            last_chunk = state.completed_chunks[-1]
            if hasattr(last_chunk, 'get_effective_end_time'):
                last_effective_time = last_chunk.get_effective_end_time()
                target_end = state.request_info.end
                if last_effective_time and target_end:
                    time_reached = last_effective_time <= target_end

        should_complete = count_reached or upbit_data_end or time_reached

        if should_complete:
            completion_reasons = []
            if count_reached:
                completion_reasons.append("ê°œìˆ˜ë‹¬ì„±")
            if upbit_data_end:
                completion_reasons.append("ì—…ë¹„íŠ¸ë°ì´í„°ë")
            if time_reached:
                completion_reasons.append("ì‹œê°„ë„ë‹¬")

            logger.debug(f"ğŸ¯ ìˆ˜ì§‘ ì™„ë£Œ: {', '.join(completion_reasons)}")

        return should_complete

    def _prepare_next_chunk(self, state: CollectionState, request_type: RequestType):
        """ë‹¤ìŒ ì²­í¬ ì¤€ë¹„ - ë¬´í•œ ë£¨í”„ ë°©ì§€"""
        # ğŸ”„ ìˆ˜ì •: ì²­í¬ ì¸ë±ìŠ¤ë¥¼ completed_chunks ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ ê³„ì‚°
        next_chunk_index = len(state.completed_chunks)
        remaining_count = state.total_requested - state.total_collected

        # ë‚¨ì€ ê°œìˆ˜ê°€ 0 ì´í•˜ë©´ ìˆ˜ì§‘ ì™„ë£Œ
        if remaining_count <= 0:
            logger.info("ë‚¨ì€ ìˆ˜ì§‘ ê°œìˆ˜ê°€ ì—†ì–´ ìˆ˜ì§‘ ì™„ë£Œ")
            state.is_completed = True
            state.current_chunk = None
            return

        next_chunk_size = min(remaining_count, self.chunk_size)

        next_chunk_params = self._create_next_chunk_params(
            state, next_chunk_size, request_type
        )

        next_chunk = self._create_next_chunk(
            collection_state=state,
            chunk_params=next_chunk_params,
            chunk_index=next_chunk_index
        )
        state.current_chunk = next_chunk

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ìƒì„±: {next_chunk.chunk_id} (ì¸ë±ìŠ¤: {next_chunk_index})")

    def _create_next_chunk_params(
        self,
        state: CollectionState,
        chunk_size: int,
        request_type: RequestType
    ) -> Dict[str, Any]:
        """ë‹¤ìŒ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„± - ì—°ì†ì„± ë³´ì¥"""
        params = {
            "market": state.symbol,
            "count": chunk_size
        }

        # ğŸ”„ ChunkResult ê¸°ë°˜ ì—°ì†ì„± (ì™„ë£Œëœ ì²­í¬ë“¤ì—ì„œ ë§ˆì§€ë§‰ ì‹œê°„ ì •ë³´ ì¶”ì¶œ)
        if state.completed_chunks:
            last_chunk = state.completed_chunks[-1]
            if hasattr(last_chunk, 'get_effective_end_time'):
                last_effective_time = last_chunk.get_effective_end_time()
                if last_effective_time:
                    try:
                        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ = ì´ì „ ì²­í¬ ìœ íš¨ ëì‹œê°„ - 1í‹± (ì—°ì†ì„± ë³´ì¥)
                        next_chunk_start = TimeUtils.get_time_by_ticks(last_effective_time, state.timeframe, -1)
                        params["to"] = next_chunk_start

                        time_source = last_chunk.get_time_source() if hasattr(last_chunk, 'get_time_source') else "unknown"
                        logger.debug(f"ChunkResult ì—°ì†ì„±: {last_effective_time} (ì¶œì²˜: {time_source}) â†’ {next_chunk_start}")

                    except Exception as e:
                        logger.warning(f"ChunkResult ì—°ì†ì„± ê³„ì‚° ì‹¤íŒ¨: {e}")

        return params

    # =========================================================================
    # íŒŒë¼ë¯¸í„° ìƒì„± ë° ìœ í‹¸ë¦¬í‹°
    # =========================================================================

    def _calculate_total_count_by_type(
        self,
        request_info: RequestInfo
    ) -> int:
        """ì‚¬ì „ ê³„ì‚°ëœ ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ë°˜í™˜ (í•­ìƒ ì¡´ì¬ ë³´ì¥)"""
        return request_info.get_expected_count()

    def _create_first_chunk_params_by_type(
        self,
        request_info: RequestInfo
    ) -> Dict[str, Any]:
        """ìš”ì²­ íƒ€ì…ë³„ ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±"""
        request_type = request_info.get_request_type()
        params = {"market": request_info.symbol}

        if request_type == RequestType.COUNT_ONLY:
            # COUNT_ONLY: to íŒŒë¼ë¯¸í„° ì—†ì´ countë§Œ ì‚¬ìš©
            chunk_size = min(request_info.count, self.chunk_size)
            params["count"] = chunk_size
            logger.debug(f"COUNT_ONLY: count={chunk_size} (ì„œë²„ ìµœì‹ ë¶€í„°)")

        elif request_type == RequestType.TO_COUNT:
            # to + count: ì‚¬ì „ ê³„ì‚°ëœ ì •ë ¬ ì‹œê°„ ì‚¬ìš©
            chunk_size = min(request_info.count, self.chunk_size)
            aligned_to = request_info.get_aligned_to_time()

            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            first_chunk_start_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

            params["count"] = chunk_size
            params["to"] = first_chunk_start_time
            logger.debug(f"TO_COUNT: ì§„ì…ì ë³´ì • {aligned_to} â†’ {first_chunk_start_time}")

        elif request_type == RequestType.TO_END:
            # to + end: ì‚¬ì „ ê³„ì‚°ëœ ì •ë ¬ ì‹œê°„ ì‚¬ìš©
            aligned_to = request_info.get_aligned_to_time()

            # ì§„ì…ì  ë³´ì • (ì‚¬ìš©ì ì‹œê°„ â†’ ë‚´ë¶€ ì‹œê°„ ë³€í™˜)
            first_chunk_start_time = TimeUtils.get_time_by_ticks(aligned_to, request_info.timeframe, -1)

            params["count"] = self.chunk_size
            params["to"] = first_chunk_start_time
            logger.debug(f"TO_END: ì§„ì…ì ë³´ì • {aligned_to} â†’ {first_chunk_start_time}")

        elif request_type == RequestType.END_ONLY:
            # END_ONLY: COUNT_ONLYì²˜ëŸ¼ to ì—†ì´ countë§Œ ì‚¬ìš©
            params["count"] = self.chunk_size
            logger.debug(f"END_ONLY: count={self.chunk_size} (ì„œë²„ ìµœì‹ ë¶€í„°)")

        return params

    # =========================================================================
    # ìµœì¢… ê²°ê³¼ ì¡°íšŒ
    # =========================================================================

    async def _get_final_result(
        self,
        collection_state: CollectionState,
        symbol: str,
        timeframe: str,
        count: Optional[int],
        to: Optional[datetime],
        end: Optional[datetime]
    ) -> List[CandleData]:
        """ìµœì¢… ê²°ê³¼ ì¡°íšŒ (CandleData ë³€í™˜ì€ ì—¬ê¸°ì„œë§Œ ìˆ˜í–‰)"""
        try:
            # ì—…ë¹„íŠ¸ API íŠ¹ì„± ê³ ë ¤í•œ ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚°
            aligned_to = collection_state.request_info.get_aligned_to_time()
            expected_count = collection_state.request_info.get_expected_count()
            request_type = collection_state.request_info.get_request_type()

            # ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„ ê³„ì‚°
            if request_type in [RequestType.COUNT_ONLY, RequestType.END_ONLY]:
                # COUNT_ONLY/END_ONLY: ì²« ë²ˆì§¸ ì²­í¬ì˜ ì‹¤ì œ API ì‘ë‹µ ì‹œì‘ì  ì‚¬ìš©
                if collection_state.completed_chunks and hasattr(collection_state.completed_chunks[0], 'api_response_start'):
                    actual_start = collection_state.completed_chunks[0].api_response_start
                else:
                    actual_start = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)
            else:
                # TO_COUNT/TO_END: aligned_toì—ì„œ 1í‹± ê³¼ê±°ë¡œ ì´ë™
                actual_start = TimeUtils.get_time_by_ticks(aligned_to, timeframe, -1)

            # Count ê¸°ë°˜ ì¢…ë£Œì  ì¬ê³„ì‚°
            actual_end = TimeUtils.get_time_by_ticks(actual_start, timeframe, -(expected_count - 1))

            logger.debug(f"ğŸ” ì‹¤ì œ ìˆ˜ì§‘ ë²”ìœ„: {aligned_to} â†’ {actual_start} ~ {actual_end} ({expected_count}ê°œ)")

            return await self.repository.get_candles_by_range(
                symbol, timeframe, actual_start, actual_end
            )

        except Exception as e:
            logger.error(f"ìµœì¢… ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
