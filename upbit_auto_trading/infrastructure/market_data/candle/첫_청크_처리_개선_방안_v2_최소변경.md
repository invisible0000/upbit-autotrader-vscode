# 첫 청크 처리 개선 방안 v2.0 - 최소 변경 원칙

> 🎯 **핵심 목표**: 최소한의 코드 변경으로 첫 청크 처리 로직을 명확히 드러내고 복잡성 제거
> 📅 **작성일**: 2025-09-24
> 🔄 **버전**: v2.0 (사용자 의견 반영)

## 📋 사용자 의견 분석 및 반영

### 🔍 추가 의견 검토 결과

**1. `_create_first_chunk` 불필요성 동의**
- ✅ **정확한 지적**: 청크 생성 자체는 문제가 아님
- ✅ **핵심 문제**: 요청 타입에 따른 `_fetch_api_data` 파라미터 처리
- ✅ **결론**: 기존 `_create_chunk` 유지하고 첫 청크 처리 로직만 분리

**2. `_create_first_chunk_params_by_type()` 활용 방안**
- ✅ **로직의 가치 인정**: 요청 타입별 파라미터 생성 로직은 유용함
- ✅ **두 가지 방안 검토 필요**:
  - 방안A: `chunk_processor.py`로 이동하여 직접 활용
  - 방안B: 현재 위치에서 `_process_first_chunk` 내부적으로 활용
- ✅ **책임과 역할 고려**: 각 방안의 아키텍처적 장단점 분석 필요

**3. `_create_subsequent_chunk()` 불필요성 동의**
- ✅ **정확한 지적**: 별도 메서드 생성은 과도한 분리
- ✅ **기존 활용**: `_create_chunk` 수정하여 사용하는 것이 합리적
- ✅ **최소 변경 원칙**: 기존 메서드 최대한 활용

## 🎯 수정된 개선 방안 - 최소 변경 원칙

### 핵심 전략: "명확한 첫 청크 처리 + 최소 구조 변경"

## 1. **process_collection 메서드 - 명시적 첫 청크 처리**

**현재 구조 (문제점):**
```python
async def process_collection(self, ...) -> CollectionResult:
    # 1. RequestInfo 생성
    request_info = RequestInfo(...)
    plan = create_collection_plan(...)  # first_chunk_params 숨겨짐

    # 2. 청크별 순차 처리 (첫 청크 특별 처리 숨겨짐)
    for chunk_index in range(plan.estimated_chunks):
        chunk = self._create_chunk(...)  # 내부에서 첫 청크 구분
        await self._process_single_chunk(chunk)  # 내부에서 첫 청크 체크
```

**개선된 구조 (최소 변경):**
```python
async def process_collection(self, ...) -> CollectionResult:
    # 1. RequestInfo 생성 (기존 유지)
    request_info = RequestInfo(...)
    plan = create_collection_plan(...)

    # 2. 첫 청크 명시적 처리 ⭐️ 핵심 개선
    first_chunk = self._create_chunk(0, request_info, plan, [])  # 기존 메서드 활용
    await self._process_first_chunk(request_info, first_chunk)   # 새 메서드
    chunks = [first_chunk]

    # 3. 완료 조건 확인 (첫 청크만으로 완료 가능)
    if should_complete_collection(request_info, chunks):
        return self._create_success_result(chunks, request_info)

    # 4. 후속 청크 반복 처리 (필요한 경우만)
    chunk_index = 1
    while not should_complete_collection(request_info, chunks):
        next_chunk = self._create_chunk(chunk_index, request_info, plan, chunks)
        await self._process_single_chunk(next_chunk)  # 첫 청크 로직 제거된 순수 버전
        chunks.append(next_chunk)
        chunk_index += 1

    return self._create_success_result(chunks, request_info)
```

**핵심 개선점:**
- ✅ 첫 청크 처리가 명확히 드러남
- ✅ 기존 `_create_chunk` 메서드 그대로 활용 (최소 변경)
- ✅ 새로운 `_process_first_chunk` 메서드로 첫 청크 로직 집중

## 2. **_create_first_chunk_params_by_type 활용 방안 분석**

### 방안 A: chunk_processor.py로 이동 (직접 활용)

**장점:**
- 첫 청크 처리 로직이 한 곳에 집중됨
- `_process_first_chunk`에서 직접 파라미터 생성 가능
- candle_business_models.py의 복잡성 감소

**단점:**
- 비즈니스 모델 계층에서 인프라 계층으로 로직 이동 (계층 역전)
- RequestType별 파라미터 생성이 두 곳에 분산될 위험

**구현 예시:**
```python
# chunk_processor.py에 추가
def _create_first_chunk_params(self, request_info: RequestInfo) -> Dict[str, Any]:
    """첫 청크 API 파라미터 생성 (candle_business_models에서 이동)"""
    # _create_first_chunk_params_by_type 로직 이식
    pass

async def _process_first_chunk(self, request_info: RequestInfo, chunk: ChunkInfo) -> None:
    """첫 청크 전용 처리"""
    # 1. API 파라미터 직접 생성
    api_params = self._create_first_chunk_params(request_info)

    # 2. 겹침 분석 조건부 실행
    if not request_info.should_skip_overlap_analysis_for_first_chunk():
        overlap_result = await self._analyze_chunk_overlap(chunk)
        chunk.set_overlap_info(overlap_result)

    # 3. API 데이터 수집
    api_data = await self._fetch_api_data_direct(api_params)
    # ...
```

### 방안 B: 현재 위치 유지 (내부적 활용) ⭐️ **권장**

**장점:**
- 계층 구조 유지 (비즈니스 모델이 파라미터 생성 담당)
- 기존 코드 최소 변경
- RequestType별 로직이 한 곳에 집중 유지

**단점:**
- candle_business_models.py와 chunk_processor.py 간 의존성 유지
- 첫 청크 로직이 약간 분산됨

**구현 예시:**
```python
# candle_business_models.py (기존 유지)
def _create_first_chunk_params_by_type(request_info: RequestInfo, chunk_size: int) -> Dict[str, Any]:
    # 기존 로직 그대로 유지
    pass

# chunk_processor.py
async def _process_first_chunk(self, request_info: RequestInfo, chunk: ChunkInfo) -> None:
    """첫 청크 전용 처리"""
    # 1. 기존 유틸리티 함수 활용하여 API 파라미터 생성
    from upbit_auto_trading.infrastructure.market_data.candle.models.candle_business_models import (
        _create_first_chunk_params_by_type
    )
    api_params = _create_first_chunk_params_by_type(request_info, self.chunk_size)

    # 2. 겹침 분석 조건부 실행
    if not request_info.should_skip_overlap_analysis_for_first_chunk():
        overlap_result = await self._analyze_chunk_overlap(chunk)
        chunk.set_overlap_info(overlap_result)

    # 3. 직접 파라미터로 API 호출
    symbol = api_params['market']
    count = api_params['count']
    to = api_params.get('to')

    api_data = await self._fetch_api_data(
        symbol=symbol,
        timeframe=request_info.timeframe,
        count=count,
        to=to
    )
    # ...
```

**💡 방안 B 권장 이유:**
- 최소 변경 원칙에 부합
- 기존 아키텍처 구조 유지
- 첫 청크 로직을 명확히 드러내면서도 기존 유틸리티 최대 활용

## 3. **_create_chunk 메서드 수정 (최소 변경)**

**현재 문제점:**
- `chunk_index == 0` 분기 처리로 복잡성 증가
- `plan.first_chunk_params` 의존성

**개선 방향:**
```python
def _create_chunk(
    self,
    chunk_index: int,
    request_info: RequestInfo,
    plan: CollectionPlan,
    completed_chunks: List[ChunkInfo]
) -> ChunkInfo:
    """청크 생성 - 첫/후속 청크 통합 처리 (최소 수정)"""

    if chunk_index == 0:
        # 첫 청크: request_info 기반 직접 계산 (plan.first_chunk_params 의존성 제거)
        collected_count = 0
        remaining_count = request_info.expected_count
        chunk_count = min(remaining_count, self.chunk_size)

        # RequestInfo의 사전 계산된 값 활용
        to_time = request_info.get_aligned_to_time() if request_info.should_align_time() else None
        if to_time and request_info.get_request_type() in [RequestType.TO_COUNT, RequestType.TO_END]:
            # 진입점 보정 (사용자 시간 → 내부 시간)
            to_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -1)

        end_time = None
        if to_time and chunk_count:
            end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

    else:
        # 후속 청크: 기존 로직 유지 (연속성 보장)
        collected_count = sum(c.calculate_effective_candle_count() for c in completed_chunks if c.is_completed())
        remaining_count = request_info.expected_count - collected_count
        chunk_count = min(remaining_count, self.chunk_size)

        # 이전 청크 기반 연속성
        last_chunk = completed_chunks[-1]
        last_effective_time = last_chunk.get_effective_end_time()
        if not last_effective_time:
            raise ValueError(f"이전 청크 {last_chunk.chunk_id}의 유효 끝 시간이 없습니다")

        to_time = TimeUtils.get_time_by_ticks(last_effective_time, request_info.timeframe, -1)
        end_time = TimeUtils.get_time_by_ticks(to_time, request_info.timeframe, -(chunk_count - 1))

    # ChunkInfo 생성 (공통)
    chunk = ChunkInfo(
        chunk_id=f"{request_info.symbol}_{request_info.timeframe}_{chunk_index:05d}",
        chunk_index=chunk_index,
        symbol=request_info.symbol,
        timeframe=request_info.timeframe,
        count=chunk_count,
        to=to_time,
        end=end_time
    )

    return chunk
```

## 4. **_process_single_chunk 메서드 정리**

**제거할 로직:**
- `is_first_chunk = chunk.chunk_index == 0` 체크
- `_should_skip_overlap_analysis()` 호출
- `_get_request_type_from_chunk()` 호출

**남길 로직:**
- 겹침 분석 (모든 후속 청크에 적용)
- API 데이터 수집
- 빈 캔들 처리
- 청크 완료 처리

```python
async def _process_single_chunk(self, chunk: ChunkInfo) -> None:
    """후속 청크 처리 (첫 청크 로직 제거된 순수 버전)"""
    logger.info(f"후속 청크 처리 시작: {chunk.chunk_id}")
    chunk.mark_processing()

    try:
        # 1. 겹침 분석 (후속 청크는 항상 실행)
        overlap_result = await self._analyze_chunk_overlap(chunk)
        if overlap_result:
            chunk.set_overlap_info(overlap_result)

        # 2. 데이터 수집 및 처리
        if chunk.needs_api_call():
            api_data = await self._fetch_api_data(chunk=chunk)
            # 빈 캔들 처리 등...
        else:
            logger.info(f"완전 겹침으로 API 호출 건너뜀: {chunk.chunk_id}")

        # 3. 청크 완료 처리
        chunk.mark_completed()
        logger.info(f"후속 청크 처리 완료: {chunk.chunk_id}")

    except Exception as e:
        chunk.mark_failed()
        logger.error(f"후속 청크 처리 실패: {chunk.chunk_id}, 오류: {e}")
        raise
```

## 5. **_fetch_api_data 메서드 단순화**

**현재 문제점:**
```python
async def _fetch_api_data(self, chunk: ChunkInfo) -> List[Dict[str, Any]]:
    api_count, api_to = chunk.get_api_params()  # 복잡한 상태 분석
    # 내부에서 타임프레임별 분기, to 파라미터 계산
```

**개선된 구조 (직접 파라미터 지원):**
```python
async def _fetch_api_data(
    self,
    chunk: Optional[ChunkInfo] = None,
    symbol: Optional[str] = None,
    timeframe: Optional[str] = None,
    count: Optional[int] = None,
    to: Optional[datetime] = None
) -> List[Dict[str, Any]]:
    """
    API 데이터 수집 - 직접 파라미터와 ChunkInfo 모두 지원

    Args:
        chunk: 기존 ChunkInfo 방식 (하위 호환성)
        symbol: 직접 파라미터 - 거래 심볼
        timeframe: 직접 파라미터 - 타임프레임
        count: 직접 파라미터 - 캔들 개수
        to: 직접 파라미터 - 시작 시점
    """
    # 파라미터 처리: 직접 파라미터 우선, 없으면 ChunkInfo에서 추출
    if symbol and timeframe and count:
        # 직접 파라미터 사용 (첫 청크 처리용)
        api_symbol = symbol
        api_timeframe = timeframe
        api_count = count
        api_to = to
        logger.debug(f"직접 파라미터 API 호출: {api_symbol} {api_timeframe}, count={api_count}, to={api_to}")
    elif chunk:
        # 기존 ChunkInfo 방식 (후속 청크용)
        api_count, api_to = chunk.get_api_params()
        api_symbol = chunk.symbol
        api_timeframe = chunk.timeframe
        logger.debug(f"ChunkInfo 기반 API 호출: {api_symbol} {api_timeframe}, count={api_count}, to={api_to}")
    else:
        raise ValueError("chunk 또는 (symbol, timeframe, count) 파라미터가 필요합니다")

    try:
        # Upbit to 파라미터 조정 (다음 틱을 가리키도록)
        to_param = None
        if api_to is not None:
            fetch_time = TimeUtils.get_time_by_ticks(api_to, api_timeframe, 1)
            to_param = fetch_time.strftime("%Y-%m-%dT%H:%M:%S")
            logger.debug(f"to 파라미터 변환: {api_to} → {to_param}")

        # 타임프레임별 API 호출 (기존 로직 유지)
        if api_timeframe in ['1m', '3m', '5m', '15m', '10m', '30m', '1h', '4h']:
            unit = int(api_timeframe.rstrip('mh'))
            api_data = await self.upbit_client.get_candles_minutes(
                unit=unit,
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1d':
            api_data = await self.upbit_client.get_candles_days(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1w':
            api_data = await self.upbit_client.get_candles_weeks(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        elif api_timeframe == '1M':
            api_data = await self.upbit_client.get_candles_months(
                market=api_symbol,
                to=to_param,
                count=api_count
            )
        else:
            raise ValueError(f"지원하지 않는 타임프레임: {api_timeframe}")

        logger.debug(f"API 응답 수신: {len(api_data)}개 캔들")
        return api_data

    except Exception as e:
        logger.error(f"API 데이터 수집 실패: {api_symbol} {api_timeframe}, 오류: {e}")
        raise
```

**개선 효과:**
- 첫 청크는 `_process_first_chunk()`에서 직접 파라미터로 호출
- 후속 청크는 기존 ChunkInfo 방식 유지 (완전 하위 호환성)
- 복잡한 상태 분석 로직 제거하면서도 기존 인터페이스 보존

## 6. **CollectionPlan 수정 (최소 변경)**

**현재:**
```python
@dataclass
class CollectionPlan:
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float
    first_chunk_params: Dict[str, Any]  # 제거 대상
```

**개선:**
```python
@dataclass
class CollectionPlan:
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float
    # first_chunk_params 필드 제거

    def __post_init__(self):
        if self.total_count <= 0:
            raise ValueError("총 캔들 개수는 1개 이상이어야 합니다")
        if self.estimated_chunks <= 0:
            raise ValueError("예상 청크 수는 1개 이상이어야 합니다")
        if self.estimated_duration_seconds < 0:
            raise ValueError("예상 소요 시간은 0초 이상이어야 합니다")
```

**create_collection_plan 함수 수정:**
```python
def create_collection_plan(
    request_info: RequestInfo,
    chunk_size: int = 200,
    api_rate_limit_rps: float = 10.0
) -> CollectionPlan:
    """RequestInfo 기반 수집 계획 생성 (first_chunk_params 제거)"""

    total_count = request_info.get_expected_count()
    estimated_chunks = (total_count + chunk_size - 1) // chunk_size
    estimated_duration_seconds = estimated_chunks / api_rate_limit_rps

    return CollectionPlan(
        total_count=total_count,
        estimated_chunks=estimated_chunks,
        estimated_duration_seconds=estimated_duration_seconds
        # first_chunk_params 제거
    )
```

## 7. **제거할 메서드들**

**ChunkProcessor에서 제거:**
- `_get_request_type_from_chunk()` - 첫 청크에서만 사용, 불필요
- `_should_skip_overlap_analysis()` - 첫 청크 전용 메서드에서 직접 처리

**유지하되 수정할 메서드:**
- `_create_chunk()` - 첫 청크 로직 단순화하되 통합 유지
- `_process_single_chunk()` - 첫 청크 관련 로직 제거
- `_fetch_api_data()` - 직접 파라미터 지원 추가 (완전 하위 호환성 유지)

## 📊 **변경 영향도 분석**

### 🟢 최소 변경 (Low Risk)
- `process_collection()` 구조 개선 - 기존 로직 유지하면서 가독성만 향상
- `_process_first_chunk()` 신규 추가 - 기존 코드에 영향 없음
- `_fetch_api_data()` 직접 파라미터 지원 추가 - 완전 하위 호환성 유지

### 🟡 중간 변경 (Medium Risk)
- `_create_chunk()` 첫 청크 로직 수정 - 테스트 필요
- `_process_single_chunk()` 첫 청크 로직 제거 - 동작 검증 필요
- `CollectionPlan.first_chunk_params` 제거 - 영향도 확인 필요

### 🔴 제거 대상 (확인 필요)
- `_get_request_type_from_chunk()` - 사용처 확인 후 제거
- `_should_skip_overlap_analysis()` - 사용처 확인 후 제거

## 🚀 **구현 순서 (최소 변경 원칙)**

### Phase 1: 새 메서드 추가 (기존 코드 영향 없음)
1. `_process_first_chunk()` 메서드 구현
2. `_fetch_api_data()` 직접 파라미터 지원 추가
3. 단위 테스트 작성

### Phase 2: 기존 메서드 수정 (점진적 적용)
1. `process_collection()` 첫 청크 처리 명시화
2. `_create_chunk()` 첫 청크 로직 단순화
3. `_process_single_chunk()` 첫 청크 로직 제거
4. 통합 테스트

### Phase 3: 불필요 코드 정리 (안전성 확보 후)
1. `CollectionPlan.first_chunk_params` 제거
2. `create_collection_plan()` 수정
3. 사용하지 않는 메서드 제거
4. 최종 검증

## 🎯 **기대 효과**

### ✅ 가독성 대폭 향상
- 첫 청크 처리가 `process_collection()`에 명확히 드러남
- 복잡한 조건 분기 로직 80% 이상 감소

### ✅ 유지보수성 향상
- 첫 청크 관련 로직이 `_process_first_chunk()`에 집중
- 후속 청크 처리는 순수하고 단순한 로직만 유지

### ✅ 최소 변경으로 최대 효과
- 기존 아키텍처 구조 그대로 유지
- 핵심 문제만 집중적으로 해결
- 리스크 최소화

### ✅ 성능 개선
- 불필요한 첫 청크 체크 로직 제거
- 메서드 호출 체인 단순화
- API 파라미터 생성 로직 최적화

---

**💡 결론**: 사용자 의견을 반영하여 최소 변경 원칙을 적용한 이 개선 방안은 복잡성은 크게 줄이면서도 기존 코드에 미치는 영향을 최소화하여 안전하고 효과적인 리팩터링을 가능하게 합니다.
