"""
ğŸ“‹ CandleDataProvider v4.0 - TASK_02 êµ¬í˜„ ì™„ë£Œ
ìš”ì²­ ì •ê·œí™” ë° ì²­í¬ ìƒì„± ë¡œì§ êµ¬í˜„ + í•˜ì´ë¸Œë¦¬ë“œ ìˆœì°¨ ì²˜ë¦¬

Created: 2025-09-11 (Updated: 2025-09-12)
Purpose: normalize_requestì™€ create_chunks ë©”ì„œë“œ êµ¬í˜„ + ìˆœì°¨ ì²­í¬ ê´€ë¦¬
Features: 4ê°€ì§€ íŒŒë¼ë¯¸í„° ì¡°í•© ì§€ì›, ì„¤ì • ê°€ëŠ¥í•œ ì²­í¬ ë¶„í• , ì‹¤ì‹œê°„ ìƒíƒœ ê´€ë¦¬
Architecture: ê¸°ì¡´ TASK_02 ìš”êµ¬ì‚¬í•­ + New04 í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ ê²°í•©
"""

from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any
from enum import Enum
from dataclasses import dataclass, field

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils
from upbit_auto_trading.infrastructure.market_data.candle.candle_models import (
    ChunkInfo, CandleData
)
from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface
)
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import (
    UpbitPublicClient
)
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import (
    OverlapAnalyzer
)

logger = create_component_logger("CandleDataProvider")


class ChunkStatus(Enum):
    """ì²­í¬ ì²˜ë¦¬ ìƒíƒœ"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class CollectionState:
    """ìº”ë“¤ ìˆ˜ì§‘ ìƒíƒœ ê´€ë¦¬"""
    request_id: str
    symbol: str
    timeframe: str
    total_requested: int
    total_collected: int = 0
    completed_chunks: List[ChunkInfo] = field(default_factory=list)
    current_chunk: Optional[ChunkInfo] = None
    last_candle_time: Optional[str] = None  # ë§ˆì§€ë§‰ ìˆ˜ì§‘ëœ ìº”ë“¤ ì‹œê°„ (ì—°ì†ì„±ìš©)
    estimated_total_chunks: int = 0
    estimated_completion_time: Optional[datetime] = None
    start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    is_completed: bool = False
    error_message: Optional[str] = None
    target_end: Optional[datetime] = None  # end íŒŒë¼ë¯¸í„° ëª©í‘œ ì‹œì  (Phase 1 ì¶”ê°€)

    # ë‚¨ì€ ì‹œê°„ ì¶”ì  í•„ë“œë“¤
    last_update_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    avg_chunk_duration: float = 0.0  # í‰ê·  ì²­í¬ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
    remaining_chunks: int = 0  # ë‚¨ì€ ì²­í¬ ìˆ˜
    estimated_remaining_seconds: float = 0.0  # ì‹¤ì‹œê°„ ê³„ì‚°ëœ ë‚¨ì€ ì‹œê°„


@dataclass
class CollectionPlan:
    """ìˆ˜ì§‘ ê³„íš (ìµœì†Œ ì •ë³´ë§Œ)"""
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float
    first_chunk_params: Dict[str, Any]  # ì²« ë²ˆì§¸ ì²­í¬ ìš”ì²­ íŒŒë¼ë¯¸í„°


class CandleDataProvider:
    """
    ìº”ë“¤ ë°ì´í„° ì œê³µì v4.0 - TASK_02 êµ¬í˜„ ì™„ë£Œ

    í•µì‹¬ ì›ë¦¬:
    1. ìµœì†Œí•œì˜ ì‚¬ì „ ê³„íš (ì²­í¬ ìˆ˜ì™€ ì˜ˆìƒ ì‹œê°„ë§Œ)
    2. ì‹¤ì‹œê°„ ìˆœì°¨ ì²­í¬ ìƒì„± (ì´ì „ ì‘ë‹µ ê¸°ë°˜)
    3. ìƒíƒœ ì¶”ì ìœ¼ë¡œ ì¤‘ë‹¨/ì¬ì‹œì‘ ì§€ì›
    4. 10 RPS ê¸°ë°˜ ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ì œê³µ

    ì¥ì :
    - new01ì˜ ì•ˆì •ì„±: ìƒíƒœ ê´€ë¦¬, ê²€ì¦, ì¤‘ë‹¨/ì¬ì‹œì‘
    - new02ì˜ íš¨ìœ¨ì„±: ìˆœì°¨ ì²˜ë¦¬, ì‘ë‹µ ê¸°ë°˜ ì—°ì†ì„±
    - ìµœì†Œ ì˜¤ë²„í—¤ë“œ: ì‚¬ì „ ê³„íš ìµœì†Œí™”
    - ì‹¤ì‹œê°„ í™•ì¥: í•„ìš”í•  ë•Œë§Œ ì²­í¬ ìƒì„±
    """

    def __init__(
        self,
        repository: CandleRepositoryInterface,
        upbit_client: UpbitPublicClient,
        overlap_analyzer: OverlapAnalyzer,
        chunk_size: int = 200
    ):
        """
        CandleDataProvider v4.0 ì´ˆê¸°í™” - DI íŒ¨í„´ ì ìš©

        Args:
            repository: CandleRepositoryInterface êµ¬í˜„ì²´ (DB ì ‘ê·¼)
            upbit_client: UpbitPublicClient (API í˜¸ì¶œ)
            overlap_analyzer: OverlapAnalyzer v5.0 (ê²¹ì¹¨ ë¶„ì„)
            chunk_size: ì²­í¬ í¬ê¸° (ê¸°ë³¸ê°’ 200, ìµœëŒ€ 200)
        """
        # ì˜ì¡´ì„± ì£¼ì…
        self.repository = repository
        self.upbit_client = upbit_client
        self.overlap_analyzer = overlap_analyzer

        # ê¸°ì¡´ ì„¤ì •
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜
        self.active_collections: Dict[str, CollectionState] = {}
        self.api_rate_limit_rps = 10  # 10 RPS ê¸°ì¤€

        logger.info("CandleDataProvider v4.0 (DI íŒ¨í„´ + í•˜ì´ë¸Œë¦¬ë“œ ìˆœì°¨ ì²˜ë¦¬) ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info(f"ì²­í¬ í¬ê¸°: {self.chunk_size}, API Rate Limit: {self.api_rate_limit_rps} RPS")
        logger.info(f"ì˜ì¡´ì„±: Repository={type(repository).__name__}, Client={type(upbit_client).__name__}")
        logger.info(f"ì˜ì¡´ì„±: OverlapAnalyzer={type(overlap_analyzer).__name__}")

    def plan_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> CollectionPlan:
        """
        ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½ (ìµœì†Œí•œì˜ ì •ë³´ë§Œ)

        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m', '5m', '1h', '1d')
            count: ì¡°íšŒí•  ìº”ë“¤ ê°œìˆ˜
            to: ì‹œì‘ì  (ì²« ë²ˆì§¸ ì²­í¬ìš©)
            end: ì¢…ë£Œì 

        Returns:
            CollectionPlan: ìµœì†Œ ê³„íš ì •ë³´
        """
        logger.info(f"ìˆ˜ì§‘ ê³„íš ìˆ˜ë¦½: {symbol} {timeframe}, count={count}, to={to}, end={end}")

        # ë™ì  ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦
        current_time = datetime.now(timezone.utc)
        if to is not None and to > current_time:
            raise ValueError(f"to ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {to}")
        if end is not None and end > current_time:
            raise ValueError(f"end ì‹œì ì´ ë¯¸ë˜ì…ë‹ˆë‹¤: {end}")

        # ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° (ìµœì†Œí•œì˜ ì •ê·œí™”)
        total_count = self._calculate_total_count(
            count=count,
            to=to,
            end=end,
            timeframe=timeframe,
            current_time=current_time
        )

        # ì˜ˆìƒ ì²­í¬ ìˆ˜ ê³„ì‚°
        estimated_chunks = (total_count + self.chunk_size - 1) // self.chunk_size

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ê³„ì‚° (10 RPS ê¸°ì¤€)
        estimated_duration_seconds = estimated_chunks / self.api_rate_limit_rps

        # ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±
        first_chunk_params = self._create_first_chunk_params(
            symbol=symbol,
            count=count,
            to=to,
            end=end,
            current_time=current_time
        )

        plan = CollectionPlan(
            total_count=total_count,
            estimated_chunks=estimated_chunks,
            estimated_duration_seconds=estimated_duration_seconds,
            first_chunk_params=first_chunk_params
        )

        logger.info(f"âœ… ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {total_count:,}ê°œ ìº”ë“¤, {estimated_chunks}ì²­í¬, "
                    f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {estimated_duration_seconds:.1f}ì´ˆ")
        return plan

    def start_collection(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> str:
        """
        ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘ (ìƒíƒœ ì¶”ì  ì‹œì‘)

        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            count: ì¡°íšŒí•  ìº”ë“¤ ê°œìˆ˜
            to: ì‹œì‘ì 
            end: ì¢…ë£Œì 

        Returns:
            str: ìˆ˜ì§‘ ìš”ì²­ ID (ìƒíƒœ ì¶”ì ìš©)
        """
        logger.info(f"ìº”ë“¤ ìˆ˜ì§‘ ì‹œì‘: {symbol} {timeframe}")

        # ê³„íš ìˆ˜ë¦½
        plan = self.plan_collection(symbol, timeframe, count, to, end)

        # ìš”ì²­ ID ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜)
        request_id = f"{symbol}_{timeframe}_{int(datetime.now().timestamp())}"

        # ìˆ˜ì§‘ ìƒíƒœ ì´ˆê¸°í™”
        estimated_completion = datetime.now(timezone.utc) + timedelta(
            seconds=plan.estimated_duration_seconds
        )

        collection_state = CollectionState(
            request_id=request_id,
            symbol=symbol,
            timeframe=timeframe,
            total_requested=plan.total_count,
            estimated_total_chunks=plan.estimated_chunks,
            estimated_completion_time=estimated_completion,
            remaining_chunks=plan.estimated_chunks,
            estimated_remaining_seconds=plan.estimated_duration_seconds,
            target_end=end  # Phase 1: end íŒŒë¼ë¯¸í„° ë³´ê´€
        )

        # ì²« ë²ˆì§¸ ì²­í¬ ìƒì„±
        first_chunk = self._create_next_chunk(
            collection_state=collection_state,
            chunk_params=plan.first_chunk_params,
            chunk_index=0
        )
        collection_state.current_chunk = first_chunk

        # ìƒíƒœ ë“±ë¡
        self.active_collections[request_id] = collection_state

        logger.info(f"âœ… ìˆ˜ì§‘ ì‹œì‘: ìš”ì²­ ID {request_id}, ì˜ˆìƒ ì™„ë£Œ: {estimated_completion}")
        return request_id

    def get_next_chunk(self, request_id: str) -> Optional[ChunkInfo]:
        """
        ë‹¤ìŒ ì²˜ë¦¬í•  ì²­í¬ ì •ë³´ ë°˜í™˜

        Args:
            request_id: ìˆ˜ì§‘ ìš”ì²­ ID

        Returns:
            ChunkInfo: ë‹¤ìŒ ì²­í¬ ì •ë³´ (ì™„ë£Œëœ ê²½ìš° None)
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.is_completed:
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œë¨: {request_id}")
            return None

        if state.current_chunk is None:
            logger.warning(f"ì²˜ë¦¬í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤: {request_id}")
            return None

        logger.debug(f"ë‹¤ìŒ ì²­í¬ ë°˜í™˜: {state.current_chunk.chunk_id}")
        return state.current_chunk

    async def _fetch_chunk_from_api(self, chunk_info: ChunkInfo) -> List[Dict[str, Any]]:
        """
        ì‹¤ì œ API í˜¸ì¶œì„ í†µí•œ ì²­í¬ ë°ì´í„° ìˆ˜ì§‘

        Args:
            chunk_info: ì²­í¬ ì •ë³´ (symbol, timeframe, count, to ë“±)

        Returns:
            List[Dict[str, Any]]: ì—…ë¹„íŠ¸ API ì‘ë‹µ ìº”ë“¤ ë°ì´í„°

        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„
            Exception: API í˜¸ì¶œ ì‹¤íŒ¨
        """
        logger.debug(f"API ì²­í¬ ìš”ì²­ ì‹œì‘: {chunk_info.chunk_id}")

        try:
            # íƒ€ì„í”„ë ˆì„ë³„ API ë©”ì„œë“œ ì„ íƒ
            if chunk_info.timeframe.endswith('m'):
                # ë¶„ë´‰ (1m, 3m, 5m, 10m, 15m, 30m, 60m, 240m)
                unit = int(chunk_info.timeframe[:-1])
                if unit not in [1, 3, 5, 10, 15, 30, 60, 240]:
                    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¶„ë´‰ ë‹¨ìœ„: {unit}")

                to_param = chunk_info.to.strftime("%Y-%m-%dT%H:%M:%S") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_minutes(
                    unit=unit,
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1d':
                # ì¼ë´‰
                to_param = chunk_info.to.strftime("%Y-%m-%d") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_days(
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1w':
                # ì£¼ë´‰
                to_param = chunk_info.to.strftime("%Y-%m-%d") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_weeks(
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            elif chunk_info.timeframe == '1M':
                # ì›”ë´‰
                to_param = chunk_info.to.strftime("%Y-%m") if chunk_info.to else None
                candles = await self.upbit_client.get_candles_months(
                    market=chunk_info.symbol,
                    count=chunk_info.count,
                    to=to_param
                )

            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {chunk_info.timeframe}")

            logger.info(f"âœ… API ì²­í¬ ì™„ë£Œ: {chunk_info.chunk_id}, ìˆ˜ì§‘: {len(candles)}ê°œ")
            return candles

        except Exception as e:
            logger.error(f"âŒ API ì²­í¬ ì‹¤íŒ¨: {chunk_info.chunk_id}, ì˜¤ë¥˜: {e}")
            raise

    def _convert_upbit_response_to_candles(
        self,
        api_response: List[Dict[str, Any]],
        timeframe: str
    ) -> List[CandleData]:
        """
        ì—…ë¹„íŠ¸ API ì‘ë‹µì„ CandleData ê°ì²´ë¡œ ë³€í™˜

        Args:
            api_response: ì—…ë¹„íŠ¸ API ì‘ë‹µ ë°ì´í„°
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m', '5m', '1d')

        Returns:
            List[CandleData]: ë³€í™˜ëœ CandleData ê°ì²´ë“¤

        Note:
            - ì—…ë¹„íŠ¸ APIëŠ” ìµœì‹ ìˆœ(ë‚´ë¦¼ì°¨ìˆœ) ì •ë ¬ë¡œ ì‘ë‹µ
            - CandleData.from_upbit_api()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì „í•œ ë³€í™˜ ìˆ˜í–‰
        """
        logger.debug(f"API ì‘ë‹µ ë³€í™˜ ì‹œì‘: {len(api_response)}ê°œ ìº”ë“¤, íƒ€ì„í”„ë ˆì„: {timeframe}")

        try:
            converted_candles = []
            for candle_dict in api_response:
                candle_data = CandleData.from_upbit_api(candle_dict, timeframe)
                converted_candles.append(candle_data)

            logger.debug(f"âœ… API ì‘ë‹µ ë³€í™˜ ì™„ë£Œ: {len(converted_candles)}ê°œ")
            return converted_candles

        except Exception as e:
            logger.error(f"âŒ API ì‘ë‹µ ë³€í™˜ ì‹¤íŒ¨: {e}")
            logger.error(f"ë¬¸ì œëœ ë°ì´í„° ìƒ˜í”Œ: {api_response[:1] if api_response else 'Empty'}")
            raise

    async def _save_candles_to_repository(
        self,
        symbol: str,
        timeframe: str,
        candles: List[CandleData]
    ) -> int:
        """
        Repositoryë¥¼ í†µí•œ ìº”ë“¤ ë°ì´í„° DB ì €ì¥

        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m', '5m')
            candles: ì €ì¥í•  CandleData ê°ì²´ë“¤

        Returns:
            int: ì‹¤ì œ ì €ì¥ëœ ìº”ë“¤ ê°œìˆ˜

        Note:
            - Repositoryì˜ save_candle_chunk() ë©”ì„œë“œ í™œìš©
            - INSERT OR IGNORE ë°©ì‹ìœ¼ë¡œ ì¤‘ë³µ ìë™ ì²˜ë¦¬
            - íŠ¸ëœì­ì…˜ ê¸°ë°˜ ì•ˆì „í•œ ì¼ê´„ ì €ì¥
        """
        if not candles:
            logger.warning("ì €ì¥í•  ìº”ë“¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return 0

        logger.debug(f"DB ì €ì¥ ì‹œì‘: {symbol} {timeframe}, {len(candles)}ê°œ")

        try:
            saved_count = await self.repository.save_candle_chunk(symbol, timeframe, candles)
            logger.debug(f"âœ… DB ì €ì¥ ì™„ë£Œ: {saved_count}/{len(candles)}ê°œ ì €ì¥ë¨")
            return saved_count

        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {symbol} {timeframe}, {len(candles)}ê°œ, ì˜¤ë¥˜: {e}")
            raise

    async def _analyze_chunk_overlap(
        self,
        symbol: str,
        timeframe: str,
        start_time: datetime,
        end_time: datetime
    ):
        """
        OverlapAnalyzerë¥¼ í™œìš©í•œ ì²­í¬ ê²¹ì¹¨ ë¶„ì„ (ì„±ëŠ¥ ìµœì í™” ìš©ë„)

        Args:
            symbol: ì‹¬ë³¼
            timeframe: íƒ€ì„í”„ë ˆì„
            start_time: ìš”ì²­ ì‹œì‘ ì‹œê°„
            end_time: ìš”ì²­ ì¢…ë£Œ ì‹œê°„

        Returns:
            OverlapResult: 5ê°€ì§€ ìƒíƒœë³„ ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼

        Note:
            - í˜„ì¬ ë²„ì „ì—ì„œëŠ” ë‹¨ìˆœíˆ API í˜¸ì¶œ ìš°ì„  ì •ì±…
            - ì¶”í›„ ì„±ëŠ¥ ìµœì í™” ì‹œ DB/API í˜¼í•© ì²˜ë¦¬ í™œìš© ì˜ˆì •
        """
        logger.debug(f"ê²¹ì¹¨ ë¶„ì„: {symbol} {timeframe}, {start_time} ~ {end_time}")

        try:
            # OverlapAnalyzer v5.0 í™œìš©
            from upbit_auto_trading.infrastructure.market_data.candle.candle_models import OverlapRequest

            # Timezone ì•ˆì „ ì²˜ë¦¬: ëª¨ë“  datetimeì„ UTCë¡œ í†µì¼
            safe_start_time = self._ensure_utc_timezone(start_time)
            safe_end_time = self._ensure_utc_timezone(end_time)

            # ì˜ˆìƒ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚°
            expected_count = self._calculate_api_count(safe_start_time, safe_end_time, timeframe)

            overlap_request = OverlapRequest(
                symbol=symbol,
                timeframe=timeframe,
                target_start=safe_start_time,
                target_end=safe_end_time,
                target_count=expected_count
            )

            overlap_result = await self.overlap_analyzer.analyze_overlap(overlap_request)
            logger.debug(f"ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼: {overlap_result.status.value} (ì˜ˆìƒ ê°œìˆ˜: {expected_count})")

            return overlap_result

        except Exception as e:
            logger.warning(f"ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨: {e}, API í˜¸ì¶œë¡œ ì§„í–‰")
            return None

    async def mark_chunk_completed(
        self,
        request_id: str
    ) -> bool:
        """
        ì‹¤ì œ API í˜¸ì¶œì„ í†µí•œ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬ ë° ë‹¤ìŒ ì²­í¬ ìƒì„±

        Args:
            request_id: ìˆ˜ì§‘ ìš”ì²­ ID

        Returns:
            bool: ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ ì—¬ë¶€

        Note:
            - ì‹¤ì œ API í˜¸ì¶œ: _fetch_chunk_from_api()
            - ë°ì´í„° ë³€í™˜: _convert_upbit_response_to_candles()
            - DB ì €ì¥: _save_candles_to_repository()
            - ì—°ì†ì„± ë³´ì¥: ì´ì „ ì²­í¬ ê¸°ë°˜ ë‹¤ìŒ ì²­í¬ ìƒì„±
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.current_chunk is None:
            raise ValueError("ì²˜ë¦¬ ì¤‘ì¸ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤")

        logger.info(f"ğŸš€ ì²­í¬ ì²˜ë¦¬ ì‹œì‘: {state.current_chunk.chunk_id}")

        try:
            # 1. OverlapAnalyzerë¡œ ê²¹ì¹¨ ë¶„ì„ (ì„±ëŠ¥ ìµœì í™”)
            chunk_start = state.current_chunk.to  # API ìš”ì²­ì˜ ì‹œì‘ì 
            chunk_end = self._calculate_chunk_end_time(state.current_chunk)  # ì˜ˆìƒ ì¢…ë£Œì 

            overlap_result = await self._analyze_chunk_overlap(
                state.symbol,
                state.timeframe,
                chunk_start,
                chunk_end
            )

            # 2. ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ë°ì´í„° ìˆ˜ì§‘ ì „ëµ
            if overlap_result and hasattr(overlap_result, 'status'):
                candle_data_list, api_calls_made = await self._collect_data_by_overlap_strategy(
                    state.current_chunk, overlap_result
                )
                logger.info(f"ğŸ” ê²¹ì¹¨ ë¶„ì„ ì ìš©: {overlap_result.status.value}, API í˜¸ì¶œ: {'ì˜ˆ' if api_calls_made else 'ì•„ë‹ˆì˜¤'}")
            else:
                # ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨ ì‹œ í´ë°±: ê¸°ì¡´ API í˜¸ì¶œ ë°©ì‹
                logger.warning("ê²¹ì¹¨ ë¶„ì„ ì‹¤íŒ¨ â†’ API í˜¸ì¶œ í´ë°±")
                api_response = await self._fetch_chunk_from_api(state.current_chunk)
                candle_data_list = self._convert_upbit_response_to_candles(
                    api_response,
                    state.timeframe
                )
                api_calls_made = True

            # 3. Repositoryë¥¼ í†µí•œ DB ì €ì¥ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ)
            if candle_data_list:
                saved_count = await self._save_candles_to_repository(
                    state.symbol,
                    state.timeframe,
                    candle_data_list
                )
            else:
                saved_count = 0
                logger.debug("ì €ì¥í•  ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ì™„ì „ ê²¹ì¹¨)")

            # 4. í˜„ì¬ ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
            completed_chunk = state.current_chunk
            completed_chunk.status = "completed"
            state.completed_chunks.append(completed_chunk)
            state.total_collected += len(candle_data_list)

            # 5. ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ ì—…ë°ì´íŠ¸ (ë‹¤ìŒ ì²­í¬ ì—°ì†ì„±ìš©)
            if candle_data_list:
                # CandleData ê°ì²´ ë˜ëŠ” dictì—ì„œ ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ ì¶”ì¶œ
                last_candle = candle_data_list[-1]
                if hasattr(last_candle, 'candle_date_time_utc'):
                    state.last_candle_time = last_candle.candle_date_time_utc
                elif isinstance(last_candle, dict) and 'candle_date_time_utc' in last_candle:
                    state.last_candle_time = last_candle['candle_date_time_utc']
                else:
                    logger.warning("ë§ˆì§€ë§‰ ìº”ë“¤ì—ì„œ ì‹œê°„ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

            # 6. ë‚¨ì€ ì‹œê°„ ì •ë³´ ì—…ë°ì´íŠ¸
            self._update_remaining_time_estimates(state)

            logger.info(f"âœ… ì²­í¬ ì™„ë£Œ: {completed_chunk.chunk_id}, "
                        f"ìˆ˜ì§‘: {len(candle_data_list)}ê°œ, ì €ì¥: {saved_count}ê°œ, "
                        f"ëˆ„ì : {state.total_collected}/{state.total_requested}, "
                        f"ë‚¨ì€ì‹œê°„: {state.estimated_remaining_seconds:.1f}ì´ˆ")

            # 7. ìˆ˜ì§‘ ì™„ë£Œ í™•ì¸ (ê°œìˆ˜ + ì‹œê°„ ì¡°ê±´)
            count_reached = state.total_collected >= state.total_requested

            # end ì‹œì  ë„ë‹¬ í™•ì¸
            end_time_reached = False
            if state.target_end and candle_data_list:
                try:
                    # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ìº”ë“¤ì˜ ì‹œê°„ì„ datetimeìœ¼ë¡œ ë³€í™˜
                    last_candle = candle_data_list[-1]
                    last_candle_time_str = None

                    if hasattr(last_candle, 'candle_date_time_utc'):
                        last_candle_time_str = last_candle.candle_date_time_utc
                    elif isinstance(last_candle, dict) and 'candle_date_time_utc' in last_candle:
                        last_candle_time_str = last_candle['candle_date_time_utc']

                    if last_candle_time_str:
                        # ISO format ì²˜ë¦¬ (Z suffix ì œê±°)
                        if last_candle_time_str.endswith('Z'):
                            last_candle_time_str = last_candle_time_str[:-1] + '+00:00'
                        last_candle_time = datetime.fromisoformat(last_candle_time_str)

                        # ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ì´ target_endë³´ë‹¤ ê³¼ê±°ë©´ ì •ì§€
                        end_time_reached = last_candle_time <= state.target_end

                        logger.debug(
                            f"ì‹œê°„ ì¡°ê±´ í™•ì¸: ë§ˆì§€ë§‰ìº”ë“¤={last_candle_time}, "
                            f"ëª©í‘œì¢…ë£Œ={state.target_end}, ë„ë‹¬={end_time_reached}"
                        )
                except Exception as e:
                    logger.warning(f"ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: {e}")

            if count_reached or end_time_reached:
                completion_reason = "ê°œìˆ˜ ë‹¬ì„±" if count_reached else "end ì‹œì  ë„ë‹¬"
                state.is_completed = True
                state.current_chunk = None
                logger.info(f"ğŸ‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ ({completion_reason}): {request_id}, {state.total_collected}ê°œ")
                return True

            # 8. ë‹¤ìŒ ì²­í¬ ìƒì„±
            next_chunk_index = len(state.completed_chunks)
            remaining_count = state.total_requested - state.total_collected
            next_chunk_size = min(remaining_count, self.chunk_size)

            # ë‹¤ìŒ ì²­í¬ íŒŒë¼ë¯¸í„° (ì—…ë¹„íŠ¸ APIì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ì†ì„± í™œìš©)
            next_chunk_params = {
                "market": state.symbol,
                "count": next_chunk_size,
                "to": state.last_candle_time  # ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì—…ë¹„íŠ¸ APIê°€ ìë™ìœ¼ë¡œ ì´ì „ ìº”ë“¤ ë°˜í™˜)
            }            # ë‹¤ìŒ ì²­í¬ ìƒì„±
            next_chunk = self._create_next_chunk(
                collection_state=state,
                chunk_params=next_chunk_params,
                chunk_index=next_chunk_index
            )
            state.current_chunk = next_chunk

            logger.debug(f"â¡ï¸ ë‹¤ìŒ ì²­í¬ ìƒì„±: {next_chunk.chunk_id}, ì”ì—¬: {remaining_count}ê°œ")
            return False

        except Exception as e:
            # ì²­í¬ ì‹¤íŒ¨ ì²˜ë¦¬
            if state.current_chunk:
                state.current_chunk.status = "failed"
                state.error_message = str(e)

            logger.error(f"âŒ ì²­í¬ ì²˜ë¦¬ ì‹¤íŒ¨: {state.current_chunk.chunk_id if state.current_chunk else 'Unknown'}, ì˜¤ë¥˜: {e}")
            raise

    def get_collection_status(self, request_id: str) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ ìƒíƒœ ì¡°íšŒ

        Args:
            request_id: ìˆ˜ì§‘ ìš”ì²­ ID

        Returns:
            Dict: ìƒì„¸ ìƒíƒœ ì •ë³´
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        progress_percentage = (state.total_collected / state.total_requested) * 100
        elapsed_time = datetime.now(timezone.utc) - state.start_time
        completed_chunks = len(state.completed_chunks)

        return {
            "request_id": request_id,
            "symbol": state.symbol,
            "timeframe": state.timeframe,
            "progress": {
                "collected": state.total_collected,
                "requested": state.total_requested,
                "percentage": round(progress_percentage, 1)
            },
            "chunks": {
                "completed": completed_chunks,
                "estimated_total": state.estimated_total_chunks,
                "current": state.current_chunk.chunk_id if state.current_chunk else None
            },
            "timing": {
                "elapsed_seconds": elapsed_time.total_seconds(),
                "estimated_total_seconds": (
                    state.estimated_completion_time - state.start_time
                    if state.estimated_completion_time else None
                ),
                "estimated_remaining_seconds": state.estimated_remaining_seconds,
                "avg_chunk_duration": state.avg_chunk_duration,
                "remaining_chunks": state.remaining_chunks
            },
            "is_completed": state.is_completed,
            "error": state.error_message
        }

    def resume_collection(self, request_id: str) -> Optional[ChunkInfo]:
        """
        ì¤‘ë‹¨ëœ ìˆ˜ì§‘ ì¬ê°œ

        Args:
            request_id: ìˆ˜ì§‘ ìš”ì²­ ID

        Returns:
            ChunkInfo: ì¬ê°œí•  ì²­í¬ ì •ë³´
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]

        if state.is_completed:
            logger.info(f"ì´ë¯¸ ì™„ë£Œëœ ìˆ˜ì§‘: {request_id}")
            return None

        if state.current_chunk is not None:
            logger.info(f"ìˆ˜ì§‘ ì¬ê°œ: {request_id}, í˜„ì¬ ì²­í¬: {state.current_chunk.chunk_id}")
            return state.current_chunk

        # í˜„ì¬ ì²­í¬ê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (ë§ˆì§€ë§‰ ìƒíƒœ ê¸°ì¤€)
        next_chunk_index = len(state.completed_chunks)
        remaining_count = state.total_requested - state.total_collected

        if remaining_count <= 0:
            state.is_completed = True
            return None

        next_chunk_size = min(remaining_count, self.chunk_size)
        next_chunk_params = {
            "market": state.symbol,
            "count": next_chunk_size,
            "to": state.last_candle_time
        }

        next_chunk = self._create_next_chunk(
            collection_state=state,
            chunk_params=next_chunk_params,
            chunk_index=next_chunk_index
        )
        state.current_chunk = next_chunk

        logger.info(f"ìˆ˜ì§‘ ì¬ê°œ: ìƒˆ ì²­í¬ ìƒì„± {next_chunk.chunk_id}")
        return next_chunk

    def _calculate_total_count(
        self,
        count: Optional[int],
        to: Optional[datetime],
        end: Optional[datetime],
        timeframe: str,
        current_time: datetime
    ) -> int:
        """ì´ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° (ìµœì†Œ ì •ê·œí™”)"""

        # countê°€ ì§ì ‘ ì œê³µëœ ê²½ìš°
        if count is not None:
            return count

        # endê°€ ì œê³µëœ ê²½ìš°: ê¸°ê°„ ê³„ì‚°
        if end is not None:
            start_time = to if to is not None else current_time
            normalized_start = TimeUtils.align_to_candle_boundary(start_time, timeframe)
            normalized_end = TimeUtils.align_to_candle_boundary(end, timeframe)

            if normalized_start <= normalized_end:
                raise ValueError("ì‹œì‘ ì‹œì ì´ ì¢…ë£Œ ì‹œì ë³´ë‹¤ ì´ì „ì…ë‹ˆë‹¤")

            return TimeUtils.calculate_expected_count(
                start_time=normalized_start,
                end_time=normalized_end,
                timeframe=timeframe
            )

        raise ValueError("count ë˜ëŠ” end ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì œê³µë˜ì–´ì•¼ í•©ë‹ˆë‹¤")

    def _create_first_chunk_params(
        self,
        symbol: str,
        count: Optional[int],
        to: Optional[datetime],
        end: Optional[datetime],
        current_time: datetime
    ) -> Dict[str, Any]:
        """ì²« ë²ˆì§¸ ì²­í¬ íŒŒë¼ë¯¸í„° ìƒì„±"""

        params = {"market": symbol}

        # count ê¸°ë°˜ ìš”ì²­ (ê°€ì¥ ë‹¨ìˆœ)
        if count is not None:
            chunk_size = min(count, self.chunk_size)
            params["count"] = chunk_size

            # toê°€ ìˆìœ¼ë©´ ì‹œì‘ì  ì§€ì •
            if to is not None:
                params["to"] = to.strftime("%Y-%m-%dT%H:%M:%S")
            # toê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (count only)

        # end ê¸°ë°˜ ìš”ì²­
        elif end is not None:
            params["count"] = self.chunk_size

            # toê°€ ìˆìœ¼ë©´ ì‹œì‘ì  ì§€ì •
            if to is not None:
                params["to"] = to.strftime("%Y-%m-%dT%H:%M:%S")
            # toê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ê¸°ì¤€ (end only)

        return params

    def _create_next_chunk(
        self,
        collection_state: CollectionState,
        chunk_params: Dict[str, Any],
        chunk_index: int
    ) -> ChunkInfo:
        """ë‹¤ìŒ ì²­í¬ ì •ë³´ ìƒì„±"""

        chunk_id = f"{collection_state.symbol}_{collection_state.timeframe}_{chunk_index:05d}"

        # chunk_paramsì—ì„œ ì‹¤ì œ 'to' ê°’ ì¶”ì¶œ (ë¬¸ìì—´ â†’ datetime ë³€í™˜)
        current_time = datetime.now(timezone.utc)

        # 'to' íŒŒë¼ë¯¸í„°ê°€ ìˆìœ¼ë©´ datetimeìœ¼ë¡œ ë³€í™˜, ì—†ìœ¼ë©´ í˜„ì¬ ì‹œê°„ ì‚¬ìš©
        if "to" in chunk_params and chunk_params["to"]:
            try:
                # ISO í˜•ì‹ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ ë³€í™˜
                to_str = chunk_params["to"]
                if isinstance(to_str, str):
                    # ISO format ì²˜ë¦¬ (ì—…ë¹„íŠ¸ APIëŠ” 'YYYY-MM-DDTHH:MM:SS' í˜•ì‹)
                    to_datetime = datetime.fromisoformat(to_str.replace('Z', '+00:00'))
                else:
                    to_datetime = current_time
            except (ValueError, TypeError):
                logger.warning(f"'to' íŒŒë¼ë¯¸í„° íŒŒì‹± ì‹¤íŒ¨: {chunk_params.get('to')}, í˜„ì¬ ì‹œê°„ ì‚¬ìš©")
                to_datetime = current_time
        else:
            to_datetime = current_time

        chunk_info = ChunkInfo(
            chunk_id=chunk_id,
            chunk_index=chunk_index,
            symbol=collection_state.symbol,
            timeframe=collection_state.timeframe,
            count=chunk_params["count"],
            to=to_datetime,  # ì‹¤ì œ chunk_paramsì˜ 'to' ê°’ ì‚¬ìš©
            end=current_time,  # endëŠ” ì¶”í›„ API ì‘ë‹µ í›„ ì—…ë°ì´íŠ¸
            status="pending"
        )

        return chunk_info

    def _update_remaining_time_estimates(self, state: CollectionState):
        """
        ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¶”ì • ì—…ë°ì´íŠ¸

        Args:
            state: ìˆ˜ì§‘ ìƒíƒœ ê°ì²´
        """
        current_time = datetime.now(timezone.utc)

        # ì™„ë£Œëœ ì²­í¬ ìˆ˜
        completed_chunks_count = len(state.completed_chunks)

        if completed_chunks_count == 0:
            # ì•„ì§ ì™„ë£Œëœ ì²­í¬ê°€ ì—†ìœ¼ë©´ ì´ˆê¸° ì¶”ì •ê°’ ì‚¬ìš©
            return

        # í‰ê·  ì²­í¬ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_elapsed = (current_time - state.start_time).total_seconds()
        state.avg_chunk_duration = total_elapsed / completed_chunks_count

        # ë‚¨ì€ ì²­í¬ ìˆ˜ ê³„ì‚°
        state.remaining_chunks = state.estimated_total_chunks - completed_chunks_count

        # ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¶”ì •
        if state.remaining_chunks > 0:
            state.estimated_remaining_seconds = state.remaining_chunks * state.avg_chunk_duration
        else:
            state.estimated_remaining_seconds = 0.0

        # ì˜ˆìƒ ì™„ë£Œ ì‹œê°„ ì—…ë°ì´íŠ¸
        if state.estimated_remaining_seconds > 0:
            state.estimated_completion_time = current_time + timedelta(
                seconds=state.estimated_remaining_seconds
            )

        # ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ë¡
        state.last_update_time = current_time

        logger.debug(f"ë‚¨ì€ ì‹œê°„ ì—…ë°ì´íŠ¸: í‰ê·  ì²­í¬ ì‹œê°„ {state.avg_chunk_duration:.2f}ì´ˆ, "
                     f"ë‚¨ì€ ì²­í¬ {state.remaining_chunks}ê°œ, ì˜ˆìƒ ë‚¨ì€ ì‹œê°„ {state.estimated_remaining_seconds:.1f}ì´ˆ")

    def get_realtime_remaining_time(self, request_id: str) -> Dict[str, Any]:
        """
        ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì •ë³´ ì¡°íšŒ

        Args:
            request_id: ìˆ˜ì§‘ ìš”ì²­ ID

        Returns:
            Dict: ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì •ë³´
        """
        if request_id not in self.active_collections:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ìš”ì²­ ID: {request_id}")

        state = self.active_collections[request_id]
        current_time = datetime.now(timezone.utc)

        # ì‹¤ì‹œê°„ ë‚¨ì€ ì‹œê°„ ì¬ê³„ì‚° (ìµœì‹  ì •ë³´ ë°˜ì˜)
        if len(state.completed_chunks) > 0:
            self._update_remaining_time_estimates(state)

        return {
            "request_id": request_id,
            "current_time": current_time.isoformat(),
            "avg_chunk_duration": state.avg_chunk_duration,
            "remaining_chunks": state.remaining_chunks,
            "estimated_remaining_seconds": state.estimated_remaining_seconds,
            "estimated_completion_time": (
                state.estimated_completion_time.isoformat()
                if state.estimated_completion_time else None
            ),
            "progress_percentage": round(
                (state.total_collected / state.total_requested) * 100, 1
            ),
            "is_on_track": self._is_collection_on_track(state),
            "performance_info": self._get_performance_info(state)
        }

    def _is_collection_on_track(self, state: CollectionState) -> bool:
        """
        ìˆ˜ì§‘ì´ ì˜ˆì •ëŒ€ë¡œ ì§„í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸

        Args:
            state: ìˆ˜ì§‘ ìƒíƒœ ê°ì²´

        Returns:
            bool: ì˜ˆì •ëŒ€ë¡œ ì§„í–‰ ì¤‘ì´ë©´ True
        """
        if len(state.completed_chunks) == 0:
            return True  # ì•„ì§ ì‹œì‘ ë‹¨ê³„

        # ì´ˆê¸° ì˜ˆìƒ ì‹œê°„ê³¼ í˜„ì¬ í‰ê·  ì‹œê°„ ë¹„êµ
        initial_expected_duration = state.estimated_total_chunks / self.api_rate_limit_rps

        # í˜„ì¬ í‰ê· ì´ ì´ˆê¸° ì˜ˆìƒì˜ 120% ì´ë‚´ë©´ ì •ìƒ
        return state.avg_chunk_duration <= initial_expected_duration * 1.2

    def _get_performance_info(self, state: CollectionState) -> Dict[str, Any]:
        """
        ìˆ˜ì§‘ ì„±ëŠ¥ ì •ë³´ ì œê³µ

        Args:
            state: ìˆ˜ì§‘ ìƒíƒœ ê°ì²´

        Returns:
            Dict: ì„±ëŠ¥ ì •ë³´
        """
        if len(state.completed_chunks) == 0:
            return {"status": "ì´ˆê¸° ë‹¨ê³„"}

        initial_expected_rps = self.api_rate_limit_rps
        current_rps = 1.0 / state.avg_chunk_duration if state.avg_chunk_duration > 0 else 0

        return {
            "expected_rps": initial_expected_rps,
            "current_rps": round(current_rps, 2),
            "efficiency_percentage": round((current_rps / initial_expected_rps) * 100, 1),
            "avg_chunk_duration": state.avg_chunk_duration,
            "total_chunks_completed": len(state.completed_chunks)
        }

    def cleanup_completed_collections(self, max_age_hours: int = 24):
        """ì™„ë£Œëœ ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬"""

        current_time = datetime.now(timezone.utc)
        cutoff_time = current_time - timedelta(hours=max_age_hours)

        completed_ids = []
        for request_id, state in self.active_collections.items():
            if state.is_completed and state.start_time < cutoff_time:
                completed_ids.append(request_id)

        for request_id in completed_ids:
            del self.active_collections[request_id]
            logger.debug(f"ì™„ë£Œëœ ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬: {request_id}")

        if completed_ids:
            logger.info(f"ìˆ˜ì§‘ ìƒíƒœ ì •ë¦¬ ì™„ë£Œ: {len(completed_ids)}ê°œ")

    def _calculate_chunk_end_time(self, chunk_info: ChunkInfo) -> datetime:
        """
        ì²­í¬ ìš”ì²­ì˜ ì˜ˆìƒ ì¢…ë£Œ ì‹œì  ê³„ì‚°

        Args:
            chunk_info: ì²­í¬ ì •ë³´

        Returns:
            datetime: ì˜ˆìƒ ì¢…ë£Œ ì‹œì  (ì—…ë¹„íŠ¸ API ë‚´ë¦¼ì°¨ìˆœ ê¸°ì¤€)
        """
        # ì—…ë¹„íŠ¸ APIëŠ” ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ to ì‹œì ì—ì„œ countë§Œí¼ ì´ì „ìœ¼ë¡œ ê³„ì‚°
        timeframe_seconds = TimeUtils.get_timeframe_delta(chunk_info.timeframe).total_seconds()
        end_time = chunk_info.to - timedelta(seconds=(chunk_info.count - 1) * timeframe_seconds)

        logger.debug(f"ì²­í¬ ì¢…ë£Œ ì‹œê°„ ê³„ì‚°: {chunk_info.to} â†’ {end_time} (ë‚´ë¦¼ì°¨ìˆœ {chunk_info.count}ê°œ)")
        return end_time

    async def _collect_data_by_overlap_strategy(
        self,
        chunk_info: ChunkInfo,
        overlap_result
    ) -> tuple[list, bool]:
        """
        ê²¹ì¹¨ ë¶„ì„ ê²°ê³¼ì— ë”°ë¥¸ ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘

        Args:
            chunk_info: ì²­í¬ ì •ë³´
            overlap_result: OverlapAnalyzer ë¶„ì„ ê²°ê³¼

        Returns:
            tuple[list, bool]: (ìˆ˜ì§‘ëœ ìº”ë“¤ ë°ì´í„° ë¦¬ìŠ¤íŠ¸, API í˜¸ì¶œ ì—¬ë¶€)
        """
        from upbit_auto_trading.infrastructure.market_data.candle.candle_models import OverlapStatus

        status = overlap_result.status

        if status == OverlapStatus.COMPLETE_OVERLAP:
            # ì™„ì „ ê²¹ì¹¨: DBì—ì„œë§Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            logger.debug("ì™„ì „ ê²¹ì¹¨ ê°ì§€ â†’ DBì—ì„œ ë°ì´í„° ì¡°íšŒ")
            db_candles = await self.repository.get_candles_by_range(
                chunk_info.symbol,
                chunk_info.timeframe,
                overlap_result.db_start,
                overlap_result.db_end
            )
            return db_candles, False  # API í˜¸ì¶œ ì—†ìŒ

        elif status == OverlapStatus.NO_OVERLAP:
            # ê²¹ì¹¨ ì—†ìŒ: APIì—ì„œë§Œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            logger.debug("ê²¹ì¹¨ ì—†ìŒ â†’ ì „ì²´ API ìš”ì²­")
            api_response = await self._fetch_chunk_from_api(chunk_info)
            candle_data_list = self._convert_upbit_response_to_candles(
                api_response, chunk_info.timeframe
            )
            return candle_data_list, True

        elif status in [OverlapStatus.PARTIAL_START, OverlapStatus.PARTIAL_MIDDLE_CONTINUOUS]:
            # ë¶€ë¶„ ê²¹ì¹¨: DB + API í˜¼í•© ì²˜ë¦¬
            logger.debug(f"ë¶€ë¶„ ê²¹ì¹¨ ({status.value}) â†’ DB + API í˜¼í•© ì²˜ë¦¬")

            # DB ë°ì´í„° ìˆ˜ì§‘
            db_candles = []
            if overlap_result.db_start and overlap_result.db_end:
                db_candles = await self.repository.get_candles_by_range(
                    chunk_info.symbol,
                    chunk_info.timeframe,
                    overlap_result.db_start,
                    overlap_result.db_end
                )

            # API ë°ì´í„° ìˆ˜ì§‘
            api_candles = []
            if overlap_result.api_start and overlap_result.api_end:
                # ì„ì‹œ ì²­í¬ ì •ë³´ë¡œ API í˜¸ì¶œ
                api_chunk = ChunkInfo(
                    chunk_id=f"{chunk_info.chunk_id}_api_partial",
                    chunk_index=chunk_info.chunk_index,
                    symbol=chunk_info.symbol,
                    timeframe=chunk_info.timeframe,
                    count=self._calculate_api_count(overlap_result.api_start, overlap_result.api_end, chunk_info.timeframe),
                    to=overlap_result.api_start,
                    end=overlap_result.api_end,
                    status="pending"
                )

                api_response = await self._fetch_chunk_from_api(api_chunk)
                api_candles = self._convert_upbit_response_to_candles(
                    api_response, chunk_info.timeframe
                )

            # ì‹œê°„ìˆœìœ¼ë¡œ ë³‘í•© (ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ê¸°ì¤€)
            combined_candles = self._merge_candles_by_time(db_candles, api_candles)
            return combined_candles, True

        else:
            # PARTIAL_MIDDLE_FRAGMENT ë˜ëŠ” ê¸°íƒ€: ì•ˆì „í•œ í´ë°± â†’ ì „ì²´ API ìš”ì²­
            logger.warning(f"ë³µì¡í•œ ê²¹ì¹¨ ìƒíƒœ ({status.value}) â†’ API í´ë°±")
            api_response = await self._fetch_chunk_from_api(chunk_info)
            candle_data_list = self._convert_upbit_response_to_candles(
                api_response, chunk_info.timeframe
            )
            return candle_data_list, True

    def _ensure_utc_timezone(self, dt: datetime) -> datetime:
        """DateTimeì´ timezoneì„ ê°€ì§€ì§€ ì•Šìœ¼ë©´ UTCë¡œ ì„¤ì •"""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt

    def _calculate_api_count(self, start_time: datetime, end_time: datetime, timeframe: str) -> int:
        """API ìš”ì²­ì— í•„ìš”í•œ ìº”ë“¤ ê°œìˆ˜ ê³„ì‚° (ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ê¸°ì¤€)"""
        timeframe_seconds = TimeUtils.get_timeframe_delta(timeframe).total_seconds()
        time_diff = (start_time - end_time).total_seconds()  # ë‚´ë¦¼ì°¨ìˆœì´ë¯€ë¡œ start > end
        return max(1, int(time_diff / timeframe_seconds) + 1)

    def _merge_candles_by_time(self, db_candles: list, api_candles: list) -> list:
        """
        ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìº”ë“¤ ë°ì´í„° ë³‘í•© (ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ìœ ì§€)

        Args:
            db_candles: DBì—ì„œ ê°€ì ¸ì˜¨ ìº”ë“¤ ë°ì´í„°
            api_candles: APIì—ì„œ ê°€ì ¸ì˜¨ ìº”ë“¤ ë°ì´í„°

        Returns:
            list: ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ë³‘í•© ìº”ë“¤ ë°ì´í„°
        """
        # ëª¨ë“  ìº”ë“¤ì„ í•©ì¹˜ê³  ì¤‘ë³µ ì œê±° í›„ ì‹œê°„ìˆœ ì •ë ¬
        all_candles = db_candles + api_candles

        # candle_date_time_utc ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° (ë”•ì…”ë„ˆë¦¬ í™œìš©)
        unique_candles = {}
        for candle in all_candles:
            if hasattr(candle, 'candle_date_time_utc'):
                key = candle.candle_date_time_utc
            elif isinstance(candle, dict):
                key = candle.get('candle_date_time_utc')
            else:
                continue

            if key not in unique_candles:
                unique_candles[key] = candle

        # ì—…ë¹„íŠ¸ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ìµœì‹  â†’ ê³¼ê±°)
        sorted_candles = sorted(
            unique_candles.values(),
            key=lambda x: (
                x.candle_date_time_utc if hasattr(x, 'candle_date_time_utc')
                else x.get('candle_date_time_utc', '')
            ),
            reverse=True  # ë‚´ë¦¼ì°¨ìˆœ
        )

        logger.debug(f"ìº”ë“¤ ë³‘í•© ì™„ë£Œ: DB {len(db_candles)}ê°œ + API {len(api_candles)}ê°œ â†’ ë³‘í•© {len(sorted_candles)}ê°œ")
        return sorted_candles
