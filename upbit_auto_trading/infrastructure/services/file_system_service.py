"""
File System Service Adapter

ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë³€ê²½ì— í•„ìš”í•œ íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ì„ ë‹´ë‹¹í•˜ëŠ” ì™¸ë¶€ ì„œë¹„ìŠ¤ ì–´ëŒ‘í„°ì…ë‹ˆë‹¤.
Domain Layerì˜ Use Caseë“¤ì´ íŒŒì¼ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ì‘ìš©í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

Features:
- ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì´ë™/ë³µì‚¬/ë°±ì—…
- ì•ˆì „í•œ íŒŒì¼ ì‘ì—… (ì›ìì  ì—°ì‚°)
- íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ (ì²´í¬ì„¬)
- ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
- ê¶Œí•œ ê²€ì¦

Design Principles:
- Adapter Pattern: Domain Service â†’ Infrastructure Service ë§¤í•‘
- Fail-Safe Operations: ëª¨ë“  íŒŒì¼ ì‘ì—…ì€ ì›ìì  ìˆ˜í–‰
- Error Transparency: Domain ì˜ˆì™¸ë¡œ ë³€í™˜í•˜ì—¬ ì „íŒŒ
"""

import os
import shutil
import hashlib
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

from upbit_auto_trading.infrastructure.logging import create_component_logger
from upbit_auto_trading.infrastructure.configuration import InfrastructurePaths

logger = create_component_logger("FileSystemService")


class FileSystemService:
    """íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ì„ ìœ„í•œ Infrastructure Service"""

    def __init__(self):
        self._logger = logger
        self._paths = InfrastructurePaths()
        self._temp_dir = self._paths.DATA_DIR / ".tmp"
        self._temp_dir.mkdir(exist_ok=True)

    async def copy_database_file(self, source_path: Path, target_path: Path) -> bool:
        """
        ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë³µì‚¬

        Args:
            source_path: ì›ë³¸ íŒŒì¼ ê²½ë¡œ
            target_path: ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ

        Returns:
            ë³µì‚¬ ì„±ê³µ ì—¬ë¶€
        """
        try:
            self._logger.info(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë³µì‚¬ ì‹œì‘: {source_path} â†’ {target_path}")

            # 1. ì „ì œ ì¡°ê±´ ê²€ì¦
            if not source_path.exists():
                raise FileNotFoundError(f"ì›ë³¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_path}")

            if not self._check_disk_space(source_path, target_path.parent):
                raise OSError("ë””ìŠ¤í¬ ê³µê°„ì´ ë¶€ì¡±í•©ë‹ˆë‹¤")

            # 2. ëŒ€ìƒ ë””ë ‰í† ë¦¬ ìƒì„±
            target_path.parent.mkdir(parents=True, exist_ok=True)

            # 3. ì„ì‹œ íŒŒì¼ë¡œ ë³µì‚¬ (ì›ìì  ì—°ì‚°)
            temp_target = self._temp_dir / f"{target_path.name}.tmp"
            shutil.copy2(source_path, temp_target)

            # 4. ë¬´ê²°ì„± ê²€ì¦
            if not self._verify_file_integrity(source_path, temp_target):
                temp_target.unlink(missing_ok=True)
                raise OSError("íŒŒì¼ ë³µì‚¬ ì¤‘ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨")

            # 5. ì›ìì  ì´ë™ (ìµœì¢… ì ìš©)
            shutil.move(temp_target, target_path)

            self._logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ: {target_path}")
            return True

        except Exception as e:
            self._logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë³µì‚¬ ì‹¤íŒ¨: {e}")
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            temp_target = self._temp_dir / f"{target_path.name}.tmp"
            temp_target.unlink(missing_ok=True)
            raise

    async def move_database_file(self, source_path: Path, target_path: Path) -> bool:
        """
        ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì´ë™

        Args:
            source_path: ì›ë³¸ íŒŒì¼ ê²½ë¡œ
            target_path: ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ

        Returns:
            ì´ë™ ì„±ê³µ ì—¬ë¶€
        """
        try:
            self._logger.info(f"ğŸ“ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì´ë™ ì‹œì‘: {source_path} â†’ {target_path}")

            # 1. ë°±ì—… ìƒì„± (ì•ˆì „ì„ ìœ„í•´)
            backup_path = self._create_backup_path(source_path)
            await self.copy_database_file(source_path, backup_path)

            # 2. íŒŒì¼ ì´ë™
            success = await self.copy_database_file(source_path, target_path)

            if success:
                # 3. ì›ë³¸ íŒŒì¼ ì‚­ì œ
                source_path.unlink()
                self._logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì´ë™ ì™„ë£Œ: {target_path}")

                # 4. ë°±ì—… íŒŒì¼ ì •ë¦¬ (í•„ìš” ì‹œ ë³´ê´€)
                # backup_path.unlink()  # ì„ì‹œ ë°±ì—… ì‚­ì œ

                return True
            else:
                raise OSError("íŒŒì¼ ë³µì‚¬ ë‹¨ê³„ì—ì„œ ì‹¤íŒ¨")

        except Exception as e:
            self._logger.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì´ë™ ì‹¤íŒ¨: {e}")
            raise

    async def create_backup(self, source_path: Path, backup_dir: Path) -> Path:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—… ìƒì„±

        Args:
            source_path: ë°±ì—…í•  íŒŒì¼ ê²½ë¡œ
            backup_dir: ë°±ì—… ë””ë ‰í† ë¦¬

        Returns:
            ìƒì„±ëœ ë°±ì—… íŒŒì¼ ê²½ë¡œ
        """
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_filename = f"{source_path.stem}_backup_{timestamp}{source_path.suffix}"
            backup_path = backup_dir / backup_filename

            self._logger.info(f"ğŸ’¾ ë°±ì—… ìƒì„± ì‹œì‘: {source_path} â†’ {backup_path}")

            await self.copy_database_file(source_path, backup_path)

            # ë°±ì—… ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                'source_file': str(source_path),
                'backup_created': datetime.now().isoformat(),
                'file_size': backup_path.stat().st_size,
                'checksum': self._calculate_checksum(backup_path)
            }

            metadata_path = backup_path.with_suffix('.meta.json')
            import json
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)

            self._logger.info(f"âœ… ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_path}")
            return backup_path

        except Exception as e:
            self._logger.error(f"âŒ ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def restore_backup(self, backup_path: Path, target_path: Path) -> bool:
        """
        ë°±ì—…ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë³µì›

        Args:
            backup_path: ë°±ì—… íŒŒì¼ ê²½ë¡œ
            target_path: ë³µì›í•  ëŒ€ìƒ ê²½ë¡œ

        Returns:
            ë³µì› ì„±ê³µ ì—¬ë¶€
        """
        try:
            self._logger.info(f"ğŸ”„ ë°±ì—… ë³µì› ì‹œì‘: {backup_path} â†’ {target_path}")

            # 1. ë°±ì—… íŒŒì¼ ê²€ì¦
            if not backup_path.exists():
                raise FileNotFoundError(f"ë°±ì—… íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {backup_path}")

            # 2. ë©”íƒ€ë°ì´í„° ê²€ì¦ (ìˆëŠ” ê²½ìš°)
            metadata_path = backup_path.with_suffix('.meta.json')
            if metadata_path.exists():
                if not self._verify_backup_metadata(backup_path, metadata_path):
                    raise OSError("ë°±ì—… íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨")

            # 3. í˜„ì¬ íŒŒì¼ ë°±ì—… (ë³µì› ì „ ì•ˆì „ì¥ì¹˜)
            if target_path.exists():
                safety_backup = await self.create_backup(target_path, self._temp_dir)
                self._logger.info(f"ğŸ›¡ï¸ ë³µì› ì „ ì•ˆì „ ë°±ì—… ìƒì„±: {safety_backup}")

            # 4. ë³µì› ì‹¤í–‰
            success = await self.copy_database_file(backup_path, target_path)

            if success:
                self._logger.info(f"âœ… ë°±ì—… ë³µì› ì™„ë£Œ: {target_path}")
                return True
            else:
                raise OSError("ë°±ì—… ë³µì› ì‹¤íŒ¨")

        except Exception as e:
            self._logger.error(f"âŒ ë°±ì—… ë³µì› ì‹¤íŒ¨: {e}")
            raise

    def get_file_info(self, file_path: Path) -> Dict[str, Any]:
        """
        íŒŒì¼ ì •ë³´ ì¡°íšŒ

        Args:
            file_path: ì¡°íšŒí•  íŒŒì¼ ê²½ë¡œ

        Returns:
            íŒŒì¼ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        try:
            if not file_path.exists():
                return {'exists': False}

            stat = file_path.stat()

            return {
                'exists': True,
                'size_bytes': stat.st_size,
                'size_mb': round(stat.st_size / (1024 * 1024), 2),
                'created': datetime.fromtimestamp(stat.st_ctime).isoformat(),
                'modified': datetime.fromtimestamp(stat.st_mtime).isoformat(),
                'checksum': self._calculate_checksum(file_path),
                'is_readable': os.access(file_path, os.R_OK),
                'is_writable': os.access(file_path, os.W_OK)
            }

        except Exception as e:
            self._logger.error(f"âŒ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {'exists': False, 'error': str(e)}

    def list_backups(self, backup_dir: Path, pattern: str = "*_backup_*") -> List[Dict[str, Any]]:
        """
        ë°±ì—… íŒŒì¼ ëª©ë¡ ì¡°íšŒ

        Args:
            backup_dir: ë°±ì—… ë””ë ‰í† ë¦¬
            pattern: ë°±ì—… íŒŒì¼ íŒ¨í„´

        Returns:
            ë°±ì—… íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not backup_dir.exists():
                return []

            backup_files = []
            for backup_path in backup_dir.glob(pattern):
                if backup_path.is_file() and not backup_path.name.endswith('.meta.json'):
                    info = self.get_file_info(backup_path)
                    info['path'] = str(backup_path)
                    info['name'] = backup_path.name
                    backup_files.append(info)

            # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ì—­ìˆœ ì •ë ¬ (ìµœì‹  ìˆœ)
            backup_files.sort(key=lambda x: x.get('modified', ''), reverse=True)

            return backup_files

        except Exception as e:
            self._logger.error(f"âŒ ë°±ì—… ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def cleanup_old_backups(self, backup_dir: Path, keep_count: int = 10) -> int:
        """
        ì˜¤ë˜ëœ ë°±ì—… íŒŒì¼ ì •ë¦¬

        Args:
            backup_dir: ë°±ì—… ë””ë ‰í† ë¦¬
            keep_count: ë³´ê´€í•  ë°±ì—… ê°œìˆ˜

        Returns:
            ì‚­ì œëœ ë°±ì—… ê°œìˆ˜
        """
        try:
            backups = self.list_backups(backup_dir)

            if len(backups) <= keep_count:
                return 0

            deleted_count = 0
            backups_to_delete = backups[keep_count:]

            for backup in backups_to_delete:
                backup_path = Path(backup['path'])
                metadata_path = backup_path.with_suffix('.meta.json')

                # ë°±ì—… íŒŒì¼ ì‚­ì œ
                if backup_path.exists():
                    backup_path.unlink()
                    deleted_count += 1

                # ë©”íƒ€ë°ì´í„° íŒŒì¼ ì‚­ì œ
                if metadata_path.exists():
                    metadata_path.unlink()

            self._logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ë°±ì—… {deleted_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
            return deleted_count

        except Exception as e:
            self._logger.error(f"âŒ ë°±ì—… ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0

    # === Private Helper Methods ===

    def _check_disk_space(self, source_path: Path, target_dir: Path) -> bool:
        """ë””ìŠ¤í¬ ê³µê°„ í™•ì¸ (íŒŒì¼ í¬ê¸°ì˜ 2ë°° ì—¬ìœ  ê³µê°„ í•„ìš”)"""
        try:
            file_size = source_path.stat().st_size
            required_space = file_size * 2  # 2ë°° ì—¬ìœ 

            free_space = shutil.disk_usage(target_dir).free

            return free_space >= required_space

        except Exception:
            return False

    def _calculate_checksum(self, file_path: Path) -> str:
        """íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (SHA-256)"""
        sha256_hash = hashlib.sha256()

        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)

        return sha256_hash.hexdigest()

    def _verify_file_integrity(self, source_path: Path, target_path: Path) -> bool:
        """íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦ (ì²´í¬ì„¬ ë¹„êµ)"""
        try:
            source_checksum = self._calculate_checksum(source_path)
            target_checksum = self._calculate_checksum(target_path)

            return source_checksum == target_checksum

        except Exception:
            return False

    def _create_backup_path(self, source_path: Path) -> Path:
        """ì„ì‹œ ë°±ì—… ê²½ë¡œ ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"{source_path.stem}_temp_backup_{timestamp}{source_path.suffix}"
        return self._temp_dir / backup_filename

    def _verify_backup_metadata(self, backup_path: Path, metadata_path: Path) -> bool:
        """ë°±ì—… ë©”íƒ€ë°ì´í„° ê²€ì¦"""
        try:
            import json

            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)

            # íŒŒì¼ í¬ê¸° ê²€ì¦
            if backup_path.stat().st_size != metadata.get('file_size'):
                return False

            # ì²´í¬ì„¬ ê²€ì¦
            current_checksum = self._calculate_checksum(backup_path)
            if current_checksum != metadata.get('checksum'):
                return False

            return True

        except Exception:
            return False


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ (Singleton íŒ¨í„´)
file_system_service = FileSystemService()
