"""
Smart Data Provider V4.0 - ë©”ì¸ API

Layer 1: í†µí•© API ì¸í„°í˜ì´ìŠ¤
- ì§€ëŠ¥í˜• ë‹¨ì¼/ë‹¤ì¤‘ ì‹¬ë³¼ ì²˜ë¦¬
- ê³„ì¸µì  ìºì‹œ ì‹œìŠ¤í…œ í†µí•©
- ì‹¤ì‹œê°„ ë°ì´í„° ê´€ë¦¬ í†µí•©
- ì„±ëŠ¥ ìµœì í™” (ëª©í‘œ: 500+ symbols/sec)
- SmartRouter í˜¸í™˜ì„±
"""
import time
import threading
from typing import Dict, List, Optional, Any, Callable
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

from upbit_auto_trading.infrastructure.logging import create_component_logger
from .market_data_models import DataResponse, Priority, PerformanceMetrics
from .market_data_manager import MarketDataManager
from .market_data_cache import MarketDataCache
from .realtime_data_manager import RealtimeDataManager
from ..smart_routing.smart_router import SmartRouter
from ..smart_routing.models import DataRequest, DataType

logger = create_component_logger("SmartDataProvider")


@dataclass
class BatchRequestResult:
    """ë°°ì¹˜ ìš”ì²­ ê²°ê³¼"""
    successful_symbols: List[str]
    failed_symbols: List[str]
    total_response_time_ms: float
    symbols_per_second: float
    cache_hit_rate: float
    individual_responses: Dict[str, Dict[str, Any]]


class SmartDataProvider:
    """Smart Data Provider V4.0 - ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, max_workers: int = 10):
        # Layer 3: ë°ì´í„° ê´€ë¦¬
        self.data_manager = MarketDataManager()

        # Layer 2: ìºì‹œ & ì‹¤ì‹œê°„
        self.cache_system = MarketDataCache()
        self.realtime_manager = RealtimeDataManager()

        # Layer 1: API ì²˜ë¦¬
        self.max_workers = max_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)

        # SmartRouter í†µí•© (ì‹¤ì œ API í†µì‹ )
        self.smart_router = SmartRouter()

        # SmartRouter ì‚¬ì „ ì´ˆê¸°í™” (WebSocket ì—°ê²° í¬í•¨)
        self._initialize_smart_router()

        # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
        self._request_count = 0
        self._start_time = time.time()
        self._lock = threading.RLock()

        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self._batch_threshold = 5  # 5ê°œ ì´ìƒì¼ ë•Œ ë°°ì¹˜ ì²˜ë¦¬
        self._max_batch_size = 50  # ìµœëŒ€ 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬

        logger.info(f"SmartDataProvider V4.0 ì´ˆê¸°í™”: max_workers={max_workers}")
        self._log_initialization_complete()

    def _initialize_smart_router(self) -> None:
        """SmartRouter ì‚¬ì „ ì´ˆê¸°í™” (WebSocket ì—°ê²° í¬í•¨)"""
        import asyncio
        try:
            # ì´ë²¤íŠ¸ ë£¨í”„ í™•ë³´
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            # SmartRouter ë¹„ë™ê¸° ì´ˆê¸°í™” (WebSocket ì—°ê²° í¬í•¨)
            loop.run_until_complete(self.smart_router.initialize())
            logger.info("âœ… SmartRouter ì‚¬ì „ ì´ˆê¸°í™” ì™„ë£Œ (WebSocket ì—°ê²°ë¨)")

        except Exception as e:
            logger.warning(f"SmartRouter ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e} - ë°ì´í„° ìš”ì²­ ì‹œ ì¬ì‹œë„")

    def _log_initialization_complete(self) -> None:
        """ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Smart Data Provider V4.0 ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("=" * 60)
        logger.info("ğŸ“Š Layer 1: ì§€ëŠ¥í˜• API ì²˜ë¦¬ (ë‹¨ì¼/ë°°ì¹˜)")
        logger.info("âš¡ Layer 2: í†µí•© ìºì‹œ + ì‹¤ì‹œê°„ ê´€ë¦¬")
        logger.info("ğŸ—„ï¸  Layer 3: ë°ì´í„° ê´€ë¦¬ + DB í†µí•©")
        logger.info("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ: 500+ symbols/sec")
        logger.info("=" * 60)

    # =================================================================
    # ë‹¨ì¼ ì‹¬ë³¼ ì²˜ë¦¬ (SmartRouter í˜¸í™˜)
    # =================================================================

    def get_ticker(self, symbol: str, priority: Priority = Priority.NORMAL) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ í‹°ì»¤ ì¡°íšŒ (SmartRouter í˜¸í™˜)"""
        return self._get_single_data(symbol, "ticker", priority)

    def invalidate_cache(self, symbol: Optional[str] = None, data_type: Optional[str] = None) -> None:
        """ìºì‹œ ë¬´íš¨í™”

        Args:
            symbol: íŠ¹ì • ì‹¬ë³¼ ìºì‹œë§Œ ë¬´íš¨í™” (Noneì´ë©´ ëª¨ë“  ì‹¬ë³¼)
            data_type: íŠ¹ì • ë°ì´í„° íƒ€ì…ë§Œ ë¬´íš¨í™” (Noneì´ë©´ ëª¨ë“  íƒ€ì…)
        """
        if symbol and data_type:
            # íŠ¹ì • ì‹¬ë³¼ì˜ íŠ¹ì • ë°ì´í„° íƒ€ì…ë§Œ ë¬´íš¨í™”
            cache_key = f"{data_type}_{symbol}"
            # FastCacheëŠ” delete ë©”ì„œë“œê°€ ì—†ìœ¼ë¯€ë¡œ ì§ì ‘ ì‚­ì œ
            if hasattr(self.cache_system.fast_cache, '_cache'):
                with self.cache_system.fast_cache._lock:
                    self.cache_system.fast_cache._cache.pop(cache_key, None)
                    self.cache_system.fast_cache._timestamps.pop(cache_key, None)
            logger.info(f"ìºì‹œ ë¬´íš¨í™”: {cache_key}")
        elif symbol:
            # íŠ¹ì • ì‹¬ë³¼ì˜ ëª¨ë“  ë°ì´í„° íƒ€ì… ë¬´íš¨í™”
            data_types = ["ticker", "orderbook", "trades"]
            for dt in data_types:
                cache_key = f"{dt}_{symbol}"
                if hasattr(self.cache_system.fast_cache, '_cache'):
                    with self.cache_system.fast_cache._lock:
                        self.cache_system.fast_cache._cache.pop(cache_key, None)
                        self.cache_system.fast_cache._timestamps.pop(cache_key, None)
            logger.info(f"ì‹¬ë³¼ {symbol}ì˜ ëª¨ë“  ìºì‹œ ë¬´íš¨í™”")
        else:
            # ì „ì²´ ìºì‹œ ë¬´íš¨í™”
            self.cache_system.fast_cache.clear()
            logger.info("ì „ì²´ ìºì‹œ ë¬´íš¨í™”")

    def _get_from_memory_cache_only(self, cache_key: str) -> Optional[dict]:
        """ë©”ëª¨ë¦¬ ìºì‹œì—ì„œë§Œ ì¡°íšŒ (DB ì¡°íšŒ ì•ˆí•¨)

        ì‹¤ì‹œê°„ ë°ì´í„°(í‹°ì»¤/í˜¸ê°€/ì²´ê²°)ëŠ” ë©”ëª¨ë¦¬ ìºì‹œë§Œ í™•ì¸
        """
        # FastCache ë¨¼ì € í™•ì¸ (ê°€ì¥ ë¹ ë¦„)
        cached_data = self.cache_system.fast_cache.get(cache_key)
        if cached_data:
            return cached_data

        # MemoryRealtimeCache í™•ì¸
        if hasattr(self.cache_system, 'memory_cache'):
            cached_data = self.cache_system.memory_cache.get(cache_key)
            if cached_data:
                return cached_data

        return None

    def _store_in_memory_cache_only(self, cache_key: str, data: dict, data_type: str) -> None:
        """ë©”ëª¨ë¦¬ ìºì‹œì—ë§Œ ì €ì¥ (DB ì €ì¥ ì•ˆí•¨)

        ì‹¤ì‹œê°„ ë°ì´í„°(í‹°ì»¤/í˜¸ê°€/ì²´ê²°)ëŠ” ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©
        """
        # FastCacheì— ì €ì¥ (200ms TTL)
        self.cache_system.fast_cache.set(cache_key, data)

        # MemoryRealtimeCacheì— ì €ì¥ (60ì´ˆ TTL)
        if hasattr(self.cache_system, 'memory_cache'):
            ttl = 60.0 if data_type == "ticker" else 30.0  # í‹°ì»¤ëŠ” 60ì´ˆ, ë‚˜ë¨¸ì§€ëŠ” 30ì´ˆ
            self.cache_system.memory_cache.set(cache_key, data, ttl)

    def validate_data_integrity(self, data: dict, data_type: str) -> bool:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦

        Args:
            data: ê²€ì¦í•  ë°ì´í„°
            data_type: ë°ì´í„° íƒ€ì…

        Returns:
            ë°ì´í„°ê°€ ìœ íš¨í•œì§€ ì—¬ë¶€
        """
        if not isinstance(data, dict):
            return False

        if data_type == "ticker":
            required_fields = ["market", "trade_price", "timestamp"]
            return all(field in data for field in required_fields)
        elif data_type == "orderbook":
            required_fields = ["market", "orderbook_units"]
            return all(field in data for field in required_fields)
        elif data_type == "trades":
            required_fields = ["market", "trade_price", "trade_volume"]
            return all(field in data for field in required_fields)

        return True  # ê¸°íƒ€ ë°ì´í„° íƒ€ì…ì€ ê¸°ë³¸ì ìœ¼ë¡œ ìœ íš¨í•˜ë‹¤ê³  ê°€ì •

    def get_orderbook(self, symbol: str, priority: Priority = Priority.NORMAL) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ í˜¸ê°€ ì¡°íšŒ"""
        return self._get_single_data(symbol, "orderbook", priority)

    def get_trades(self, symbol: str, priority: Priority = Priority.NORMAL) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ ì²´ê²° ë‚´ì—­ ì¡°íšŒ"""
        return self._get_single_data(symbol, "trades", priority)

    def get_candles(
        self,
        symbol: str,
        candle_type: str = "1m",
        count: int = 200,
        priority: Priority = Priority.NORMAL
    ) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ ìº”ë“¤ ì¡°íšŒ"""
        cache_key = f"candles_{symbol}_{candle_type}_{count}"

        # ìºì‹œ í™•ì¸
        cached_data = self.cache_system.get(cache_key, "candles")
        if cached_data:
            return DataResponse(
                success=True,
                data=cached_data,
                error_message=None,
                cache_hit=True,
                response_time_ms=0.1,
                data_source="cache"
            )

        # ë°ì´í„° ê´€ë¦¬ìë¥¼ í†µí•œ ì¡°íšŒ
        start_time = time.time()

        try:
            candle_data = self.data_manager.get_candle_data(symbol, candle_type, count)

            response_time_ms = (time.time() - start_time) * 1000

            if candle_data:
                # ìº”ë“¤ ë°ì´í„°ë¥¼ Dict í˜•íƒœë¡œ í¬ì¥
                response_data = {
                    'symbol': symbol,
                    'candle_type': candle_type,
                    'count': count,
                    'candles': candle_data
                }

                # ìºì‹œ ì €ì¥
                self.cache_system.set(cache_key, response_data, "candles")

                return DataResponse(
                    success=True,
                    data=response_data,
                    error_message=None,
                    cache_hit=False,
                    response_time_ms=response_time_ms,
                    data_source="database"
                )
            else:
                return DataResponse(
                    success=False,
                    data=None,
                    error_message=f"ìº”ë“¤ ë°ì´í„° ì—†ìŒ: {symbol}",
                    cache_hit=False,
                    response_time_ms=response_time_ms,
                    data_source="database"
                )

        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            logger.error(f"ìº”ë“¤ ì¡°íšŒ ì˜¤ë¥˜ ({symbol}): {e}")

            return DataResponse(
                success=False,
                data=None,
                error_message=str(e),
                cache_hit=False,
                response_time_ms=response_time_ms,
                data_source="error"
            )

    def _get_single_data(self, symbol: str, data_type: str, priority: Priority) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ (ê³µí†µ ë¡œì§)"""
        cache_key = f"{data_type}_{symbol}"

        # ğŸ”§ ë°ì´í„° íƒ€ì…ë³„ ìºì‹œ ì¡°íšŒ ì „ëµ
        start_time = time.time()
        cached_data = None

        if data_type in ["ticker", "orderbook", "trades"]:
            # ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ ìºì‹œë§Œ í™•ì¸
            cached_data = self._get_from_memory_cache_only(cache_key)
        else:
            # ìº”ë“¤ ë°ì´í„°ëŠ” í†µí•© ìºì‹œ í™•ì¸ (DB + ë©”ëª¨ë¦¬)
            cached_data = self.cache_system.get(cache_key, data_type)

        if cached_data:
            cache_time_ms = (time.time() - start_time) * 1000
            return DataResponse.create_success(
                data=cached_data,
                cache_hit=True,
                response_time_ms=cache_time_ms,
                data_source="cache"
            )

        try:
            # SmartRouterë¥¼ í†µí•œ ì‹¤ì œ API í˜¸ì¶œ
            from ..smart_routing.models import RealtimePriority

            data_request = DataRequest(
                symbols=[symbol],
                data_type=DataType.TICKER if data_type == "ticker" else DataType.ORDERBOOK,
                realtime_priority=RealtimePriority.MEDIUM
            )

            # SmartRouter ë¹„ë™ê¸° í˜¸ì¶œì„ ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

            router_response = loop.run_until_complete(self.smart_router.get_data(data_request))

            # ğŸ”§ SmartRouter ì‘ë‹µ êµ¬ì¡° ê²€ì¦ ë° ë°ì´í„° ì¶”ì¶œ
            if not router_response.get("success"):
                error_msg = router_response.get("error", "SmartRouter ìš”ì²­ ì‹¤íŒ¨")
                raise Exception(error_msg)

            # SmartRouterì˜ "data" í•„ë“œê°€ ì™„ì „í•œ í‹°ì»¤ ë°ì´í„°ë¥¼ í¬í•¨
            # ì‹¬ë³¼ë³„ ë¶„ë¦¬ êµ¬ì¡°ê°€ ì•„ë‹Œ ì§ì ‘ ë°ì´í„° êµ¬ì¡°
            router_data = router_response.get("data")
            if not router_data:
                raise Exception("SmartRouterì—ì„œ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ")

            # ë‹¨ì¼ ì‹¬ë³¼ ìš”ì²­ì˜ ê²½ìš° router_dataê°€ ì§ì ‘ í‹°ì»¤ ë°ì´í„°
            # ë‹¤ì¤‘ ì‹¬ë³¼ì˜ ê²½ìš° {symbol: data} êµ¬ì¡°ì¼ ìˆ˜ ìˆìŒ
            if isinstance(router_data, dict):
                # market í•„ë“œê°€ ìˆìœ¼ë©´ ì§ì ‘ í‹°ì»¤ ë°ì´í„°
                if 'market' in router_data or 'trade_price' in router_data:
                    api_data = router_data
                # ì‹¬ë³¼ì„ í‚¤ë¡œ í•˜ëŠ” êµ¬ì¡°ì¸ì§€ í™•ì¸
                elif symbol in router_data:
                    api_data = router_data[symbol]
                else:
                    # ì˜ˆìƒì¹˜ ëª»í•œ êµ¬ì¡° - ì „ì²´ ë°ì´í„° ì‚¬ìš©
                    logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ SmartRouter ë°ì´í„° êµ¬ì¡°: {list(router_data.keys())}")
                    api_data = router_data
            else:
                api_data = router_data

            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë°ì´í„° ì†ŒìŠ¤ ì •ë³´ ì¶”ì¶œ
            metadata = router_response.get("metadata", {})
            data_source = metadata.get("channel", "unknown")

            # ğŸ”§ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
            if not self.validate_data_integrity(api_data, data_type):
                logger.error(f"ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨ - symbol: {symbol}, data_type: {data_type}")
                logger.error(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°: {api_data}")
                raise Exception(f"ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ ì‹¤íŒ¨: {data_type} ë°ì´í„°ê°€ í•„ìˆ˜ í•„ë“œë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŒ")

            response_time_ms = (time.time() - start_time) * 1000

            # ğŸ”§ ë°ì´í„° íƒ€ì…ë³„ ìºì‹œ ì „ëµ ì ìš©
            # í‹°ì»¤/í˜¸ê°€/ì²´ê²°: ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš© (DB ì €ì¥ ì•ˆí•¨)
            # ìº”ë“¤: DB + ë©”ëª¨ë¦¬ ìºì‹œ ì‚¬ìš© (ë°ì´í„° ì›ì²œ)
            try:
                if data_type in ["ticker", "orderbook", "trades"]:
                    # ì‹¤ì‹œê°„ ë°ì´í„°ëŠ” ë©”ëª¨ë¦¬ ìºì‹œë§Œ ì‚¬ìš©
                    self._store_in_memory_cache_only(cache_key, api_data, data_type)
                    logger.debug(f"ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ ì„±ê³µ: {cache_key} ({data_type})")
                else:
                    # ìº”ë“¤ ë°ì´í„°ëŠ” DB + ë©”ëª¨ë¦¬ ìºì‹œ ëª¨ë‘ ì‚¬ìš©
                    self.cache_system.set(cache_key, api_data, data_type)
                    logger.debug(f"DB+ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ ì„±ê³µ: {cache_key} ({data_type})")
            except Exception as cache_error:
                logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {cache_error} - ë°ì´í„°ëŠ” ë°˜í™˜ë¨")

            # í†µê³„ ì—…ë°ì´íŠ¸
            with self._lock:
                self._request_count += 1

            return DataResponse.create_success(
                data=api_data,
                cache_hit=False,
                response_time_ms=response_time_ms,
                data_source=data_source,
                data_type=data_type,
                data_timestamp=datetime.now()
            )

        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            logger.error(f"ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({symbol}, {data_type}): {e}")

            return DataResponse.create_error(
                error=str(e),
                cache_hit=False,
                response_time_ms=response_time_ms,
                data_source="error"
            )

    # =================================================================
    # ë‹¤ì¤‘ ì‹¬ë³¼ ì²˜ë¦¬ (ë°°ì¹˜ ìµœì í™”)
    # =================================================================

    def get_multiple_tickers(
        self,
        symbols: List[str],
        priority: Priority = Priority.NORMAL
    ) -> BatchRequestResult:
        """ë‹¤ì¤‘ ì‹¬ë³¼ í‹°ì»¤ ì¡°íšŒ"""
        return self._get_multiple_data(symbols, "ticker", priority)

    def get_multiple_orderbooks(
        self,
        symbols: List[str],
        priority: Priority = Priority.NORMAL
    ) -> BatchRequestResult:
        """ë‹¤ì¤‘ ì‹¬ë³¼ í˜¸ê°€ ì¡°íšŒ"""
        return self._get_multiple_data(symbols, "orderbook", priority)

    def _get_multiple_data(
        self,
        symbols: List[str],
        data_type: str,
        priority: Priority
    ) -> BatchRequestResult:
        """ë‹¤ì¤‘ ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ (ì§€ëŠ¥í˜• ë°°ì¹˜ ì²˜ë¦¬)"""
        start_time = time.time()

        # ì…ë ¥ ê²€ì¦
        if not symbols:
            return BatchRequestResult(
                successful_symbols=[],
                failed_symbols=[],
                total_response_time_ms=0.0,
                symbols_per_second=0.0,
                cache_hit_rate=0.0,
                individual_responses={}
            )

        logger.info(f"ë‹¤ì¤‘ {data_type} ì¡°íšŒ ì‹œì‘: {len(symbols)}ê°œ ì‹¬ë³¼")

        # ë°°ì¹˜ í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
        if len(symbols) < self._batch_threshold:
            # ì†Œê·œëª¨: ìˆœì°¨ ì²˜ë¦¬
            return self._process_sequential(symbols, data_type, priority, start_time)
        else:
            # ëŒ€ê·œëª¨: ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬
            return self._process_parallel_batches(symbols, data_type, priority, start_time)

    def _process_sequential(
        self,
        symbols: List[str],
        data_type: str,
        priority: Priority,
        start_time: float
    ) -> BatchRequestResult:
        """ìˆœì°¨ ì²˜ë¦¬ (ì†Œê·œëª¨ ìš”ì²­ìš©)"""
        successful_symbols = []
        failed_symbols = []
        individual_responses = {}
        cache_hits = 0

        for symbol in symbols:
            response = self._get_single_data(symbol, data_type, priority)

            if response.success:
                successful_symbols.append(symbol)
                individual_responses[symbol] = response.data

                if response.cache_hit:
                    cache_hits += 1
            else:
                failed_symbols.append(symbol)
                individual_responses[symbol] = {'error': response.error_message}

        total_time_ms = (time.time() - start_time) * 1000
        symbols_per_second = len(symbols) / (total_time_ms / 1000) if total_time_ms > 0 else 0
        cache_hit_rate = (cache_hits / len(symbols) * 100) if symbols else 0

        logger.info(f"ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_symbols)}/{len(symbols)} ì„±ê³µ, {symbols_per_second:.1f} symbols/sec")

        return BatchRequestResult(
            successful_symbols=successful_symbols,
            failed_symbols=failed_symbols,
            total_response_time_ms=total_time_ms,
            symbols_per_second=symbols_per_second,
            cache_hit_rate=cache_hit_rate,
            individual_responses=individual_responses
        )

    def _process_parallel_batches(
        self,
        symbols: List[str],
        data_type: str,
        priority: Priority,
        start_time: float
    ) -> BatchRequestResult:
        """ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ (ëŒ€ê·œëª¨ ìš”ì²­ìš©)"""
        # ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [
            symbols[i:i + self._max_batch_size]
            for i in range(0, len(symbols), self._max_batch_size)
        ]

        logger.info(f"ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬: {len(symbols)}ê°œ ì‹¬ë³¼ì„ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")

        successful_symbols = []
        failed_symbols = []
        individual_responses = {}
        total_cache_hits = 0

        # ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_batch = {
                executor.submit(self._process_batch, batch, data_type, priority): batch
                for batch in batches
            }

            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]

                try:
                    batch_result = future.result()

                    successful_symbols.extend(batch_result.successful_symbols)
                    failed_symbols.extend(batch_result.failed_symbols)
                    individual_responses.update(batch_result.individual_responses)

                    # ìºì‹œ íˆíŠ¸ ê³„ì‚°
                    total_cache_hits += batch_result.cache_hit_rate * len(batch) / 100

                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    failed_symbols.extend(batch)

                    for symbol in batch:
                        individual_responses[symbol] = {'error': str(e)}

        total_time_ms = (time.time() - start_time) * 1000
        symbols_per_second = len(symbols) / (total_time_ms / 1000) if total_time_ms > 0 else 0
        cache_hit_rate = (total_cache_hits / len(symbols) * 100) if symbols else 0

        logger.info(f"ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_symbols)}/{len(symbols)} ì„±ê³µ, {symbols_per_second:.1f} symbols/sec")

        return BatchRequestResult(
            successful_symbols=successful_symbols,
            failed_symbols=failed_symbols,
            total_response_time_ms=total_time_ms,
            symbols_per_second=symbols_per_second,
            cache_hit_rate=cache_hit_rate,
            individual_responses=individual_responses
        )

    def _process_batch(self, batch_symbols: List[str], data_type: str, priority: Priority) -> BatchRequestResult:
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        start_time = time.time()

        successful_symbols = []
        failed_symbols = []
        individual_responses = {}
        cache_hits = 0

        for symbol in batch_symbols:
            response = self._get_single_data(symbol, data_type, priority)

            if response.success:
                successful_symbols.append(symbol)
                individual_responses[symbol] = response.data

                if response.cache_hit:
                    cache_hits += 1
            else:
                failed_symbols.append(symbol)
                individual_responses[symbol] = {'error': response.error_message}

        total_time_ms = (time.time() - start_time) * 1000
        symbols_per_second = len(batch_symbols) / (total_time_ms / 1000) if total_time_ms > 0 else 0
        cache_hit_rate = (cache_hits / len(batch_symbols) * 100) if batch_symbols else 0

        return BatchRequestResult(
            successful_symbols=successful_symbols,
            failed_symbols=failed_symbols,
            total_response_time_ms=total_time_ms,
            symbols_per_second=symbols_per_second,
            cache_hit_rate=cache_hit_rate,
            individual_responses=individual_responses
        )

    # =================================================================
    # ì‹¤ì‹œê°„ ë°ì´í„° ê´€ë¦¬
    # =================================================================

    def subscribe_realtime_data(
        self,
        symbols: List[str],
        data_types: Optional[List[str]] = None,
        callback: Optional[Callable] = None
    ) -> str:
        """ì‹¤ì‹œê°„ ë°ì´í„° êµ¬ë…"""
        subscription_id = f"sub_{int(time.time() * 1000)}"
        data_types = data_types or ["ticker"]

        success = self.realtime_manager.subscribe_to_symbols(
            subscription_id, symbols, data_types, callback
        )

        if success:
            logger.info(f"ì‹¤ì‹œê°„ êµ¬ë… ìƒì„±: {subscription_id}, {len(symbols)}ê°œ ì‹¬ë³¼")
            return subscription_id
        else:
            logger.error(f"ì‹¤ì‹œê°„ êµ¬ë… ì‹¤íŒ¨: {symbols}")
            return ""

    def unsubscribe_realtime_data(self, subscription_id: str) -> bool:
        """ì‹¤ì‹œê°„ ë°ì´í„° êµ¬ë… í•´ì œ"""
        return self.realtime_manager.unsubscribe_symbols(subscription_id)

    # =================================================================
    # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
    # =================================================================

    def get_performance_metrics(self) -> PerformanceMetrics:
        """ì„±ëŠ¥ ì§€í‘œ ì¡°íšŒ"""
        with self._lock:
            elapsed_time = time.time() - self._start_time
            throughput = self._request_count / elapsed_time if elapsed_time > 0 else 0

            return PerformanceMetrics(
                total_requests=self._request_count,
                successful_requests=self._request_count,  # ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ê³„ì‚° í•„ìš”
                failed_requests=0,
                avg_response_time_ms=0.0,  # ì‹¤ì œë¡œëŠ” í‰ê·  ê³„ì‚° í•„ìš”
                min_response_time_ms=0.0,
                max_response_time_ms=0.0,
                symbols_per_second=round(throughput, 2)
            )

    def get_comprehensive_status(self) -> Dict[str, Any]:
        """ì¢…í•© ìƒíƒœ ì •ë³´"""
        # ê° ë ˆì´ì–´ë³„ í†µê³„ ìˆ˜ì§‘
        cache_stats = self.cache_system.get_comprehensive_stats()
        realtime_stats = self.realtime_manager.get_comprehensive_stats()
        data_manager_stats = self.data_manager.get_comprehensive_status()
        performance_metrics = self.get_performance_metrics()

        return {
            'system_info': {
                'version': '4.0',
                'uptime_seconds': time.time() - self._start_time,
                'max_workers': self.max_workers,
                'batch_threshold': self._batch_threshold,
                'max_batch_size': self._max_batch_size
            },
            'performance': {
                'total_requests': performance_metrics.total_requests,
                'symbols_per_second': performance_metrics.symbols_per_second,
                'target_symbols_per_second': 500
            },
            'layer_1_api': {
                'request_count': self._request_count,
                'thread_pool_workers': self.max_workers
            },
            'layer_2_cache': cache_stats,
            'layer_2_realtime': realtime_stats,
            'layer_3_data': data_manager_stats,
            'timestamp': datetime.now().isoformat()
        }

    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
            logger.info("SmartDataProvider V4.0 ì •ë¦¬ ì™„ë£Œ")
        except Exception:
            pass
