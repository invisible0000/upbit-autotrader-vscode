"""
Smart Data Provider V4.0 - ë©”ì¸ API

Layer 1: í†µí•© API ì¸í„°í˜ì´ìŠ¤
- ì§€ëŠ¥í˜• ë‹¨ì¼/ë‹¤ì¤‘ ì‹¬ë³¼ ì²˜ë¦¬
- ê³„ì¸µì  ìºì‹œ ì‹œìŠ¤í…œ í†µí•©
- ì‹¤ì‹œê°„ ë°ì´í„° ê´€ë¦¬ í†µí•©
- ì„±ëŠ¥ ìµœì í™” (ëª©í‘œ: 500+ symbols/sec)
- SmartRouter í˜¸í™˜ì„±
"""
import time
import asyncio
import threading
from typing import Dict, List, Optional, Union, Any, Callable
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass

from upbit_auto_trading.infrastructure.logging import create_component_logger
from .market_data_models import DataResponse, Priority, PerformanceMetrics
from .market_data_manager import MarketDataManager
from .market_data_cache import MarketDataCache
from .realtime_data_manager import RealtimeDataManager

logger = create_component_logger("SmartDataProvider")


@dataclass
class BatchRequestResult:
    """ë°°ì¹˜ ìš”ì²­ ê²°ê³¼"""
    successful_symbols: List[str]
    failed_symbols: List[str]
    total_response_time_ms: float
    symbols_per_second: float
    cache_hit_rate: float
    individual_responses: Dict[str, Dict[str, Any]]


class SmartDataProvider:
    """Smart Data Provider V4.0 - ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self, max_workers: int = 10):
        # Layer 3: ë°ì´í„° ê´€ë¦¬
        self.data_manager = MarketDataManager()

        # Layer 2: ìºì‹œ & ì‹¤ì‹œê°„
        self.cache_system = MarketDataCache()
        self.realtime_manager = RealtimeDataManager()

        # Layer 1: API ì²˜ë¦¬
        self.max_workers = max_workers
        self.thread_pool = ThreadPoolExecutor(max_workers=max_workers)

        # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
        self._request_count = 0
        self._start_time = time.time()
        self._lock = threading.RLock()

        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self._batch_threshold = 5  # 5ê°œ ì´ìƒì¼ ë•Œ ë°°ì¹˜ ì²˜ë¦¬
        self._max_batch_size = 50  # ìµœëŒ€ 50ê°œì”© ë°°ì¹˜ ì²˜ë¦¬

        logger.info(f"SmartDataProvider V4.0 ì´ˆê¸°í™”: max_workers={max_workers}")
        self._log_initialization_complete()

    def _log_initialization_complete(self) -> None:
        """ì´ˆê¸°í™” ì™„ë£Œ ë¡œê·¸"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Smart Data Provider V4.0 ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("=" * 60)
        logger.info("ğŸ“Š Layer 1: ì§€ëŠ¥í˜• API ì²˜ë¦¬ (ë‹¨ì¼/ë°°ì¹˜)")
        logger.info("âš¡ Layer 2: í†µí•© ìºì‹œ + ì‹¤ì‹œê°„ ê´€ë¦¬")
        logger.info("ğŸ—„ï¸  Layer 3: ë°ì´í„° ê´€ë¦¬ + DB í†µí•©")
        logger.info("ğŸ¯ ì„±ëŠ¥ ëª©í‘œ: 500+ symbols/sec")
        logger.info("=" * 60)

    # =================================================================
    # ë‹¨ì¼ ì‹¬ë³¼ ì²˜ë¦¬ (SmartRouter í˜¸í™˜)
    # =================================================================

    def get_ticker(self, symbol: str, priority: Priority = Priority.NORMAL) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ í‹°ì»¤ ì¡°íšŒ (SmartRouter í˜¸í™˜)"""
        return self._get_single_data(symbol, "ticker", priority)

    def get_orderbook(self, symbol: str, priority: Priority = Priority.NORMAL) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ í˜¸ê°€ ì¡°íšŒ"""
        return self._get_single_data(symbol, "orderbook", priority)

    def get_trades(self, symbol: str, priority: Priority = Priority.NORMAL) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ ì²´ê²° ë‚´ì—­ ì¡°íšŒ"""
        return self._get_single_data(symbol, "trades", priority)

    def get_candles(
        self,
        symbol: str,
        candle_type: str = "1m",
        count: int = 200,
        priority: Priority = Priority.NORMAL
    ) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ ìº”ë“¤ ì¡°íšŒ"""
        cache_key = f"candles_{symbol}_{candle_type}_{count}"

        # ìºì‹œ í™•ì¸
        cached_data = self.cache_system.get(cache_key, "candles")
        if cached_data:
            return DataResponse(
                success=True,
                data=cached_data,
                error_message=None,
                cache_hit=True,
                response_time_ms=0.1,
                data_source="cache"
            )

        # ë°ì´í„° ê´€ë¦¬ìë¥¼ í†µí•œ ì¡°íšŒ
        start_time = time.time()

        try:
            candle_data = self.data_manager.get_candle_data(symbol, candle_type, count)

            response_time_ms = (time.time() - start_time) * 1000

            if candle_data:
                # ìºì‹œ ì €ì¥
                self.cache_system.set(cache_key, candle_data, "candles")

                return DataResponse(
                    success=True,
                    data=candle_data,
                    error_message=None,
                    cache_hit=False,
                    response_time_ms=response_time_ms,
                    data_source="database"
                )
            else:
                return DataResponse(
                    success=False,
                    data=None,
                    error_message=f"ìº”ë“¤ ë°ì´í„° ì—†ìŒ: {symbol}",
                    cache_hit=False,
                    response_time_ms=response_time_ms,
                    data_source="database"
                )

        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            logger.error(f"ìº”ë“¤ ì¡°íšŒ ì˜¤ë¥˜ ({symbol}): {e}")

            return DataResponse(
                success=False,
                data=None,
                error_message=str(e),
                cache_hit=False,
                response_time_ms=response_time_ms,
                data_source="error"
            )

    def _get_single_data(self, symbol: str, data_type: str, priority: Priority) -> DataResponse:
        """ë‹¨ì¼ ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ (ê³µí†µ ë¡œì§)"""
        cache_key = f"{data_type}_{symbol}"

        # ìºì‹œ í™•ì¸
        start_time = time.time()
        cached_data = self.cache_system.get(cache_key, data_type)

        if cached_data:
            cache_time_ms = (time.time() - start_time) * 1000
            return DataResponse(
                success=True,
                data=cached_data,
                error_message=None,
                cache_hit=True,
                response_time_ms=cache_time_ms,
                data_source="cache"
            )

        # ì‹¤ì œ ë°ì´í„° ì¡°íšŒ (ì‹œë®¬ë ˆì´ì…˜)
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” upbit API í˜¸ì¶œ
            api_data = self._simulate_api_call(symbol, data_type)

            response_time_ms = (time.time() - start_time) * 1000

            # ìºì‹œ ì €ì¥
            self.cache_system.set(cache_key, api_data, data_type)

            # í†µê³„ ì—…ë°ì´íŠ¸
            with self._lock:
                self._request_count += 1

            return DataResponse(
                success=True,
                data=api_data,
                error_message=None,
                cache_hit=False,
                response_time_ms=response_time_ms,
                data_source="api"
            )

        except Exception as e:
            response_time_ms = (time.time() - start_time) * 1000
            logger.error(f"ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({symbol}, {data_type}): {e}")

            return DataResponse(
                success=False,
                data=None,
                error_message=str(e),
                cache_hit=False,
                response_time_ms=response_time_ms,
                data_source="error"
            )

    def _simulate_api_call(self, symbol: str, data_type: str) -> Dict[str, Any]:
        """API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œ êµ¬í˜„ ì‹œ ëŒ€ì²´ í•„ìš”)"""
        # ì‹¤ì œ ì‘ë‹µ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (0.1-0.5ì´ˆ)
        import random
        time.sleep(random.uniform(0.001, 0.005))  # 1-5ms ì‹œë®¬ë ˆì´ì…˜

        # ì‹œë®¬ë ˆì´ì…˜ ë°ì´í„°
        return {
            'symbol': symbol,
            'data_type': data_type,
            'timestamp': datetime.now().isoformat(),
            'price': random.uniform(1000, 100000),
            'volume': random.uniform(1, 1000),
            'simulated': True
        }

    # =================================================================
    # ë‹¤ì¤‘ ì‹¬ë³¼ ì²˜ë¦¬ (ë°°ì¹˜ ìµœì í™”)
    # =================================================================

    def get_multiple_tickers(
        self,
        symbols: List[str],
        priority: Priority = Priority.NORMAL
    ) -> BatchRequestResult:
        """ë‹¤ì¤‘ ì‹¬ë³¼ í‹°ì»¤ ì¡°íšŒ"""
        return self._get_multiple_data(symbols, "ticker", priority)

    def get_multiple_orderbooks(
        self,
        symbols: List[str],
        priority: Priority = Priority.NORMAL
    ) -> BatchRequestResult:
        """ë‹¤ì¤‘ ì‹¬ë³¼ í˜¸ê°€ ì¡°íšŒ"""
        return self._get_multiple_data(symbols, "orderbook", priority)

    def _get_multiple_data(
        self,
        symbols: List[str],
        data_type: str,
        priority: Priority
    ) -> BatchRequestResult:
        """ë‹¤ì¤‘ ì‹¬ë³¼ ë°ì´í„° ì¡°íšŒ (ì§€ëŠ¥í˜• ë°°ì¹˜ ì²˜ë¦¬)"""
        start_time = time.time()

        # ì…ë ¥ ê²€ì¦
        if not symbols:
            return BatchRequestResult(
                successful_symbols=[],
                failed_symbols=[],
                total_response_time_ms=0.0,
                symbols_per_second=0.0,
                cache_hit_rate=0.0,
                individual_responses={}
            )

        logger.info(f"ë‹¤ì¤‘ {data_type} ì¡°íšŒ ì‹œì‘: {len(symbols)}ê°œ ì‹¬ë³¼")

        # ë°°ì¹˜ í¬ê¸°ì— ë”°ë¥¸ ì²˜ë¦¬ ë°©ì‹ ê²°ì •
        if len(symbols) < self._batch_threshold:
            # ì†Œê·œëª¨: ìˆœì°¨ ì²˜ë¦¬
            return self._process_sequential(symbols, data_type, priority, start_time)
        else:
            # ëŒ€ê·œëª¨: ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬
            return self._process_parallel_batches(symbols, data_type, priority, start_time)

    def _process_sequential(
        self,
        symbols: List[str],
        data_type: str,
        priority: Priority,
        start_time: float
    ) -> BatchRequestResult:
        """ìˆœì°¨ ì²˜ë¦¬ (ì†Œê·œëª¨ ìš”ì²­ìš©)"""
        successful_symbols = []
        failed_symbols = []
        individual_responses = {}
        cache_hits = 0

        for symbol in symbols:
            response = self._get_single_data(symbol, data_type, priority)

            if response.success:
                successful_symbols.append(symbol)
                individual_responses[symbol] = response.data

                if response.cache_hit:
                    cache_hits += 1
            else:
                failed_symbols.append(symbol)
                individual_responses[symbol] = {'error': response.error_message}

        total_time_ms = (time.time() - start_time) * 1000
        symbols_per_second = len(symbols) / (total_time_ms / 1000) if total_time_ms > 0 else 0
        cache_hit_rate = (cache_hits / len(symbols) * 100) if symbols else 0

        logger.info(f"ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_symbols)}/{len(symbols)} ì„±ê³µ, {symbols_per_second:.1f} symbols/sec")

        return BatchRequestResult(
            successful_symbols=successful_symbols,
            failed_symbols=failed_symbols,
            total_response_time_ms=total_time_ms,
            symbols_per_second=symbols_per_second,
            cache_hit_rate=cache_hit_rate,
            individual_responses=individual_responses
        )

    def _process_parallel_batches(
        self,
        symbols: List[str],
        data_type: str,
        priority: Priority,
        start_time: float
    ) -> BatchRequestResult:
        """ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ (ëŒ€ê·œëª¨ ìš”ì²­ìš©)"""
        # ë°°ì¹˜ë¡œ ë¶„í• 
        batches = [
            symbols[i:i + self._max_batch_size]
            for i in range(0, len(symbols), self._max_batch_size)
        ]

        logger.info(f"ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬: {len(symbols)}ê°œ ì‹¬ë³¼ì„ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")

        successful_symbols = []
        failed_symbols = []
        individual_responses = {}
        total_cache_hits = 0

        # ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_batch = {
                executor.submit(self._process_batch, batch, data_type, priority): batch
                for batch in batches
            }

            for future in as_completed(future_to_batch):
                batch = future_to_batch[future]

                try:
                    batch_result = future.result()

                    successful_symbols.extend(batch_result.successful_symbols)
                    failed_symbols.extend(batch_result.failed_symbols)
                    individual_responses.update(batch_result.individual_responses)

                    # ìºì‹œ íˆíŠ¸ ê³„ì‚°
                    total_cache_hits += batch_result.cache_hit_rate * len(batch) / 100

                except Exception as e:
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    failed_symbols.extend(batch)

                    for symbol in batch:
                        individual_responses[symbol] = {'error': str(e)}

        total_time_ms = (time.time() - start_time) * 1000
        symbols_per_second = len(symbols) / (total_time_ms / 1000) if total_time_ms > 0 else 0
        cache_hit_rate = (total_cache_hits / len(symbols) * 100) if symbols else 0

        logger.info(f"ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_symbols)}/{len(symbols)} ì„±ê³µ, {symbols_per_second:.1f} symbols/sec")

        return BatchRequestResult(
            successful_symbols=successful_symbols,
            failed_symbols=failed_symbols,
            total_response_time_ms=total_time_ms,
            symbols_per_second=symbols_per_second,
            cache_hit_rate=cache_hit_rate,
            individual_responses=individual_responses
        )

    def _process_batch(self, batch_symbols: List[str], data_type: str, priority: Priority) -> BatchRequestResult:
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬"""
        start_time = time.time()

        successful_symbols = []
        failed_symbols = []
        individual_responses = {}
        cache_hits = 0

        for symbol in batch_symbols:
            response = self._get_single_data(symbol, data_type, priority)

            if response.success:
                successful_symbols.append(symbol)
                individual_responses[symbol] = response.data

                if response.cache_hit:
                    cache_hits += 1
            else:
                failed_symbols.append(symbol)
                individual_responses[symbol] = {'error': response.error_message}

        total_time_ms = (time.time() - start_time) * 1000
        symbols_per_second = len(batch_symbols) / (total_time_ms / 1000) if total_time_ms > 0 else 0
        cache_hit_rate = (cache_hits / len(batch_symbols) * 100) if batch_symbols else 0

        return BatchRequestResult(
            successful_symbols=successful_symbols,
            failed_symbols=failed_symbols,
            total_response_time_ms=total_time_ms,
            symbols_per_second=symbols_per_second,
            cache_hit_rate=cache_hit_rate,
            individual_responses=individual_responses
        )

    # =================================================================
    # ì‹¤ì‹œê°„ ë°ì´í„° ê´€ë¦¬
    # =================================================================

    def subscribe_realtime_data(
        self,
        symbols: List[str],
        data_types: List[str] = None,
        callback: Optional[Callable] = None
    ) -> str:
        """ì‹¤ì‹œê°„ ë°ì´í„° êµ¬ë…"""
        subscription_id = f"sub_{int(time.time() * 1000)}"
        data_types = data_types or ["ticker"]

        success = self.realtime_manager.subscribe_to_symbols(
            subscription_id, symbols, data_types, callback
        )

        if success:
            logger.info(f"ì‹¤ì‹œê°„ êµ¬ë… ìƒì„±: {subscription_id}, {len(symbols)}ê°œ ì‹¬ë³¼")
            return subscription_id
        else:
            logger.error(f"ì‹¤ì‹œê°„ êµ¬ë… ì‹¤íŒ¨: {symbols}")
            return ""

    def unsubscribe_realtime_data(self, subscription_id: str) -> bool:
        """ì‹¤ì‹œê°„ ë°ì´í„° êµ¬ë… í•´ì œ"""
        return self.realtime_manager.unsubscribe_symbols(subscription_id)

    # =================================================================
    # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
    # =================================================================

    def get_performance_metrics(self) -> PerformanceMetrics:
        """ì„±ëŠ¥ ì§€í‘œ ì¡°íšŒ"""
        with self._lock:
            elapsed_time = time.time() - self._start_time
            throughput = self._request_count / elapsed_time if elapsed_time > 0 else 0

            # ìºì‹œ í†µê³„
            cache_stats = self.cache_system.get_comprehensive_stats()

            return PerformanceMetrics(
                total_requests=self._request_count,
                successful_requests=self._request_count,  # ì‹¤ì œë¡œëŠ” ë” ì •í™•í•œ ê³„ì‚° í•„ìš”
                failed_requests=0,
                avg_response_time_ms=0.0,  # ì‹¤ì œë¡œëŠ” í‰ê·  ê³„ì‚° í•„ìš”
                min_response_time_ms=0.0,
                max_response_time_ms=0.0,
                symbols_per_second=round(throughput, 2)
            )

    def get_comprehensive_status(self) -> Dict[str, Any]:
        """ì¢…í•© ìƒíƒœ ì •ë³´"""
        # ê° ë ˆì´ì–´ë³„ í†µê³„ ìˆ˜ì§‘
        cache_stats = self.cache_system.get_comprehensive_stats()
        realtime_stats = self.realtime_manager.get_comprehensive_stats()
        data_manager_stats = self.data_manager.get_comprehensive_status()
        performance_metrics = self.get_performance_metrics()

        return {
            'system_info': {
                'version': '4.0',
                'uptime_seconds': time.time() - self._start_time,
                'max_workers': self.max_workers,
                'batch_threshold': self._batch_threshold,
                'max_batch_size': self._max_batch_size
            },
            'performance': {
                'total_requests': performance_metrics.total_requests,
                'symbols_per_second': performance_metrics.symbols_per_second,
                'target_symbols_per_second': 500
            },
            'layer_1_api': {
                'request_count': self._request_count,
                'thread_pool_workers': self.max_workers
            },
            'layer_2_cache': cache_stats,
            'layer_2_realtime': realtime_stats,
            'layer_3_data': data_manager_stats,
            'timestamp': datetime.now().isoformat()
        }

    def __del__(self):
        """ì†Œë©¸ì - ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=True)
            logger.info("SmartDataProvider V4.0 ì •ë¦¬ ì™„ë£Œ")
        except:
            pass
