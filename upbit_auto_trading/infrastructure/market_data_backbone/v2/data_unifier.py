"""
DataUnifier V3 - Phase 1.3 ê³ ê¸‰ ë°ì´í„° í†µí•© ë° ì •ê·œí™” ì‹œìŠ¤í…œ

âœ… Phase 1.1: ê¸°ë³¸ REST API ë°ì´í„° í†µí•©
âœ… Phase 1.2: WebSocket ë°ì´í„° í†µí•©
ğŸ”¥ Phase 1.3: ê³ ê¸‰ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ
    - ë°ì´í„° ì •ê·œí™” ë¡œì§
    - í†µí•© ìŠ¤í‚¤ë§ˆ ê´€ë¦¬
    - ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
    - ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ
    - ë°ì´í„° ì¼ê´€ì„± ê²€ì¦
"""

from typing import Dict, Any, List, Optional, Tuple
from decimal import Decimal, ROUND_HALF_UP
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import asyncio
import hashlib
import json
from collections import defaultdict

from upbit_auto_trading.infrastructure.logging import create_component_logger
from .market_data_backbone import TickerData


class DataSource(Enum):
    """ë°ì´í„° ì†ŒìŠ¤ ì—´ê±°í˜•"""
    REST = "rest"
    WEBSOCKET = "websocket"
    WEBSOCKET_SIMPLE = "websocket_simple"
    CACHED = "cached"


class DataQuality(Enum):
    """ë°ì´í„° í’ˆì§ˆ ë“±ê¸‰"""
    HIGH = "high"           # ì™„ì „í•œ ë°ì´í„°, ëª¨ë“  í•„ë“œ ê²€ì¦ í†µê³¼
    MEDIUM = "medium"       # ì¼ë¶€ í•„ë“œ ëˆ„ë½ ë˜ëŠ” ì¶”ì •ê°’ í¬í•¨
    LOW = "low"             # í•„ìˆ˜ í•„ë“œë§Œ ì¡´ì¬, ê²€ì¦ ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜ ë³´ì •
    INVALID = "invalid"     # ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ë°ì´í„°


@dataclass(frozen=True)
class NormalizedTickerData:
    """ì •ê·œí™”ëœ í‹°ì»¤ ë°ì´í„° ëª¨ë¸ (Phase 1.3)"""
    # ê¸°ë³¸ TickerData í™•ì¥
    ticker_data: TickerData

    # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    data_quality: DataQuality
    confidence_score: Decimal  # 0.0 ~ 1.0
    normalization_timestamp: datetime
    data_checksum: str

    # ê²€ì¦ ê²°ê³¼
    validation_errors: Tuple[str, ...]
    corrected_fields: Tuple[str, ...]

    # ì„±ëŠ¥ ë©”íŠ¸ë¦­
    processing_time_ms: Decimal
    data_source_priority: int  # 1(ìµœê³ ) ~ 10(ìµœì €)


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬ êµ¬ì¡°"""
    data: NormalizedTickerData
    created_at: datetime
    access_count: int
    last_access: datetime
    ttl_seconds: int


class DataNormalizer:
    """ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self._logger = create_component_logger("DataNormalizer")

        # ì •ê·œí™” ê·œì¹™
        self._price_precision = Decimal('0.01')  # ì› ë‹¨ìœ„
        self._rate_precision = Decimal('0.0001')  # 0.01% ë‹¨ìœ„
        self._volume_precision = Decimal('0.00000001')  # 8ìë¦¬ ì†Œìˆ˜ì 

        # ê²€ì¦ ê·œì¹™
        self._max_price_change_rate = Decimal('30.0')  # 30% ì´ìƒ ë³€í™”ìœ¨ì€ ì˜ì‹¬
        self._min_valid_price = Decimal('1.0')  # ìµœì†Œ ìœ íš¨ ê°€ê²©
        self._max_valid_price = Decimal('1000000000.0')  # ìµœëŒ€ ìœ íš¨ ê°€ê²© (10ì–µì›)

    def normalize_ticker(self, raw_data: Dict[str, Any], source: DataSource) -> NormalizedTickerData:
        """
        ì›ì‹œ ë°ì´í„°ë¥¼ ì •ê·œí™”ëœ í˜•íƒœë¡œ ë³€í™˜

        Args:
            raw_data: API ì›ì‹œ ì‘ë‹µ ë°ì´í„°
            source: ë°ì´í„° ì†ŒìŠ¤ íƒ€ì…

        Returns:
            NormalizedTickerData: ì •ê·œí™”ëœ ë°ì´í„°
        """
        start_time = datetime.now()

        # 0. í•„ìˆ˜ í•„ë“œ ê²€ì¦ (ì˜ˆì™¸ ë°œìƒ ê°€ëŠ¥)
        self._validate_required_fields(raw_data, source)

        try:
            # 1. ê¸°ë³¸ TickerData ìƒì„±
            ticker_data = self._create_ticker_data(raw_data, source)

            # 2. ë°ì´í„° ì •ê·œí™”
            normalized_ticker = self._apply_normalization(ticker_data)

            # 3. ê²€ì¦ ìˆ˜í–‰
            validation_errors, corrected_fields = self._validate_and_correct(normalized_ticker)

            # 4. í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
            quality = self._determine_quality(normalized_ticker, validation_errors)

            # 5. ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = self._calculate_confidence(normalized_ticker, source, quality)

            # 6. ì²´í¬ì„¬ ìƒì„±
            checksum = self._generate_checksum(normalized_ticker)

            processing_time = (datetime.now() - start_time).total_seconds() * 1000

            return NormalizedTickerData(
                ticker_data=normalized_ticker,
                data_quality=quality,
                confidence_score=confidence,
                normalization_timestamp=datetime.now(),
                data_checksum=checksum,
                validation_errors=tuple(validation_errors),
                corrected_fields=tuple(corrected_fields),
                processing_time_ms=Decimal(str(processing_time)),
                data_source_priority=self._get_source_priority(source)
            )

        except Exception as e:
            self._logger.error(f"ë°ì´í„° ì •ê·œí™” ì‹¤íŒ¨: {e}")
            # ì—ëŸ¬ ì‹œì—ë„ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return self._create_fallback_data(raw_data, source, str(e))

    def _create_ticker_data(self, raw_data: Dict[str, Any], source: DataSource) -> TickerData:
        """ì†ŒìŠ¤ë³„ TickerData ìƒì„±"""
        if source == DataSource.REST:
            return self._create_from_rest(raw_data)
        elif source in [DataSource.WEBSOCKET, DataSource.WEBSOCKET_SIMPLE]:
            return self._create_from_websocket(raw_data, source)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ì†ŒìŠ¤: {source}")

    def _create_from_rest(self, data: Dict[str, Any]) -> TickerData:
        """REST API ë°ì´í„°ë¡œë¶€í„° TickerData ìƒì„±"""
        return TickerData(
            symbol=data["market"],
            current_price=Decimal(str(data["trade_price"])),
            change_rate=Decimal(str(data.get("signed_change_rate", 0))) * 100,
            change_amount=Decimal(str(data.get("signed_change_price", 0))),
            volume_24h=Decimal(str(data.get("acc_trade_volume_24h", 0))),
            high_24h=Decimal(str(data.get("high_price", 0))),
            low_24h=Decimal(str(data.get("low_price", 0))),
            prev_closing_price=Decimal(str(data.get("prev_closing_price", 0))),
            timestamp=datetime.now(),
            source=DataSource.REST.value
        )

    def _create_from_websocket(self, data: Dict[str, Any], source: DataSource) -> TickerData:
        """WebSocket ë°ì´í„°ë¡œë¶€í„° TickerData ìƒì„±"""
        if source == DataSource.WEBSOCKET_SIMPLE:
            # SIMPLE í¬ë§· (ì¶•ì•½ëœ í•„ë“œëª…)
            symbol_field = "cd"
            price_field = "tp"
            change_rate_field = "scr"
            change_amount_field = "scp"
            volume_field = "aav24"
            high_field = "hp"
            low_field = "lp"
        else:
            # DEFAULT í¬ë§· (ì „ì²´ í•„ë“œëª…)
            symbol_field = "code"
            price_field = "trade_price"
            change_rate_field = "signed_change_rate"
            change_amount_field = "signed_change_price"
            volume_field = "acc_trade_volume_24h"
            high_field = "high_price"
            low_field = "low_price"

        return TickerData(
            symbol=data[symbol_field],
            current_price=Decimal(str(data[price_field])),
            change_rate=Decimal(str(data.get(change_rate_field, 0))) * 100,
            change_amount=Decimal(str(data.get(change_amount_field, 0))),
            volume_24h=Decimal(str(data.get(volume_field, 0))),
            high_24h=Decimal(str(data.get(high_field, 0))),
            low_24h=Decimal(str(data.get(low_field, 0))),
            prev_closing_price=Decimal(str(data.get("prev_closing_price", 0))),
            timestamp=datetime.now(),
            source=source.value
        )

    def _apply_normalization(self, ticker_data: TickerData) -> TickerData:
        """ë°ì´í„° ì •ê·œí™” ì ìš©"""
        return TickerData(
            symbol=ticker_data.symbol,
            current_price=ticker_data.current_price.quantize(self._price_precision),
            change_rate=ticker_data.change_rate.quantize(self._rate_precision),
            change_amount=ticker_data.change_amount.quantize(self._price_precision),
            volume_24h=ticker_data.volume_24h.quantize(self._volume_precision),
            high_24h=ticker_data.high_24h.quantize(self._price_precision),
            low_24h=ticker_data.low_24h.quantize(self._price_precision),
            prev_closing_price=ticker_data.prev_closing_price.quantize(self._price_precision),
            timestamp=ticker_data.timestamp,
            source=ticker_data.source
        )

    def _validate_and_correct(self, ticker_data: TickerData) -> Tuple[List[str], List[str]]:
        """ë°ì´í„° ê²€ì¦ ë° ë³´ì •"""
        errors = []
        corrected_fields = []

        # ê°€ê²© ë²”ìœ„ ê²€ì¦
        if ticker_data.current_price < self._min_valid_price:
            errors.append(f"í˜„ì¬ê°€ê°€ ìµœì†Œê°’ë³´ë‹¤ ë‚®ìŒ: {ticker_data.current_price}")

        if ticker_data.current_price > self._max_valid_price:
            errors.append(f"í˜„ì¬ê°€ê°€ ìµœëŒ€ê°’ë³´ë‹¤ ë†’ìŒ: {ticker_data.current_price}")

        # ë³€í™”ìœ¨ ê²€ì¦
        if abs(ticker_data.change_rate) > self._max_price_change_rate:
            errors.append(f"ë³€í™”ìœ¨ì´ ì„ê³„ê°’ ì´ˆê³¼: {ticker_data.change_rate}%")

        # ê³ ì €ê°€ ë…¼ë¦¬ ê²€ì¦
        if ticker_data.high_24h < ticker_data.low_24h:
            errors.append("24ì‹œê°„ ê³ ê°€ê°€ ì €ê°€ë³´ë‹¤ ë‚®ìŒ")
            corrected_fields.append("high_24h")

        return errors, corrected_fields

    def _determine_quality(self, ticker_data: TickerData, validation_errors: List[str]) -> DataQuality:
        """ë°ì´í„° í’ˆì§ˆ ë“±ê¸‰ ê²°ì •"""
        if len(validation_errors) == 0:
            return DataQuality.HIGH
        elif len(validation_errors) <= 2:
            return DataQuality.MEDIUM
        elif len(validation_errors) <= 5:
            return DataQuality.LOW
        else:
            return DataQuality.INVALID

    def _calculate_confidence(self, ticker_data: TickerData, source: DataSource, quality: DataQuality) -> Decimal:
        """ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°"""
        base_confidence = {
            DataQuality.HIGH: Decimal('1.0'),
            DataQuality.MEDIUM: Decimal('0.8'),
            DataQuality.LOW: Decimal('0.5'),
            DataQuality.INVALID: Decimal('0.1')
        }[quality]

        # ì†ŒìŠ¤ë³„ ê°€ì¤‘ì¹˜
        source_weight = {
            DataSource.REST: Decimal('1.0'),
            DataSource.WEBSOCKET: Decimal('0.95'),
            DataSource.WEBSOCKET_SIMPLE: Decimal('0.9'),
            DataSource.CACHED: Decimal('0.85')
        }.get(source, Decimal('0.5'))

        return base_confidence * source_weight

    def _generate_checksum(self, ticker_data: TickerData) -> str:
        """ë°ì´í„° ì²´í¬ì„¬ ìƒì„±"""
        data_string = f"{ticker_data.symbol}:{ticker_data.current_price}:{ticker_data.timestamp}"
        return hashlib.md5(data_string.encode()).hexdigest()[:16]

    def _get_source_priority(self, source: DataSource) -> int:
        """ë°ì´í„° ì†ŒìŠ¤ ìš°ì„ ìˆœìœ„ ë°˜í™˜"""
        return {
            DataSource.REST: 1,
            DataSource.WEBSOCKET: 2,
            DataSource.WEBSOCKET_SIMPLE: 3,
            DataSource.CACHED: 4
        }.get(source, 10)

    def _create_fallback_data(self, raw_data: Dict[str, Any], source: DataSource, error_msg: str) -> NormalizedTickerData:
        """ì—ëŸ¬ ì‹œ í´ë°± ë°ì´í„° ìƒì„±"""
        # ìµœì†Œí•œì˜ TickerData ìƒì„±
        fallback_ticker = TickerData(
            symbol=self._extract_symbol_safe(raw_data, source),
            current_price=Decimal('0'),
            change_rate=Decimal('0'),
            change_amount=Decimal('0'),
            volume_24h=Decimal('0'),
            high_24h=Decimal('0'),
            low_24h=Decimal('0'),
            prev_closing_price=Decimal('0'),
            timestamp=datetime.now(),
            source=source.value
        )

        return NormalizedTickerData(
            ticker_data=fallback_ticker,
            data_quality=DataQuality.INVALID,
            confidence_score=Decimal('0'),
            normalization_timestamp=datetime.now(),
            data_checksum="error",
            validation_errors=(error_msg,),
            corrected_fields=(),
            processing_time_ms=Decimal('0'),
            data_source_priority=10
        )

    def _validate_required_fields(self, raw_data: Dict[str, Any], source: DataSource) -> None:
        """í•„ìˆ˜ í•„ë“œ ê²€ì¦ - ëˆ„ë½ ì‹œ ì˜ˆì™¸ ë°œìƒ"""
        required_fields = []

        if source == DataSource.REST:
            required_fields = ["market", "trade_price"]
        elif source == DataSource.WEBSOCKET:
            required_fields = ["code", "trade_price"]
        elif source == DataSource.WEBSOCKET_SIMPLE:
            required_fields = ["cd", "tp"]

        missing_fields = []
        for field in required_fields:
            if field not in raw_data:
                missing_fields.append(field)

        if missing_fields:
            error_msg = f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {', '.join(missing_fields)}"
            self._logger.error(error_msg)
            raise ValueError(error_msg)

    def _extract_symbol_safe(self, raw_data: Dict[str, Any], source: DataSource) -> str:
        """ì•ˆì „í•œ ì‹¬ë³¼ ì¶”ì¶œ"""
        try:
            if source == DataSource.REST:
                return raw_data.get("market", "UNKNOWN")
            elif source == DataSource.WEBSOCKET:
                return raw_data.get("code", "UNKNOWN")
            elif source == DataSource.WEBSOCKET_SIMPLE:
                return raw_data.get("cd", "UNKNOWN")
        except Exception:
            pass
        return "UNKNOWN"


class IntelligentCache:
    """ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ (Phase 1.3)"""

    def __init__(self, default_ttl: int = 60):
        self._logger = create_component_logger("IntelligentCache")
        self._cache: Dict[str, CacheEntry] = {}
        self._default_ttl = default_ttl

        # ìºì‹œ í†µê³„
        self._hit_count = 0
        self._miss_count = 0
        self._eviction_count = 0

    async def get(self, cache_key: str) -> Optional[NormalizedTickerData]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        entry = self._cache.get(cache_key)

        if entry is None:
            self._miss_count += 1
            return None

        # TTL ê²€ì¦
        if self._is_expired(entry):
            self._evict(cache_key)
            self._miss_count += 1
            return None

        # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
        entry.access_count += 1
        entry.last_access = datetime.now()

        self._hit_count += 1
        return entry.data

    async def set(self, cache_key: str, data: NormalizedTickerData, ttl: Optional[int] = None) -> None:
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        effective_ttl = ttl or self._default_ttl

        entry = CacheEntry(
            data=data,
            created_at=datetime.now(),
            access_count=1,
            last_access=datetime.now(),
            ttl_seconds=effective_ttl
        )

        self._cache[cache_key] = entry
        self._cleanup_if_needed()

    def _is_expired(self, entry: CacheEntry) -> bool:
        """ìºì‹œ ì—”íŠ¸ë¦¬ ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        age = (datetime.now() - entry.created_at).total_seconds()
        return age > entry.ttl_seconds

    def _evict(self, cache_key: str) -> None:
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        if cache_key in self._cache:
            del self._cache[cache_key]
            self._eviction_count += 1

    def _cleanup_if_needed(self) -> None:
        """ìºì‹œ í¬ê¸°ê°€ ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ë©´ ì •ë¦¬"""
        max_size = 1000  # ìµœëŒ€ 1000ê°œ ì—”íŠ¸ë¦¬

        if len(self._cache) > max_size:
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ ì—”íŠ¸ë¦¬ ì œê±°
            sorted_entries = sorted(
                self._cache.items(),
                key=lambda x: x[1].last_access
            )

            # 20% ì œê±°
            remove_count = max_size // 5
            for cache_key, _ in sorted_entries[:remove_count]:
                self._evict(cache_key)

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´ ë°˜í™˜"""
        total_requests = self._hit_count + self._miss_count
        hit_rate = (self._hit_count / total_requests * 100) if total_requests > 0 else 0

        return {
            "total_entries": len(self._cache),
            "hit_count": self._hit_count,
            "miss_count": self._miss_count,
            "hit_rate_percent": round(hit_rate, 2),
            "eviction_count": self._eviction_count
        }


class DataUnifier:
    """
    ê³ ê¸‰ ë°ì´í„° í†µí•© ë° ê´€ë¦¬ ì‹œìŠ¤í…œ (Phase 1.3)

    ê¸°ëŠ¥:
    âœ… REST/WebSocket ë°ì´í„° í†µí•©
    âœ… ë°ì´í„° ì •ê·œí™” ë° ê²€ì¦
    âœ… ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ
    âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
    âœ… ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
    """

    def __init__(self, cache_ttl: int = 60):
        """DataUnifier ì´ˆê¸°í™”"""
        self._logger = create_component_logger("DataUnifier")

        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._normalizer = DataNormalizer()
        self._cache = IntelligentCache(default_ttl=cache_ttl)

        # ì„±ëŠ¥ í†µê³„
        self._processing_stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "normalization_count": 0,
            "error_count": 0
        }

        # ë°ì´í„° ì¼ê´€ì„± ì¶”ì 
        self._consistency_tracker: Dict[str, List[NormalizedTickerData]] = defaultdict(list)

    async def unify_ticker_data(self, raw_data: Dict[str, Any], source: str, use_cache: bool = True) -> NormalizedTickerData:
        """
        í†µí•© í‹°ì»¤ ë°ì´í„° ì²˜ë¦¬ (Phase 1.3 ê³ ë„í™”)

        Args:
            raw_data: ì›ë³¸ API ì‘ë‹µ ë°ì´í„°
            source: ë°ì´í„° ì†ŒìŠ¤ ("rest", "websocket", "websocket_simple")
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            NormalizedTickerData: ì •ê·œí™”ëœ í†µí•© ë°ì´í„°

        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ ë˜ëŠ” ì˜ëª»ëœ ë°ì´í„°
        """
        self._processing_stats["total_requests"] += 1

        try:
            # 1. ìºì‹œ í™•ì¸
            cache_key = self._generate_cache_key(raw_data, source)

            if use_cache:
                cached_data = await self._cache.get(cache_key)
                if cached_data:
                    self._processing_stats["cache_hits"] += 1
                    self._logger.debug(f"ìºì‹œ íˆíŠ¸: {cache_key}")
                    return cached_data

            # 2. ë°ì´í„° ì†ŒìŠ¤ íƒ€ì… ê²°ì •
            data_source = self._determine_data_source(source)

            # 3. ë°ì´í„° ì •ê·œí™” ìˆ˜í–‰
            normalized_data = self._normalizer.normalize_ticker(raw_data, data_source)
            self._processing_stats["normalization_count"] += 1

            # 4. ì¼ê´€ì„± ê²€ì¦
            await self._verify_data_consistency(normalized_data)

            # 5. ìºì‹œì— ì €ì¥
            if use_cache:
                await self._cache.set(cache_key, normalized_data)

            self._logger.debug(f"ë°ì´í„° í†µí•© ì™„ë£Œ: {normalized_data.ticker_data.symbol} ({normalized_data.data_quality.value})")
            return normalized_data

        except Exception as e:
            self._processing_stats["error_count"] += 1
            self._logger.error(f"ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}")
            raise ValueError(f"ë°ì´í„° í†µí•© ì‹¤íŒ¨: {e}") from e

    async def unify_multiple_ticker_data(self, data_batch: List[Tuple[Dict[str, Any], str]]) -> List[NormalizedTickerData]:
        """
        ëŒ€ìš©ëŸ‰ ë°ì´í„° ë°°ì¹˜ ì²˜ë¦¬ (Phase 1.3)

        Args:
            data_batch: (raw_data, source) íŠœí”Œì˜ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[NormalizedTickerData]: ì •ê·œí™”ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        self._logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(data_batch)}ê°œ ë°ì´í„°")

        # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
        tasks = [
            self.unify_ticker_data(raw_data, source)
            for raw_data, source in data_batch
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ í•„í„°ë§ (ì—ëŸ¬ ì œì™¸)
        successful_results = [
            result for result in results
            if isinstance(result, NormalizedTickerData)
        ]

        error_count = len(results) - len(successful_results)
        if error_count > 0:
            self._logger.warning(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ {error_count}ê°œ ì—ëŸ¬ ë°œìƒ")

        self._logger.info(f"ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(successful_results)}/{len(data_batch)} ì„±ê³µ")
        return successful_results

    def get_processing_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ì •ë³´ ë°˜í™˜"""
        cache_stats = self._cache.get_cache_stats()

        total_requests = max(self._processing_stats["total_requests"], 1)

        return {
            "processing_stats": self._processing_stats.copy(),
            "cache_stats": cache_stats,
            "cache_hit_rate": (self._processing_stats["cache_hits"] / total_requests * 100),
            "error_rate": (self._processing_stats["error_count"] / total_requests * 100)
        }

    def _generate_cache_key(self, raw_data: Dict[str, Any], source: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì‹¬ë³¼ê³¼ ì†ŒìŠ¤ ê¸°ë°˜ í‚¤ ìƒì„±
        symbol = self._extract_symbol(raw_data, source)
        data_hash = hashlib.md5(json.dumps(raw_data, sort_keys=True).encode()).hexdigest()[:8]
        return f"{symbol}:{source}:{data_hash}"

    def _extract_symbol(self, raw_data: Dict[str, Any], source: str) -> str:
        """ë°ì´í„°ì—ì„œ ì‹¬ë³¼ ì¶”ì¶œ"""
        if source == "rest":
            return raw_data.get("market", "UNKNOWN")
        elif source == "websocket":
            return raw_data.get("code", "UNKNOWN")
        elif source == "websocket_simple":
            return raw_data.get("cd", "UNKNOWN")
        else:
            return "UNKNOWN"

    def _determine_data_source(self, source: str) -> DataSource:
        """ë¬¸ìì—´ ì†ŒìŠ¤ë¥¼ DataSource enumìœ¼ë¡œ ë³€í™˜"""
        source_mapping = {
            "rest": DataSource.REST,
            "websocket": DataSource.WEBSOCKET,
            "websocket_simple": DataSource.WEBSOCKET_SIMPLE
        }

        if source not in source_mapping:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° ì†ŒìŠ¤: {source}")

        return source_mapping[source]

    async def _verify_data_consistency(self, normalized_data: NormalizedTickerData) -> None:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì¦ ë° ì¶”ì """
        symbol = normalized_data.ticker_data.symbol

        # ì¶”ì  ë°ì´í„°ì— ì¶”ê°€
        self._consistency_tracker[symbol].append(normalized_data)

        # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ë³´ê´€ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
        if len(self._consistency_tracker[symbol]) > 100:
            self._consistency_tracker[symbol] = self._consistency_tracker[symbol][-50:]
