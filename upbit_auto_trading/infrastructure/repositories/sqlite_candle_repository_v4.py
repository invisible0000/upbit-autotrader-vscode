"""
CandleDataProvider v4.0 - Infrastructure Repository êµ¬í˜„ì²´

DDD Infrastructure Layerì—ì„œ CandleRepositoryInterfaceë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.
DatabaseManagerë¥¼ í™œìš©í•œ Connection Pooling + WAL ëª¨ë“œì™€
ê¸°ì¡´ ê°œë³„ í…Œì´ë¸” ìµœì í™”ë¥¼ ê²°í•©í•œ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ì…ë‹ˆë‹¤.

Architecture:
- DDD ì¤€ìˆ˜: Domain Interface ì™„ì „ êµ¬í˜„
- ì„±ëŠ¥ ìµœì í™”: DatabaseManager + Connection Pooling + WAL ëª¨ë“œ
- ì—…ë¹„íŠ¸ íŠ¹í™”: ì‹¬ë³¼ë³„ ê°œë³„ í…Œì´ë¸” + INSERT OR IGNORE ìµœì í™”
- 4ë‹¨ê³„ ìµœì í™”: ê²¹ì¹¨ ë¶„ì„ ì§€ì› ë©”ì„œë“œ êµ¬í˜„
"""

import sqlite3
import time
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any

from upbit_auto_trading.domain.repositories.candle_repository_interface import (
    CandleRepositoryInterface, CandleData, CandleQueryResult
)
from upbit_auto_trading.infrastructure.database.database_manager import DatabaseManager
from upbit_auto_trading.infrastructure.logging import create_component_logger

logger = create_component_logger("SqliteCandleRepositoryV4")


class TablePerformanceOptimizer:
    """ê°œë³„ í…Œì´ë¸” êµ¬ì¡° ìµœì í™” + DatabaseManager ê²°í•©"""

    def __init__(self):
        self._table_cache = {}
        logger.debug("TablePerformanceOptimizer ì´ˆê¸°í™”")

    def get_table_name(self, symbol: str, timeframe: str) -> str:
        """ì‹¬ë³¼ê³¼ íƒ€ì„í”„ë ˆì„ìœ¼ë¡œ í…Œì´ë¸”ëª… ìƒì„±"""
        return f"candles_{symbol.replace('-', '_')}_{timeframe}"

    def create_table_sql(self, table_name: str) -> str:
        """ê°œë³„ í…Œì´ë¸” ìƒì„± SQL (ì—…ë¹„íŠ¸ íŠ¹í™” ìµœì í™” - PRD ì¤€ìˆ˜)"""
        return f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            market TEXT NOT NULL,
            candle_date_time_utc DATETIME PRIMARY KEY,  -- ğŸ”‘ PRD ìš”êµ¬ì‚¬í•­: ì§ì ‘ PRIMARY KEY
            candle_date_time_kst DATETIME NOT NULL,
            opening_price DECIMAL(20,8) NOT NULL,
            high_price DECIMAL(20,8) NOT NULL,
            low_price DECIMAL(20,8) NOT NULL,
            trade_price DECIMAL(20,8) NOT NULL,
            timestamp BIGINT NOT NULL,
            candle_acc_trade_price DECIMAL(30,8) NOT NULL,
            candle_acc_trade_volume DECIMAL(30,8) NOT NULL,
            unit INTEGER DEFAULT 1,
            trade_count INTEGER DEFAULT 0,
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP
        );

        -- ì„±ëŠ¥ ìµœì í™” ì¸ë±ìŠ¤ (PRIMARY KEY ì™¸ ì¶”ê°€)
        CREATE INDEX IF NOT EXISTS idx_{table_name}_timestamp ON {table_name}(timestamp);
        CREATE INDEX IF NOT EXISTS idx_{table_name}_created_at ON {table_name}(created_at);
        CREATE INDEX IF NOT EXISTS idx_{table_name}_market ON {table_name}(market);
        """

    def batch_insert_with_ignore(self, conn: sqlite3.Connection, table_name: str, candles: List[CandleData]) -> int:
        """INSERT OR IGNORE + executemany ìµœì í™”"""
        if not candles:
            return 0

        sql = f"""
        INSERT OR IGNORE INTO {table_name} (
            market, candle_date_time_utc, candle_date_time_kst,
            opening_price, high_price, low_price, trade_price,
            timestamp, candle_acc_trade_price, candle_acc_trade_volume,
            unit, trade_count
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """

        candle_tuples = [
            (
                candle.market,
                candle.candle_date_time_utc.isoformat(),
                candle.candle_date_time_kst.isoformat(),
                candle.opening_price,
                candle.high_price,
                candle.low_price,
                candle.trade_price,
                candle.timestamp,
                candle.candle_acc_trade_price,
                candle.candle_acc_trade_volume,
                candle.unit,
                candle.trade_count
            )
            for candle in candles
        ]

        cursor = conn.executemany(sql, candle_tuples)
        return cursor.rowcount


class SqliteCandleRepository(CandleRepositoryInterface):
    """SQLite ê¸°ë°˜ ìº”ë“¤ ë°ì´í„° Repository v4.0 (DDD + ì„±ëŠ¥ ìµœì í™”)"""

    def __init__(self, database_manager: DatabaseManager):
        """
        Args:
            database_manager: DatabaseManager ì¸ìŠ¤í„´ìŠ¤ (ì˜ì¡´ì„± ì£¼ì…)
        """
        self._db_manager = database_manager
        self._table_optimizer = TablePerformanceOptimizer()
        logger.info("SqliteCandleRepository v4.0 ì´ˆê¸°í™” - DDD + ì„±ëŠ¥ ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ")

    # === í•µì‹¬ CRUD ë©”ì„œë“œ ===

    async def save_candles(self, symbol: str, timeframe: str, candles: List[CandleData]) -> int:
        """ìº”ë“¤ ë°ì´í„° ì €ì¥ (INSERT OR IGNORE ê¸°ë°˜ ì¤‘ë³µ ìë™ ì²˜ë¦¬)"""
        if not candles:
            return 0

        table_name = await self.ensure_table_exists(symbol, timeframe)

        start_time = time.time()

        # DatabaseManagerë¡œ Connection Pooling + WAL ëª¨ë“œ í™œìš©
        with self._db_manager.get_connection('market_data') as conn:
            inserted_count = self._table_optimizer.batch_insert_with_ignore(conn, table_name, candles)

        query_time_ms = (time.time() - start_time) * 1000

        logger.debug(f"ìº”ë“¤ ì €ì¥ ì™„ë£Œ: {symbol}_{timeframe}, ìš”ì²­={len(candles)}, ì‹¤ì œì‚½ì…={inserted_count}, ì‹œê°„={query_time_ms:.1f}ms")
        return inserted_count

    async def get_candles(self,
                          symbol: str,
                          timeframe: str,
                          start_time: Optional[datetime] = None,
                          end_time: Optional[datetime] = None,
                          count: Optional[int] = None,
                          order_desc: bool = True) -> CandleQueryResult:
        """ìº”ë“¤ ë°ì´í„° ì¡°íšŒ"""

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        if not await self.table_exists(symbol, timeframe):
            return CandleQueryResult(candles=[], total_count=0, query_time_ms=0.0)

        # ì¿¼ë¦¬ ì¡°ê±´ êµ¬ì„±
        where_conditions = []
        params = []

        if start_time:
            where_conditions.append("candle_date_time_utc >= ?")
            params.append(start_time.isoformat())

        if end_time:
            where_conditions.append("candle_date_time_utc <= ?")
            params.append(end_time.isoformat())

        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        order_clause = "candle_date_time_utc DESC" if order_desc else "candle_date_time_utc ASC"
        limit_clause = f"LIMIT {count}" if count else ""

        sql = f"""
        SELECT
            market, candle_date_time_utc, candle_date_time_kst,
            opening_price, high_price, low_price, trade_price,
            timestamp, candle_acc_trade_price, candle_acc_trade_volume,
            unit, trade_count
        FROM {table_name}
        WHERE {where_clause}
        ORDER BY {order_clause}
        {limit_clause}
        """

        start_query_time = time.time()

        with self._db_manager.get_connection('market_data') as conn:
            cursor = conn.execute(sql, params)
            rows = cursor.fetchall()

            # ì´ ê°œìˆ˜ ì¡°íšŒ (í˜ì´ì§• ì§€ì›)
            count_sql = f"SELECT COUNT(*) FROM {table_name} WHERE {where_clause}"
            total_count = conn.execute(count_sql, params).fetchone()[0]

        query_time_ms = (time.time() - start_query_time) * 1000

        # CandleData ê°ì²´ë¡œ ë³€í™˜
        candles = []
        for row in rows:
            candle = CandleData(
                market=row[0],
                candle_date_time_utc=datetime.fromisoformat(row[1]),
                candle_date_time_kst=datetime.fromisoformat(row[2]),
                opening_price=float(row[3]),
                high_price=float(row[4]),
                low_price=float(row[5]),
                trade_price=float(row[6]),
                timestamp=int(row[7]),
                candle_acc_trade_price=float(row[8]),
                candle_acc_trade_volume=float(row[9]),
                unit=int(row[10]),
                trade_count=int(row[11])
            )
            candles.append(candle)

        logger.debug(f"ìº”ë“¤ ì¡°íšŒ ì™„ë£Œ: {symbol}_{timeframe}, ê²°ê³¼={len(candles)}, ì´ê°œìˆ˜={total_count}, ì‹œê°„={query_time_ms:.1f}ms")

        return CandleQueryResult(
            candles=candles,
            total_count=total_count,
            query_time_ms=query_time_ms,
            cache_hit=False
        )

    async def get_latest_candle(self, symbol: str, timeframe: str) -> Optional[CandleData]:
        """ìµœì‹  ìº”ë“¤ ë°ì´í„° ì¡°íšŒ"""
        result = await self.get_candles(symbol, timeframe, count=1, order_desc=True)
        return result.candles[0] if result.candles else None

    async def count_candles(self,
                            symbol: str,
                            timeframe: str,
                            start_time: Optional[datetime] = None,
                            end_time: Optional[datetime] = None) -> int:
        """ìº”ë“¤ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ"""

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        if not await self.table_exists(symbol, timeframe):
            return 0

        where_conditions = []
        params = []

        if start_time:
            where_conditions.append("candle_date_time_utc >= ?")
            params.append(start_time.isoformat())

        if end_time:
            where_conditions.append("candle_date_time_utc <= ?")
            params.append(end_time.isoformat())

        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
        sql = f"SELECT COUNT(*) FROM {table_name} WHERE {where_clause}"

        with self._db_manager.get_connection('market_data') as conn:
            count = conn.execute(sql, params).fetchone()[0]

        return count

    # === í…Œì´ë¸” ê´€ë¦¬ ë©”ì„œë“œ ===

    async def ensure_table_exists(self, symbol: str, timeframe: str) -> str:
        """ìº”ë“¤ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        # ìºì‹œ í™•ì¸
        if table_name in self._table_optimizer._table_cache:
            return table_name

        with self._db_manager.get_connection('market_data') as conn:
            # í…Œì´ë¸” ìƒì„± (ì‹¬ë³¼ë³„ ê°œë³„ í…Œì´ë¸” ìµœì í™”)
            create_sql = self._table_optimizer.create_table_sql(table_name)
            conn.executescript(create_sql)

            # ìºì‹œì— ì¶”ê°€
            self._table_optimizer._table_cache[table_name] = True

        logger.debug(f"í…Œì´ë¸” ìƒì„±/í™•ì¸ ì™„ë£Œ: {table_name}")
        return table_name

    async def table_exists(self, symbol: str, timeframe: str) -> bool:
        """ìº”ë“¤ í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        # ìºì‹œ í™•ì¸
        if table_name in self._table_optimizer._table_cache:
            return True

        with self._db_manager.get_connection('market_data') as conn:
            cursor = conn.execute("""
                SELECT name FROM sqlite_master
                WHERE type='table' AND name=?
            """, (table_name,))

            exists = cursor.fetchone() is not None

            if exists:
                self._table_optimizer._table_cache[table_name] = True

            return exists

    async def get_table_stats(self, symbol: str, timeframe: str) -> Optional[Dict[str, Any]]:
        """íŠ¹ì • ìº”ë“¤ í…Œì´ë¸” í†µê³„ ì¡°íšŒ"""

        if not await self.table_exists(symbol, timeframe):
            return None

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        with self._db_manager.get_connection('market_data') as conn:
            # ê¸°ë³¸ í†µê³„
            stats_sql = f"""
            SELECT
                COUNT(*) as record_count,
                MIN(candle_date_time_utc) as earliest_time,
                MAX(candle_date_time_utc) as latest_time,
                MIN(timestamp) as earliest_timestamp,
                MAX(timestamp) as latest_timestamp
            FROM {table_name}
            """

            cursor = conn.execute(stats_sql)
            row = cursor.fetchone()

            if not row or row[0] == 0:
                return {
                    "table_name": table_name,
                    "record_count": 0,
                    "earliest_time": None,
                    "latest_time": None,
                    "size_mb": 0.0
                }

            # í…Œì´ë¸” í¬ê¸° ì¡°íšŒ (ê·¼ì‚¬ì¹˜)
            size_sql = f"SELECT page_count * page_size as size FROM pragma_page_count('{table_name}'), pragma_page_size"
            try:
                size_bytes = conn.execute(size_sql).fetchone()[0] or 0
                size_mb = size_bytes / (1024 * 1024)
            except sqlite3.Error:
                size_mb = 0.0

            return {
                "table_name": table_name,
                "symbol": symbol,
                "timeframe": timeframe,
                "record_count": row[0],
                "earliest_time": row[1],
                "latest_time": row[2],
                "earliest_timestamp": row[3],
                "latest_timestamp": row[4],
                "size_mb": round(size_mb, 2)
            }

    async def get_all_candle_tables(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ìº”ë“¤ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ"""

        with self._db_manager.get_connection('market_data') as conn:
            cursor = conn.execute("""
                SELECT name FROM sqlite_master
                WHERE type='table' AND name LIKE 'candles_%'
                ORDER BY name
            """)

            table_names = [row[0] for row in cursor.fetchall()]

        # ê° í…Œì´ë¸”ì˜ í†µê³„ ì •ë³´ ìˆ˜ì§‘
        table_infos = []
        for table_name in table_names:
            try:
                # í…Œì´ë¸”ëª…ì—ì„œ ì‹¬ë³¼ê³¼ íƒ€ì„í”„ë ˆì„ ì¶”ì¶œ
                parts = table_name.replace('candles_', '').split('_')
                if len(parts) >= 3:  # KRW_BTC_1m í˜•íƒœ
                    symbol = f"{parts[0]}-{parts[1]}"
                    timeframe = "_".join(parts[2:])

                    stats = await self.get_table_stats(symbol, timeframe)
                    if stats:
                        table_infos.append(stats)

            except Exception as e:
                logger.warning(f"í…Œì´ë¸” {table_name} í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                continue

        return table_infos

    # === 4ë‹¨ê³„ ìµœì í™” ì§€ì› ë©”ì„œë“œ ===

    async def check_complete_overlap(self,
                                     symbol: str,
                                     timeframe: str,
                                     start_time: datetime,
                                     count: int) -> bool:
        """ì™„ì „ ê²¹ì¹¨ í™•ì¸ (4ë‹¨ê³„ ìµœì í™” - Step 2)"""

        if not await self.table_exists(symbol, timeframe):
            return False

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        # íƒ€ì„í”„ë ˆì„ ê°„ê²©ì„ ë¶„ìœ¼ë¡œ ê³„ì‚° (ê°„ë‹¨í•œ ë§¤í•‘)
        timeframe_minutes = self._parse_timeframe_to_minutes(timeframe)
        if timeframe_minutes is None:
            return False

        end_time = start_time + timedelta(minutes=timeframe_minutes * (count - 1))

        sql = f"""
        SELECT COUNT(*) FROM {table_name}
        WHERE candle_date_time_utc BETWEEN ? AND ?
        """

        with self._db_manager.get_connection('market_data') as conn:
            db_count = conn.execute(sql, (start_time.isoformat(), end_time.isoformat())).fetchone()[0]

        # ì™„ì „ ì¼ì¹˜ = DB ê°œìˆ˜ì™€ ìš”ì²­ ê°œìˆ˜ ë™ì¼
        is_complete_overlap = db_count == count

        logger.debug(f"ì™„ì „ ê²¹ì¹¨ í™•ì¸: {symbol}_{timeframe}, DBê°œìˆ˜={db_count}, ìš”ì²­ê°œìˆ˜={count}, ì™„ì „ê²¹ì¹¨={is_complete_overlap}")
        return is_complete_overlap

    async def check_fragmentation(self,
                                  symbol: str,
                                  timeframe: str,
                                  start_time: datetime,
                                  count: int,
                                  gap_threshold_seconds: int) -> int:
        """íŒŒí¸í™” ê²¹ì¹¨ í™•ì¸ (4ë‹¨ê³„ ìµœì í™” - Step 3)"""

        if not await self.table_exists(symbol, timeframe):
            return 0

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        timeframe_minutes = self._parse_timeframe_to_minutes(timeframe)
        if timeframe_minutes is None:
            return 0

        end_time = start_time + timedelta(minutes=timeframe_minutes * (count - 1))

        # SQLite LAG ìœˆë„ìš° í•¨ìˆ˜ í™œìš©
        sql = f"""
        WITH time_gaps AS (
            SELECT
                candle_date_time_utc,
                LAG(candle_date_time_utc) OVER (ORDER BY candle_date_time_utc) as prev_time
            FROM {table_name}
            WHERE candle_date_time_utc BETWEEN ? AND ?
            ORDER BY candle_date_time_utc
        )
        SELECT COUNT(*) as gap_count
        FROM time_gaps
        WHERE (strftime('%s', candle_date_time_utc) - strftime('%s', prev_time)) > ?
        """

        with self._db_manager.get_connection('market_data') as conn:
            gap_count = conn.execute(sql, (
                start_time.isoformat(),
                end_time.isoformat(),
                gap_threshold_seconds
            )).fetchone()[0]

        logger.debug(f"íŒŒí¸í™” í™•ì¸: {symbol}_{timeframe}, ê°„ê²©ê°œìˆ˜={gap_count}, ì„ê³„ê°’={gap_threshold_seconds}ì´ˆ")
        return gap_count

    async def find_connected_end(self,
                                 symbol: str,
                                 timeframe: str,
                                 start_time: datetime,
                                 max_count: int = 200) -> Optional[datetime]:
        """ì—°ê²°ëœ ë ì°¾ê¸° (4ë‹¨ê³„ ìµœì í™” - Step 4)"""

        if not await self.table_exists(symbol, timeframe):
            return None

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        timeframe_minutes = self._parse_timeframe_to_minutes(timeframe)
        if timeframe_minutes is None:
            return None

        timeframe_seconds = timeframe_minutes * 60

        # SQLite ROW_NUMBER + datetime í•¨ìˆ˜ í™œìš©
        sql = f"""
        WITH consecutive_candles AS (
            SELECT
                candle_date_time_utc,
                ROW_NUMBER() OVER (ORDER BY candle_date_time_utc) as row_num,
                datetime(candle_date_time_utc,
                         '-' || ((ROW_NUMBER() OVER (ORDER BY candle_date_time_utc) - 1) * {timeframe_seconds}) || ' seconds'
                ) as expected_start
            FROM {table_name}
            WHERE candle_date_time_utc >= ?
            ORDER BY candle_date_time_utc
            LIMIT ?
        )
        SELECT MAX(candle_date_time_utc) as connected_end
        FROM consecutive_candles
        WHERE expected_start = ?
        """

        with self._db_manager.get_connection('market_data') as conn:
            result = conn.execute(sql, (
                start_time.isoformat(),
                max_count,
                start_time.isoformat()
            )).fetchone()

        if result and result[0]:
            connected_end = datetime.fromisoformat(result[0])
            logger.debug(f"ì—°ê²°ëœ ë ì°¾ê¸°: {symbol}_{timeframe}, ì‹œì‘={start_time}, ë={connected_end}")
            return connected_end

        return None

    # === ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë©”ì„œë“œ ===

    async def get_performance_metrics(self, symbol: str, timeframe: str) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œ ì¡°íšŒ"""

        stats = await self.get_table_stats(symbol, timeframe)
        if not stats:
            return {
                "avg_query_time_ms": 0.0,
                "cache_hit_rate": 0.0,
                "table_size_mb": 0.0,
                "fragmentation_rate": 0.0,
                "last_update_time": None
            }

        # ê°„ë‹¨í•œ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        return {
            "avg_query_time_ms": 10.0,  # ì˜ˆì‹œ ê°’ (ì‹¤ì œë¡œëŠ” ë¡œê¹…ìœ¼ë¡œ ìˆ˜ì§‘)
            "cache_hit_rate": 0.85,     # ì˜ˆì‹œ ê°’ (ìºì‹œ ì‹œìŠ¤í…œê³¼ ì—°ë™)
            "table_size_mb": stats["size_mb"],
            "fragmentation_rate": 0.05,  # ì˜ˆì‹œ ê°’ (íŒŒí¸í™” ë¹„ìœ¨)
            "last_update_time": stats["latest_time"]
        }

    # === ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ë©”ì„œë“œ ===

    async def validate_data_integrity(self, symbol: str, timeframe: str) -> Dict[str, Any]:
        """ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦"""

        if not await self.table_exists(symbol, timeframe):
            return {
                "duplicate_count": 0,
                "missing_count": 0,
                "time_consistency": 1.0,
                "data_coverage_rate": 0.0
            }

        table_name = self._table_optimizer.get_table_name(symbol, timeframe)

        with self._db_manager.get_connection('market_data') as conn:
            # ì¤‘ë³µ ê²€ì‚¬ (UNIQUE ì œì•½ìœ¼ë¡œ ì¸í•´ 0ì´ì–´ì•¼ í•¨)
            duplicate_sql = f"""
            SELECT candle_date_time_utc, COUNT(*) as cnt
            FROM {table_name}
            GROUP BY candle_date_time_utc
            HAVING COUNT(*) > 1
            """
            duplicate_count = len(conn.execute(duplicate_sql).fetchall())

            # ê¸°ë³¸ í†µê³„
            total_count = await self.count_candles(symbol, timeframe)

        return {
            "duplicate_count": duplicate_count,
            "missing_count": 0,  # ì‹¤ì œë¡œëŠ” ì—°ì†ì„± ê²€ì‚¬ í•„ìš”
            "time_consistency": 1.0 if duplicate_count == 0 else 0.9,
            "data_coverage_rate": 1.0 if total_count > 0 else 0.0
        }

    # === ìœ í‹¸ë¦¬í‹° ë©”ì„œë“œ ===

    def _parse_timeframe_to_minutes(self, timeframe: str) -> Optional[int]:
        """íƒ€ì„í”„ë ˆì„ì„ ë¶„ ë‹¨ìœ„ë¡œ ë³€í™˜"""
        timeframe_map = {
            "1m": 1, "3m": 3, "5m": 5, "15m": 15, "30m": 30,
            "1h": 60, "2h": 120, "4h": 240, "6h": 360, "8h": 480, "12h": 720,
            "1d": 1440, "3d": 4320, "1w": 10080, "1M": 43200
        }
        return timeframe_map.get(timeframe)
