#!/usr/bin/env python3
"""
ğŸš€ Advanced Data Info to DB Migration Manager
=============================================

data_info í´ë”ì˜ ëª¨ë“  variables_* YAML íŒŒì¼ë“¤ì„ 
í™•ì¥ëœ DB ìŠ¤í‚¤ë§ˆ(v3.0)ë¡œ ì™„ì „ ë§ˆì´ê·¸ë ˆì´ì…˜í•˜ëŠ” ê³ ê¸‰ ê´€ë¦¬ì

ì£¼ìš” ê¸°ëŠ¥:
- variables_help_texts.yaml â†’ tv_help_texts í…Œì´ë¸”
- variables_placeholder_texts.yaml â†’ tv_placeholder_texts í…Œì´ë¸”  
- variables_indicator_categories.yaml â†’ tv_indicator_categories í…Œì´ë¸”
- variables_parameter_types.yaml â†’ tv_parameter_types í…Œì´ë¸”
- variables_indicator_library.yaml â†’ tv_indicator_library í…Œì´ë¸”
- variables_workflow_guide.yaml â†’ tv_workflow_guides í…Œì´ë¸”

ì‘ì„±ì¼: 2025-07-30
ì‘ì„±ì: GitHub Copilot
"""

import os
import sqlite3
import yaml
import json
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path


class AdvancedDataInfoMigrator:
    """data_info â†’ í™•ì¥ DB ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ë¦¬ì"""
    
    def __init__(self, db_path: str, data_info_path: str = None):
        """
        ì´ˆê¸°í™”
        
        Args:
            db_path: ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
            data_info_path: data_info í´ë” ê²½ë¡œ (Noneì´ë©´ ìë™ ê°ì§€)
        """
        self.db_path = db_path
        self.data_info_path = data_info_path or self._get_default_data_info_path()
        self.migration_log = []
        
    def _get_default_data_info_path(self) -> str:
        """ê¸°ë³¸ data_info í´ë” ê²½ë¡œ ë°˜í™˜"""
        current_dir = Path(__file__).parent
        return str(current_dir.parent / "data_info")
    
    def _log(self, message: str, level: str = "INFO"):
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ë¡œê·¸ ê¸°ë¡"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {level}: {message}"
        self.migration_log.append(log_entry)
        print(log_entry)
    
    def _get_db_connection(self) -> sqlite3.Connection:
        """DB ì—°ê²° ë°˜í™˜"""
        conn = sqlite3.connect(self.db_path)
        conn.row_factory = sqlite3.Row
        return conn
    
    def _load_yaml_file(self, filename: str) -> Dict[str, Any]:
        """YAML íŒŒì¼ ë¡œë“œ"""
        file_path = Path(self.data_info_path) / filename
        if not file_path.exists():
            self._log(f"YAML íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}", "WARNING")
            return {}
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
                self._log(f"YAML íŒŒì¼ ë¡œë“œ ì„±ê³µ: {filename}")
                return data or {}
        except Exception as e:
            self._log(f"YAML íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {filename}: {str(e)}", "ERROR")
            return {}
    
    def check_schema_version(self) -> Tuple[bool, str]:
        """ìŠ¤í‚¤ë§ˆ ë²„ì „ í™•ì¸"""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT version FROM tv_schema_version ORDER BY applied_at DESC LIMIT 1")
                result = cursor.fetchone()
                
                if result:
                    version = result['version']
                    is_v3_compatible = version >= '3.0.0'
                    return is_v3_compatible, version
                else:
                    return False, "Unknown"
        except Exception as e:
            self._log(f"ìŠ¤í‚¤ë§ˆ ë²„ì „ í™•ì¸ ì‹¤íŒ¨: {str(e)}", "ERROR")
            return False, "Error"
    
    def setup_extended_schema(self) -> bool:
        """í™•ì¥ ìŠ¤í‚¤ë§ˆ ì„¤ì • (v3.0)"""
        schema_file = Path(self.data_info_path) / "schema_extended_v3.sql"
        
        if not schema_file.exists():
            self._log(f"í™•ì¥ ìŠ¤í‚¤ë§ˆ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {schema_file}", "ERROR")
            return False
        
        try:
            with open(schema_file, 'r', encoding='utf-8') as f:
                schema_sql = f.read()
            
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                cursor.executescript(schema_sql)
                conn.commit()
                
            self._log("í™•ì¥ ìŠ¤í‚¤ë§ˆ v3.0 ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            self._log(f"í™•ì¥ ìŠ¤í‚¤ë§ˆ ì„¤ì • ì‹¤íŒ¨: {str(e)}", "ERROR")
            return False
    
    def migrate_help_texts(self) -> bool:
        """variables_help_texts.yaml â†’ tv_help_texts ë§ˆì´ê·¸ë ˆì´ì…˜"""
        self._log("ğŸ“ ë„ì›€ë§ í…ìŠ¤íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        data = self._load_yaml_file("variables_help_texts.yaml")
        if not data:
            return False
        
        help_texts = data.get('help_texts', {})
        if not help_texts:
            self._log("ë„ì›€ë§ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", "WARNING")
            return True
        
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì¬ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ë¹„)
                cursor.execute("DELETE FROM tv_help_texts WHERE help_key LIKE 'yaml_%'")
                
                # ë„ì›€ë§ í…ìŠ¤íŠ¸ ì‚½ì…
                for help_key, help_text in help_texts.items():
                    # ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ (í‚¤ ì´ë¦„ì—ì„œ)
                    category = help_key.split('_')[0] if '_' in help_key else 'general'
                    
                    cursor.execute("""
                        INSERT INTO tv_help_texts (
                            help_key, help_text, help_category, usage_context, 
                            language_code, created_at, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        f"yaml_{help_key}",  # ì ‘ë‘ì‚¬ë¡œ YAML ê¸°ì› í‘œì‹œ
                        help_text,
                        category,
                        'general',
                        'ko',
                        datetime.now(),
                        datetime.now()
                    ))
                
                conn.commit()
                count = len(help_texts)
                self._log(f"ë„ì›€ë§ í…ìŠ¤íŠ¸ {count}ê°œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._log(f"ë„ì›€ë§ í…ìŠ¤íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}", "ERROR")
            return False
    
    def migrate_placeholder_texts(self) -> bool:
        """variables_placeholder_texts.yaml â†’ tv_placeholder_texts ë§ˆì´ê·¸ë ˆì´ì…˜"""
        self._log("ğŸ¯ í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        data = self._load_yaml_file("variables_placeholder_texts.yaml")
        if not data:
            return False
        
        placeholder_library = data.get('placeholder_library', {})
        if not placeholder_library:
            self._log("í”Œë ˆì´ìŠ¤í™€ë” ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", "WARNING")
            return True
        
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                cursor.execute("DELETE FROM tv_placeholder_texts")
                
                # í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ì‚½ì…
                for variable_id, placeholder_data in placeholder_library.items():
                    # ê¸°ë³¸ í”Œë ˆì´ìŠ¤í™€ë”ë“¤ ì‚½ì…
                    for placeholder_type in ['target', 'name', 'description']:
                        if placeholder_type in placeholder_data:
                            cursor.execute("""
                                INSERT INTO tv_placeholder_texts (
                                    variable_id, placeholder_type, placeholder_text,
                                    scenario_order, is_primary, language_code, created_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (
                                variable_id,
                                placeholder_type,
                                placeholder_data[placeholder_type],
                                0,
                                1,  # ê¸°ë³¸ í”Œë ˆì´ìŠ¤í™€ë”ëŠ” primary
                                'ko',
                                datetime.now()
                            ))
                    
                    # ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ë“¤ ì‚½ì…
                    usage_scenarios = placeholder_data.get('usage_scenarios', [])
                    for i, scenario in enumerate(usage_scenarios):
                        cursor.execute("""
                            INSERT INTO tv_placeholder_texts (
                                variable_id, placeholder_type, placeholder_text,
                                scenario_order, is_primary, language_code, created_at
                            ) VALUES (?, ?, ?, ?, ?, ?, ?)
                        """, (
                            variable_id,
                            'scenario',
                            scenario,
                            i + 1,
                            0,  # ì‹œë‚˜ë¦¬ì˜¤ëŠ” primary ì•„ë‹˜
                            'ko',
                            datetime.now()
                        ))
                
                conn.commit()
                total_count = sum(len(v.get('usage_scenarios', [])) + 3 for v in placeholder_library.values())
                self._log(f"í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ {total_count}ê°œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._log(f"í”Œë ˆì´ìŠ¤í™€ë” í…ìŠ¤íŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}", "ERROR")
            return False
    
    def migrate_indicator_library(self) -> bool:
        """variables_indicator_library.yaml â†’ tv_indicator_library ë§ˆì´ê·¸ë ˆì´ì…˜"""
        self._log("ğŸ“š ì§€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        data = self._load_yaml_file("variables_indicator_library.yaml")
        if not data:
            return False
        
        indicators = data.get('indicators', {})
        if not indicators:
            self._log("ì§€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤", "WARNING")
            return True
        
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                cursor.execute("DELETE FROM tv_indicator_library")
                
                # ì§€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì •ë³´ ì‚½ì…
                for variable_id, indicator_info in indicators.items():
                    # ê° ì½˜í…ì¸  íƒ€ì…ë³„ë¡œ ì‚½ì…
                    content_order = 0
                    
                    for content_type in ['definition', 'calculation', 'interpretation', 'usage_tip']:
                        if content_type in indicator_info:
                            content_order += 1
                            cursor.execute("""
                                INSERT INTO tv_indicator_library (
                                    variable_id, content_type, content_ko, content_order,
                                    reference_links, examples, created_at
                                ) VALUES (?, ?, ?, ?, ?, ?, ?)
                            """, (
                                variable_id,
                                content_type,
                                indicator_info[content_type],
                                content_order,
                                json.dumps(indicator_info.get('reference_links', [])),
                                json.dumps(indicator_info.get('examples', [])),
                                datetime.now()
                            ))
                
                conn.commit()
                self._log(f"ì§€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬ {len(indicators)}ê°œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._log(f"ì§€í‘œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}", "ERROR")
            return False
    
    def migrate_workflow_guides(self) -> bool:
        """variables_workflow_guide.yaml â†’ tv_workflow_guides ë§ˆì´ê·¸ë ˆì´ì…˜"""
        self._log("ğŸ“‹ ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘...")
        
        data = self._load_yaml_file("variables_workflow_guide.yaml")
        if not data:
            return False
        
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
                cursor.execute("DELETE FROM tv_workflow_guides")
                
                # ì›Œí¬í”Œë¡œìš° ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë° ì‚½ì…
                workflow_sections = data.get('workflow', {})
                if workflow_sections:
                    order = 0
                    for section_key, section_data in workflow_sections.items():
                        order += 1
                        cursor.execute("""
                            INSERT INTO tv_workflow_guides (
                                guide_type, guide_title_ko, guide_content,
                                display_order, target_audience, importance_level,
                                is_active, created_at
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            'section',
                            section_key.replace('_', ' ').title(),
                            json.dumps(section_data, ensure_ascii=False),
                            order,
                            'both',
                            3,  # ì¤‘ê°„ ì¤‘ìš”ë„
                            1,
                            datetime.now()
                        ))
                
                # ê¸°íƒ€ ê°€ì´ë“œ ì„¹ì…˜ë“¤ë„ ì¶”ê°€
                for guide_type in ['principles', 'checklist', 'troubleshooting']:
                    if guide_type in data:
                        order += 1
                        cursor.execute("""
                            INSERT INTO tv_workflow_guides (
                                guide_type, guide_title_ko, guide_content,
                                display_order, target_audience, importance_level,
                                is_active, created_at
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                        """, (
                            guide_type,
                            guide_type.replace('_', ' ').title(),
                            json.dumps(data[guide_type], ensure_ascii=False),
                            order,
                            'both',
                            4,  # ë†’ì€ ì¤‘ìš”ë„
                            1,
                            datetime.now()
                        ))
                
                conn.commit()
                self._log(f"ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
                return True
                
        except Exception as e:
            self._log(f"ì›Œí¬í”Œë¡œìš° ê°€ì´ë“œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}", "ERROR")
            return False
    
    def run_full_migration(self) -> Dict[str, Any]:
        """ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"""
        self._log("ğŸš€ data_info â†’ DB ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
        
        results = {
            'success': True,
            'schema_setup': False,
            'migrations': {},
            'error_count': 0,
            'log': []
        }
        
        # 1. ìŠ¤í‚¤ë§ˆ ë²„ì „ í™•ì¸ ë° ì„¤ì •
        is_compatible, current_version = self.check_schema_version()
        self._log(f"í˜„ì¬ ìŠ¤í‚¤ë§ˆ ë²„ì „: {current_version}")
        
        if not is_compatible:
            self._log("v3.0 í˜¸í™˜ ìŠ¤í‚¤ë§ˆ ì„¤ì • ì¤‘...")
            if self.setup_extended_schema():
                results['schema_setup'] = True
            else:
                results['success'] = False
                results['error_count'] += 1
                self._log("ìŠ¤í‚¤ë§ˆ ì„¤ì • ì‹¤íŒ¨ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ë‹¨", "ERROR")
                results['log'] = self.migration_log
                return results
        
        # 2. ê° ì»´í¬ë„ŒíŠ¸ ë§ˆì´ê·¸ë ˆì´ì…˜
        migration_tasks = [
            ('help_texts', self.migrate_help_texts),
            ('placeholder_texts', self.migrate_placeholder_texts),
            ('indicator_library', self.migrate_indicator_library),
            ('workflow_guides', self.migrate_workflow_guides),
        ]
        
        for task_name, task_func in migration_tasks:
            self._log(f"âš™ï¸ {task_name} ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰ ì¤‘...")
            try:
                success = task_func()
                results['migrations'][task_name] = success
                if not success:
                    results['error_count'] += 1
            except Exception as e:
                self._log(f"{task_name} ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜ˆì™¸ ë°œìƒ: {str(e)}", "ERROR")
                results['migrations'][task_name] = False
                results['error_count'] += 1
        
        # 3. ìµœì¢… ê²°ê³¼ í‰ê°€
        if results['error_count'] > 0:
            results['success'] = False
            self._log(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ (ì˜¤ë¥˜ {results['error_count']}ê°œ)", "ERROR")
        else:
            self._log("âœ… ëª¨ë“  ë§ˆì´ê·¸ë ˆì´ì…˜ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        
        results['log'] = self.migration_log
        return results
    
    def get_migration_summary(self) -> Dict[str, Any]:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ìš”ì•½"""
        try:
            with self._get_db_connection() as conn:
                cursor = conn.cursor()
                
                summary = {}
                
                # ê° í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ
                tables = [
                    'tv_help_texts',
                    'tv_placeholder_texts', 
                    'tv_indicator_library',
                    'tv_workflow_guides'
                ]
                
                for table in tables:
                    cursor.execute(f"SELECT COUNT(*) as count FROM {table}")
                    result = cursor.fetchone()
                    summary[table] = result['count'] if result else 0
                
                return summary
                
        except Exception as e:
            self._log(f"ìš”ì•½ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}", "ERROR")
            return {}


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    current_dir = Path(__file__).parent
    db_path = current_dir / "test_migration.db"
    
    # ë§ˆì´ê·¸ë ˆì´í„° ìƒì„± ë° ì‹¤í–‰
    migrator = AdvancedDataInfoMigrator(str(db_path))
    
    print("ğŸš€ Advanced Data Info Migration í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)
    
    # ì „ì²´ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    results = migrator.run_full_migration()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼:")
    print(f"âœ… ì„±ê³µ ì—¬ë¶€: {results['success']}")
    print(f"ğŸ”§ ìŠ¤í‚¤ë§ˆ ì„¤ì •: {results['schema_setup']}")
    print(f"âŒ ì˜¤ë¥˜ ê°œìˆ˜: {results['error_count']}")
    
    print("\nğŸ—‚ï¸ ê°œë³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼:")
    for task, success in results['migrations'].items():
        status = "âœ…" if success else "âŒ"
        print(f"  {status} {task}")
    
    # ìš”ì•½ ì •ë³´ ì¶œë ¥
    summary = migrator.get_migration_summary()
    if summary:
        print("\nğŸ“‹ í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜:")
        for table, count in summary.items():
            print(f"  ğŸ“„ {table}: {count}ê°œ")
    
    print("\nğŸ”— í…ŒìŠ¤íŠ¸ DB ê²½ë¡œ:", db_path)


if __name__ == "__main__":
    main()
