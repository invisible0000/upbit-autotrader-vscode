# ğŸ“‹ TASK-20250802-18: ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

## ğŸ¯ **ì‘ì—… ê°œìš”**
ë¦¬íŒ©í† ë§ëœ ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë“¤ì˜ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤ì‹œí•˜ê³ , ê¸°ì¡´ trigger_builder_screen.pyì™€ 100% í˜¸í™˜ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

## ğŸ“Š **í˜„ì¬ ìƒí™©**

### **ë¦¬íŒ©í† ë§ëœ ì»´í¬ë„ŒíŠ¸ë“¤**
```python
# ê¸°ì¡´ 1642ë¼ì¸ â†’ ë¶„ë¦¬ëœ êµ¬ì¡°
â”œâ”€â”€ trigger_builder_screen.py (UIë§Œ ë‚¨ê¹€, ~400ë¼ì¸ ì˜ˆìƒ)
â”œâ”€â”€ business_logic/triggers/
â”‚   â”œâ”€â”€ engines/
â”‚   â”‚   â”œâ”€â”€ technical_indicator_calculator.py      # TASK-11
â”‚   â”‚   â”œâ”€â”€ trigger_point_detector.py             # TASK-12
â”‚   â”‚   â””â”€â”€ cross_signal_analyzer.py              # TASK-13
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ trigger_orchestration_service.py      # TASK-14
â”‚   â”‚   â””â”€â”€ condition_management_service.py       # TASK-16
â”‚   â””â”€â”€ models/                                   # TASK-15
â”œâ”€â”€ business_logic/visualization/
â”‚   â”œâ”€â”€ engines/chart_data_engine.py              # TASK-17
â”‚   â””â”€â”€ services/minichart_orchestration_service.py
â””â”€â”€ ui/desktop/adapters/triggers/
    â””â”€â”€ trigger_builder_adapter.py                # TASK-15
```

### **í†µí•© í…ŒìŠ¤íŠ¸ ë²”ìœ„**
```python
# 1. í•µì‹¬ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
â”œâ”€â”€ ê¸°ìˆ  ì§€í‘œ ê³„ì‚° â†’ íŠ¸ë¦¬ê±° íƒì§€ â†’ í¬ë¡œìŠ¤ ì‹ í˜¸ ë¶„ì„
â”œâ”€â”€ ì¡°ê±´ ìƒì„± â†’ ê²€ì¦ â†’ ì‹œë®¬ë ˆì´ì…˜
â”œâ”€â”€ ì°¨íŠ¸ ë°ì´í„° ìƒì„± â†’ ì‹œê°í™” â†’ UI í‘œì‹œ
â””â”€â”€ ì „ì²´ ì›Œí¬í”Œë¡œìš° ì—”ë“œíˆ¬ì—”ë“œ í…ŒìŠ¤íŠ¸

# 2. ê¸°ì¡´ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
â”œâ”€â”€ trigger_builder_screen.pyì˜ ëª¨ë“  ê¸°ì¡´ ë©”ì„œë“œ í˜¸ì¶œ
â”œâ”€â”€ ê¸°ì¡´ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ì™€ ë™ì¼ì„± ë³´ì¥
â”œâ”€â”€ UI ì´ë²¤íŠ¸ ì²˜ë¦¬ í˜¸í™˜ì„±
â””â”€â”€ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ í˜¸í™˜ì„±

# 3. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ (10,000+ ë°ì´í„° í¬ì¸íŠ¸)
â”œâ”€â”€ ë³µì¡í•œ ì¡°ê±´ ì¡°í•© ì²˜ë¦¬
â”œâ”€â”€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
â””â”€â”€ ì‘ë‹µ ì‹œê°„ ë²¤ì¹˜ë§ˆí¬
```

## ğŸ—ï¸ **êµ¬í˜„ ëª©í‘œ**

### **í†µí•© í…ŒìŠ¤íŠ¸ êµ¬ì¡°**
```
tests/integration/trigger_builder/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_full_workflow_integration.py           # ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_backwards_compatibility.py            # ê¸°ì¡´ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_performance_integration.py            # ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_ui_adapter_integration.py             # UI ì–´ëŒ‘í„° í†µí•© í…ŒìŠ¤íŠ¸
â”œâ”€â”€ test_chart_integration.py                  # ì°¨íŠ¸ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
â””â”€â”€ fixtures/
    â”œâ”€â”€ sample_price_data.json                 # í…ŒìŠ¤íŠ¸ìš© ê°€ê²© ë°ì´í„°
    â”œâ”€â”€ sample_trigger_conditions.json         # í…ŒìŠ¤íŠ¸ìš© ì¡°ê±´ ë°ì´í„°
    â””â”€â”€ benchmark_results.json                 # ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ê°’
```

### **í˜¸í™˜ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸**
```
tests/compatibility/trigger_builder/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ test_legacy_method_compatibility.py        # ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„±
â”œâ”€â”€ test_simulation_result_compatibility.py    # ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ë™ì¼ì„±
â”œâ”€â”€ test_ui_behavior_compatibility.py          # UI ë™ì‘ í˜¸í™˜ì„±
â””â”€â”€ test_database_compatibility.py             # ë°ì´í„°ë² ì´ìŠ¤ í˜¸í™˜ì„±
```

## ğŸ“‹ **ìƒì„¸ ì‘ì—… ë‚´ìš©**

### **1. ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ (4ì‹œê°„)**
```python
# tests/integration/trigger_builder/test_full_workflow_integration.py
"""
íŠ¸ë¦¬ê±° ë¹Œë” ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸
"""

import pytest
import logging
from typing import List, Dict, Any
from unittest.mock import Mock, patch

from business_logic.triggers.services.trigger_orchestration_service import TriggerOrchestrationService
from business_logic.visualization.services.minichart_orchestration_service import MinichartOrchestrationService
from ui.desktop.adapters.triggers.trigger_builder_adapter import TriggerBuilderAdapter

class TestFullWorkflowIntegration:
    """ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def sample_price_data(self):
        """í…ŒìŠ¤íŠ¸ìš© ê°€ê²© ë°ì´í„°"""
        return [100 + i + (i % 10) * 2 for i in range(100)]
    
    @pytest.fixture
    def trigger_service(self):
        """íŠ¸ë¦¬ê±° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„œë¹„ìŠ¤"""
        return TriggerOrchestrationService()
    
    @pytest.fixture
    def chart_service(self):
        """ì°¨íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì„œë¹„ìŠ¤"""
        return MinichartOrchestrationService()
    
    @pytest.fixture
    def ui_adapter(self, trigger_service, chart_service):
        """UI ì–´ëŒ‘í„°"""
        return TriggerBuilderAdapter(trigger_service, chart_service)
    
    def test_complete_indicator_calculation_workflow(self, ui_adapter, sample_price_data):
        """ì™„ì „í•œ ì§€í‘œ ê³„ì‚° ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # Given: ê°€ê²© ë°ì´í„°ì™€ ì§€í‘œ ì„¤ì •
        indicator_configs = {
            "SMA": {"period": 20},
            "RSI": {"period": 14},
            "MACD": {"fast": 12, "slow": 26, "signal": 9}
        }
        
        # When: ì§€í‘œ ê³„ì‚° ì‹¤í–‰
        result = ui_adapter.calculate_all_indicators(sample_price_data, indicator_configs)
        
        # Then: ëª¨ë“  ì§€í‘œê°€ ì„±ê³µì ìœ¼ë¡œ ê³„ì‚°ë¨
        assert result.success
        assert "SMA" in result.indicators
        assert "RSI" in result.indicators
        assert "MACD" in result.indicators
        
        # ê³„ì‚°ëœ ì§€í‘œ ê°’ ê²€ì¦
        assert len(result.indicators["SMA"]) == len(sample_price_data)
        assert all(0 <= rsi <= 100 for rsi in result.indicators["RSI"] if rsi is not None)
        
        logging.info(f"ì§€í‘œ ê³„ì‚° ì™„ë£Œ: {list(result.indicators.keys())}")
    
    def test_complete_trigger_detection_workflow(self, ui_adapter, sample_price_data):
        """ì™„ì „í•œ íŠ¸ë¦¬ê±° íƒì§€ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # Given: íŠ¸ë¦¬ê±° ì¡°ê±´ ì„¤ì •
        condition = {
            "variable": "SMA_20",
            "operator": "crosses_above",
            "target": "price",
            "threshold": 105
        }
        
        # When: íŠ¸ë¦¬ê±° íƒì§€ ì‹¤í–‰
        result = ui_adapter.detect_triggers(sample_price_data, [condition])
        
        # Then: íŠ¸ë¦¬ê±°ê°€ ì„±ê³µì ìœ¼ë¡œ íƒì§€ë¨
        assert result.success
        assert isinstance(result.trigger_points, list)
        
        # íƒì§€ëœ íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ ê²€ì¦
        for point in result.trigger_points:
            assert 0 <= point < len(sample_price_data)
        
        logging.info(f"íŠ¸ë¦¬ê±° íƒì§€ ì™„ë£Œ: {len(result.trigger_points)}ê°œ í¬ì¸íŠ¸")
    
    def test_complete_simulation_workflow(self, ui_adapter, sample_price_data):
        """ì™„ì „í•œ ì‹œë®¬ë ˆì´ì…˜ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # Given: ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •
        simulation_config = {
            "conditions": [
                {
                    "variable": "RSI_14",
                    "operator": "less_than",
                    "threshold": 30
                }
            ],
            "indicators": {
                "RSI": {"period": 14},
                "SMA": {"period": 20}
            }
        }
        
        # When: ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        result = ui_adapter.run_complete_simulation(sample_price_data, simulation_config)
        
        # Then: ì‹œë®¬ë ˆì´ì…˜ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨
        assert result.success
        assert result.chart_data is not None
        assert len(result.trigger_points) >= 0
        
        # ì°¨íŠ¸ ë°ì´í„° ê²€ì¦
        chart_summary = result.chart_data.get_data_summary()
        assert chart_summary["price_points"] == len(sample_price_data)
        assert "RSI" in chart_summary["indicators"]
        
        logging.info(f"ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ: {chart_summary}")
    
    def test_chart_visualization_integration(self, ui_adapter, sample_price_data):
        """ì°¨íŠ¸ ì‹œê°í™” í†µí•© í…ŒìŠ¤íŠ¸"""
        # Given: ì°¨íŠ¸ ì„¤ì •
        chart_config = {
            "indicators": ["SMA_20", "RSI_14"],
            "show_signals": True,
            "theme": "dark"
        }
        
        # When: ì°¨íŠ¸ ìƒì„± ë° ì‹œê°í™”
        result = ui_adapter.create_visualization(sample_price_data, chart_config)
        
        # Then: ì°¨íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨
        assert result.success
        assert result.chart_data is not None
        
        # ì°¨íŠ¸ ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        chart_data = result.chart_data
        assert len(chart_data.price_data) == len(sample_price_data)
        assert "SMA" in chart_data.indicators
        assert "RSI" in chart_data.indicators
        
        logging.info(f"ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {chart_data.get_data_summary()}")
    
    def test_error_handling_integration(self, ui_adapter):
        """ì˜¤ë¥˜ ì²˜ë¦¬ í†µí•© í…ŒìŠ¤íŠ¸"""
        # Given: ì˜ëª»ëœ ì…ë ¥ ë°ì´í„°
        invalid_data = []
        invalid_conditions = [{"invalid": "condition"}]
        
        # When: ì˜¤ë¥˜ ìƒí™© ì²˜ë¦¬
        result = ui_adapter.detect_triggers(invalid_data, invalid_conditions)
        
        # Then: ì ì ˆí•œ ì˜¤ë¥˜ ì²˜ë¦¬
        assert not result.success
        assert result.error_message is not None
        
        logging.info(f"ì˜¤ë¥˜ ì²˜ë¦¬ í™•ì¸: {result.error_message}")
    
    @pytest.mark.performance
    def test_large_dataset_performance(self, ui_adapter):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        # Given: ëŒ€ìš©ëŸ‰ ê°€ê²© ë°ì´í„° (10,000 í¬ì¸íŠ¸)
        large_price_data = [100 + i * 0.1 + (i % 100) * 0.5 for i in range(10000)]
        
        # When: ë³µì¡í•œ ì¡°ê±´ìœ¼ë¡œ íŠ¸ë¦¬ê±° íƒì§€
        import time
        start_time = time.time()
        
        result = ui_adapter.detect_triggers(large_price_data, [
            {"variable": "SMA_50", "operator": "crosses_above", "target": "price"},
            {"variable": "RSI_14", "operator": "less_than", "threshold": 30}
        ])
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Then: ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
        assert result.success
        assert execution_time < 5.0  # 5ì´ˆ ì´ë‚´ ì²˜ë¦¬
        
        logging.info(f"ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œê°„: {execution_time:.2f}ì´ˆ")
    
    def test_memory_usage_integration(self, ui_adapter, sample_price_data):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µí•© í…ŒìŠ¤íŠ¸"""
        import psutil
        import os
        
        # Given: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # When: ë°˜ë³µì ì¸ ê³„ì‚° ìˆ˜í–‰
        for _ in range(10):
            result = ui_adapter.calculate_all_indicators(sample_price_data, {
                "SMA": {"period": 20},
                "EMA": {"period": 20},
                "RSI": {"period": 14},
                "MACD": {"fast": 12, "slow": 26, "signal": 9}
            })
            assert result.success
        
        # Then: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        assert memory_increase < 50  # 50MB ì´ë‚´ ì¦ê°€
        
        logging.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€: {memory_increase:.2f}MB")
```

### **2. ê¸°ì¡´ í˜¸í™˜ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ (3ì‹œê°„)**
```python
# tests/compatibility/trigger_builder/test_legacy_method_compatibility.py
"""
ê¸°ì¡´ trigger_builder_screen.py ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
"""

import pytest
from unittest.mock import Mock, patch, MagicMock
from PyQt6.QtWidgets import QApplication, QWidget
import sys

# ê¸°ì¡´ trigger_builder_screen.pyì™€ ìƒˆë¡œìš´ ì–´ëŒ‘í„° ì„í¬íŠ¸
from ui.desktop.screens.strategy_management.components.triggers.trigger_builder_screen import TriggerBuilderScreen
from ui.desktop.adapters.triggers.trigger_builder_adapter import TriggerBuilderAdapter

class TestLegacyMethodCompatibility:
    """ê¸°ì¡´ ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture(scope="module")
    def qapp(self):
        """PyQt6 ì• í”Œë¦¬ì¼€ì´ì…˜ í”½ìŠ¤ì²˜"""
        if not QApplication.instance():
            app = QApplication(sys.argv)
        else:
            app = QApplication.instance()
        yield app
        app.quit()
    
    @pytest.fixture
    def mock_parent(self, qapp):
        """Mock ë¶€ëª¨ ìœ„ì ¯"""
        return Mock(spec=QWidget)
    
    @pytest.fixture
    def legacy_screen(self, mock_parent):
        """ê¸°ì¡´ íŠ¸ë¦¬ê±° ë¹Œë” ìŠ¤í¬ë¦°"""
        return TriggerBuilderScreen(mock_parent)
    
    @pytest.fixture
    def sample_data(self):
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°"""
        return {
            "price_data": [100 + i for i in range(50)],
            "variable_name": "SMA_20",
            "period": 20,
            "threshold": 105
        }
    
    def test_calculate_sma_compatibility(self, legacy_screen, sample_data):
        """_calculate_sma ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        # Given: ê¸°ì¡´ ë°©ì‹ì˜ SMA ê³„ì‚°
        price_data = sample_data["price_data"]
        period = sample_data["period"]
        
        # When: ê¸°ì¡´ ë©”ì„œë“œ í˜¸ì¶œ
        with patch.object(legacy_screen, '_data_manager') as mock_manager:
            mock_manager.get_price_data.return_value = price_data
            
            legacy_result = legacy_screen._calculate_sma(price_data, period)
        
        # ìƒˆë¡œìš´ ì–´ëŒ…í„°ë¥¼ í†µí•œ ê³„ì‚°
        adapter = TriggerBuilderAdapter()
        new_result = adapter.calculate_technical_indicator("SMA", price_data, {"period": period})
        
        # Then: ê²°ê³¼ê°€ ë™ì¼í•¨
        assert len(legacy_result) == len(new_result.values)
        
        # ê°’ ë¹„êµ (ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©)
        for legacy_val, new_val in zip(legacy_result, new_result.values):
            if legacy_val is not None and new_val is not None:
                assert abs(legacy_val - new_val) < 0.001
    
    def test_calculate_trigger_points_compatibility(self, legacy_screen, sample_data):
        """calculate_trigger_points ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        # Given: íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ ê³„ì‚° íŒŒë¼ë¯¸í„°
        price_data = sample_data["price_data"]
        variable_name = sample_data["variable_name"]
        threshold = sample_data["threshold"]
        
        # When: ê¸°ì¡´ ë©”ì„œë“œ í˜¸ì¶œ
        with patch.object(legacy_screen, '_prepare_calculation_data') as mock_prep:
            mock_prep.return_value = {"SMA_20": [100 + i * 0.5 for i in range(50)]}
            
            legacy_result = legacy_screen.calculate_trigger_points(
                variable_name, "crosses_above", threshold
            )
        
        # ìƒˆë¡œìš´ ì–´ëŒ‘í„°ë¥¼ í†µí•œ ê³„ì‚°
        adapter = TriggerBuilderAdapter()
        condition = {
            "variable": variable_name,
            "operator": "crosses_above", 
            "threshold": threshold
        }
        new_result = adapter.detect_triggers(price_data, [condition])
        
        # Then: ê²°ê³¼ê°€ ë™ì¼í•¨
        assert isinstance(legacy_result, list)
        assert isinstance(new_result.trigger_points, list)
        
        # íŠ¸ë¦¬ê±° í¬ì¸íŠ¸ ê°œìˆ˜ ë¹„êµ
        assert len(legacy_result) == len(new_result.trigger_points)
    
    def test_simulation_method_compatibility(self, legacy_screen, sample_data):
        """ì‹œë®¬ë ˆì´ì…˜ ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        # Given: ì‹œë®¬ë ˆì´ì…˜ íŒŒë¼ë¯¸í„°
        price_data = sample_data["price_data"]
        
        # When: ê¸°ì¡´ ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        with patch.object(legacy_screen, 'trigger_simulation_service') as mock_service:
            mock_service.run_simulation.return_value = {
                "success": True,
                "trigger_points": [10, 20, 30],
                "chart_data": Mock()
            }
            
            legacy_result = legacy_screen.run_trigger_simulation()
        
        # ìƒˆë¡œìš´ ì–´ëŒ‘í„°ë¥¼ í†µí•œ ì‹œë®¬ë ˆì´ì…˜
        adapter = TriggerBuilderAdapter()
        simulation_config = {
            "conditions": [{"variable": "SMA_20", "operator": "crosses_above", "threshold": 105}],
            "indicators": {"SMA": {"period": 20}}
        }
        new_result = adapter.run_complete_simulation(price_data, simulation_config)
        
        # Then: ê²°ê³¼ êµ¬ì¡°ê°€ í˜¸í™˜ë¨
        assert "success" in legacy_result
        assert hasattr(new_result, "success")
        assert "trigger_points" in legacy_result
        assert hasattr(new_result, "trigger_points")
    
    def test_ui_event_handler_compatibility(self, legacy_screen):
        """UI ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        # Given: UI ì´ë²¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        mock_event = Mock()
        
        # When: ê¸°ì¡´ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ í˜¸ì¶œ
        original_handlers = [
            "on_variable_changed",
            "on_operator_changed", 
            "on_threshold_changed",
            "on_calculate_clicked"
        ]
        
        # Then: ëª¨ë“  í•¸ë“¤ëŸ¬ê°€ ì¡´ì¬í•˜ê³  í˜¸ì¶œ ê°€ëŠ¥í•¨
        for handler_name in original_handlers:
            assert hasattr(legacy_screen, handler_name)
            handler = getattr(legacy_screen, handler_name)
            assert callable(handler)
    
    def test_data_access_compatibility(self, legacy_screen):
        """ë°ì´í„° ì ‘ê·¼ ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        # Given: ë°ì´í„° ì ‘ê·¼ ë©”ì„œë“œë“¤
        data_methods = [
            "get_current_variable_data",
            "get_calculation_results",
            "get_trigger_points",
            "get_simulation_results"
        ]
        
        # When & Then: ëª¨ë“  ë©”ì„œë“œê°€ ì¡´ì¬í•¨
        for method_name in data_methods:
            assert hasattr(legacy_screen, method_name)
            method = getattr(legacy_screen, method_name)
            assert callable(method)
    
    def test_configuration_compatibility(self, legacy_screen):
        """ì„¤ì • ê´€ë ¨ ë©”ì„œë“œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        # Given: ì„¤ì • ê´€ë ¨ ë©”ì„œë“œë“¤
        config_methods = [
            "save_current_configuration",
            "load_configuration",
            "reset_to_defaults"
        ]
        
        # When & Then: ëª¨ë“  ì„¤ì • ë©”ì„œë“œê°€ ì¡´ì¬í•¨
        for method_name in config_methods:
            if hasattr(legacy_screen, method_name):
                method = getattr(legacy_screen, method_name)
                assert callable(method)
```

### **3. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ (2ì‹œê°„)**
```python
# tests/integration/trigger_builder/test_performance_integration.py
"""
ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬
"""

import pytest
import time
import psutil
import os
from typing import List, Dict, Any
import json
from pathlib import Path

from ui.desktop.adapters.triggers.trigger_builder_adapter import TriggerBuilderAdapter

class TestPerformanceIntegration:
    """ì„±ëŠ¥ í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def performance_adapter(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì–´ëŒ‘í„°"""
        return TriggerBuilderAdapter()
    
    @pytest.fixture
    def benchmark_data(self):
        """ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ë°ì´í„° ë¡œë“œ"""
        benchmark_file = Path(__file__).parent / "fixtures" / "benchmark_results.json"
        if benchmark_file.exists():
            with open(benchmark_file, 'r') as f:
                return json.load(f)
        return {
            "max_execution_time": {
                "small_dataset": 1.0,    # 100 í¬ì¸íŠ¸
                "medium_dataset": 3.0,   # 1,000 í¬ì¸íŠ¸  
                "large_dataset": 10.0    # 10,000 í¬ì¸íŠ¸
            },
            "max_memory_usage": {
                "small_dataset": 10,     # 10MB
                "medium_dataset": 50,    # 50MB
                "large_dataset": 200     # 200MB
            }
        }
    
    def generate_price_data(self, size: int) -> List[float]:
        """í…ŒìŠ¤íŠ¸ìš© ê°€ê²© ë°ì´í„° ìƒì„±"""
        import random
        random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        base_price = 100.0
        prices = [base_price]
        
        for i in range(1, size):
            change = random.uniform(-2.0, 2.0)
            new_price = max(prices[-1] + change, 1.0)
            prices.append(new_price)
        
        return prices
    
    @pytest.mark.performance
    def test_small_dataset_performance(self, performance_adapter, benchmark_data):
        """ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (100 í¬ì¸íŠ¸)"""
        # Given: ì†Œê·œëª¨ ë°ì´í„°
        price_data = self.generate_price_data(100)
        conditions = [
            {"variable": "SMA_20", "operator": "crosses_above", "target": "price"},
            {"variable": "RSI_14", "operator": "less_than", "threshold": 30}
        ]
        
        # When: ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = performance_adapter.detect_triggers(price_data, conditions)
        execution_time = time.time() - start_time
        
        # Then: ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
        assert result.success
        max_time = benchmark_data["max_execution_time"]["small_dataset"]
        assert execution_time < max_time, f"ì‹¤í–‰ ì‹œê°„ {execution_time:.2f}ì´ˆê°€ ê¸°ì¤€ {max_time}ì´ˆ ì´ˆê³¼"
        
        print(f"ì†Œê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    @pytest.mark.performance
    def test_medium_dataset_performance(self, performance_adapter, benchmark_data):
        """ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (1,000 í¬ì¸íŠ¸)"""
        # Given: ì¤‘ê°„ ê·œëª¨ ë°ì´í„°
        price_data = self.generate_price_data(1000)
        simulation_config = {
            "conditions": [
                {"variable": "SMA_50", "operator": "crosses_above", "target": "EMA_20"},
                {"variable": "MACD", "operator": "crosses_above", "target": "MACD_signal"}
            ],
            "indicators": {
                "SMA": {"period": 50},
                "EMA": {"period": 20},
                "MACD": {"fast": 12, "slow": 26, "signal": 9}
            }
        }
        
        # When: ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = performance_adapter.run_complete_simulation(price_data, simulation_config)
        execution_time = time.time() - start_time
        
        # Then: ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
        assert result.success
        max_time = benchmark_data["max_execution_time"]["medium_dataset"]
        assert execution_time < max_time, f"ì‹¤í–‰ ì‹œê°„ {execution_time:.2f}ì´ˆê°€ ê¸°ì¤€ {max_time}ì´ˆ ì´ˆê³¼"
        
        print(f"ì¤‘ê°„ ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    @pytest.mark.performance
    def test_large_dataset_performance(self, performance_adapter, benchmark_data):
        """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ (10,000 í¬ì¸íŠ¸)"""
        # Given: ëŒ€ê·œëª¨ ë°ì´í„°
        price_data = self.generate_price_data(10000)
        complex_conditions = [
            {"variable": "SMA_200", "operator": "crosses_above", "target": "price"},
            {"variable": "RSI_14", "operator": "between", "min_threshold": 30, "max_threshold": 70},
            {"variable": "MACD", "operator": "crosses_above", "target": "MACD_signal"},
            {"variable": "EMA_50", "operator": "greater_than", "target": "SMA_200"}
        ]
        
        # When: ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = performance_adapter.detect_triggers(price_data, complex_conditions)
        execution_time = time.time() - start_time
        
        # Then: ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
        assert result.success
        max_time = benchmark_data["max_execution_time"]["large_dataset"]
        assert execution_time < max_time, f"ì‹¤í–‰ ì‹œê°„ {execution_time:.2f}ì´ˆê°€ ê¸°ì¤€ {max_time}ì´ˆ ì´ˆê³¼"
        
        print(f"ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    @pytest.mark.performance
    def test_memory_usage_benchmark(self, performance_adapter, benchmark_data):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        # Given: ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # When: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì—°ì† ì²˜ë¦¬
        for size in [100, 1000, 5000]:
            price_data = self.generate_price_data(size)
            
            for _ in range(5):  # 5ë²ˆ ë°˜ë³µ
                result = performance_adapter.calculate_all_indicators(price_data, {
                    "SMA": {"period": 20},
                    "EMA": {"period": 20}, 
                    "RSI": {"period": 14},
                    "MACD": {"fast": 12, "slow": 26, "signal": 9},
                    "BOLLINGER": {"period": 20, "std_dev": 2}
                })
                assert result.success
        
        # Then: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        max_memory = benchmark_data["max_memory_usage"]["large_dataset"]
        assert memory_increase < max_memory, f"ë©”ëª¨ë¦¬ ì¦ê°€ {memory_increase:.1f}MBê°€ ê¸°ì¤€ {max_memory}MB ì´ˆê³¼"
        
        print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€: {memory_increase:.1f}MB")
    
    @pytest.mark.performance  
    def test_concurrent_operations_performance(self, performance_adapter):
        """ë™ì‹œ ì—°ì‚° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        import threading
        import queue
        
        # Given: ì—¬ëŸ¬ ë™ì‹œ ì‘ì—…
        price_data = self.generate_price_data(1000)
        results_queue = queue.Queue()
        
        def worker_task(worker_id: int):
            try:
                result = performance_adapter.detect_triggers(price_data, [
                    {"variable": f"SMA_{20 + worker_id}", "operator": "crosses_above", "target": "price"}
                ])
                results_queue.put(("success", worker_id, result.success))
            except Exception as e:
                results_queue.put(("error", worker_id, str(e)))
        
        # When: ë™ì‹œ ì‹¤í–‰
        threads = []
        start_time = time.time()
        
        for i in range(5):  # 5ê°œ ìŠ¤ë ˆë“œ
            thread = threading.Thread(target=worker_task, args=(i,))
            threads.append(thread)
            thread.start()
        
        for thread in threads:
            thread.join()
        
        execution_time = time.time() - start_time
        
        # Then: ëª¨ë“  ì‘ì—… ì„±ê³µ ë° ì„±ëŠ¥ í™•ì¸
        results = []
        while not results_queue.empty():
            results.append(results_queue.get())
        
        assert len(results) == 5
        assert all(result[0] == "success" and result[2] for result in results)
        assert execution_time < 10.0  # 10ì´ˆ ì´ë‚´
        
        print(f"ë™ì‹œ ì‘ì—… ì²˜ë¦¬ ì‹œê°„: {execution_time:.3f}ì´ˆ")
    
    def test_save_benchmark_results(self, benchmark_data):
        """ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥"""
        # ìƒˆë¡œìš´ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìƒì„± (ì‹¤ì œ ì¸¡ì •ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸)
        updated_benchmark = {
            "last_updated": time.time(),
            "max_execution_time": {
                "small_dataset": 1.0,
                "medium_dataset": 3.0,
                "large_dataset": 10.0
            },
            "max_memory_usage": {
                "small_dataset": 10,
                "medium_dataset": 50,
                "large_dataset": 200
            },
            "test_environment": {
                "python_version": "3.13",
                "platform": "Windows",
                "cpu_count": os.cpu_count(),
                "total_memory": psutil.virtual_memory().total / 1024 / 1024 / 1024  # GB
            }
        }
        
        # ë²¤ì¹˜ë§ˆí¬ íŒŒì¼ ì €ì¥
        benchmark_file = Path(__file__).parent / "fixtures" / "benchmark_results.json"
        benchmark_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(benchmark_file, 'w') as f:
            json.dump(updated_benchmark, f, indent=2)
        
        print(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {benchmark_file}")
```

### **4. í†µí•© ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ (1ì‹œê°„)**
```powershell
# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
Write-Host "ğŸš€ íŠ¸ë¦¬ê±° ë¹Œë” ë¦¬íŒ©í† ë§ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘" -ForegroundColor Green

# 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
Write-Host "`nğŸ“‹ 1. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰..." -ForegroundColor Yellow
pytest tests/unit/business_logic/triggers/ -v --tb=short

if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨" -ForegroundColor Red
    exit 1
}

# 2. í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
Write-Host "`nğŸ”— 2. í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰..." -ForegroundColor Yellow
pytest tests/integration/trigger_builder/ -v --tb=short

if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨" -ForegroundColor Red
    exit 1
}

# 3. í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
Write-Host "`nğŸ”„ 3. í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰..." -ForegroundColor Yellow
pytest tests/compatibility/trigger_builder/ -v --tb=short

if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨" -ForegroundColor Red
    exit 1
}

# 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
Write-Host "`nâš¡ 4. ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰..." -ForegroundColor Yellow
pytest tests/integration/trigger_builder/test_performance_integration.py -v -m performance --tb=short

if ($LASTEXITCODE -ne 0) {
    Write-Host "âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨" -ForegroundColor Red
    exit 1
}

# 5. ì½”ë“œ ì»¤ë²„ë¦¬ì§€ í™•ì¸
Write-Host "`nğŸ“Š 5. ì½”ë“œ ì»¤ë²„ë¦¬ì§€ í™•ì¸..." -ForegroundColor Yellow
pytest tests/ --cov=business_logic.triggers --cov=ui.desktop.adapters.triggers --cov-report=html --cov-report=term

# 6. í†µí•© ê²€ì¦ ì™„ë£Œ
Write-Host "`nâœ… ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!" -ForegroundColor Green
Write-Host "ğŸ“ˆ ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸: htmlcov/index.html" -ForegroundColor Cyan
```

## âœ… **ì™„ë£Œ ê¸°ì¤€**

### **í†µí•© í…ŒìŠ¤íŠ¸ ì²´í¬ë¦¬ìŠ¤íŠ¸**
- [ ] ì „ì²´ ì›Œí¬í”Œë¡œìš° í†µí•© í…ŒìŠ¤íŠ¸ 100% í†µê³¼
- [ ] ê¸°ì¡´ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ 100% í†µê³¼
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ì¶©ì¡±
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ì¤€ ì¶©ì¡±
- [ ] ì½”ë“œ ì»¤ë²„ë¦¬ì§€ 95% ì´ìƒ

### **í’ˆì§ˆ ê¸°ì¤€**
- [ ] ëª¨ë“  ê¸°ì¡´ ê¸°ëŠ¥ 100% ë™ì‘
- [ ] ì„±ëŠ¥ ì €í•˜ ì—†ìŒ (ê¸°ì¡´ ëŒ€ë¹„ ë™ë“± ì´ìƒ)
- [ ] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ
- [ ] ë™ì‹œì„± ì•ˆì „ì„± ë³´ì¥

### **ê²€ì¦ ëª…ë ¹ì–´**
```powershell
# ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/integration/ tests/compatibility/ -v

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest -m performance -v

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ ì „ì²´ í…ŒìŠ¤íŠ¸
pytest --cov=business_logic --cov=ui.desktop.adapters --cov-report=html
```

## ğŸ”— **ì—°ê´€ ì‘ì—…**
- **ì´ì „**: TASK-20250802-17 (ë¯¸ë‹ˆì°¨íŠ¸ ì‹œê°í™” ì„œë¹„ìŠ¤ êµ¬í˜„)
- **ë‹¤ìŒ**: TASK-20250802-19 (ì„±ëŠ¥ ìµœì í™” ë° ë²¤ì¹˜ë§ˆí‚¹)
- **ê´€ë ¨**: ëª¨ë“  ì´ì „ TASK (TASK-11 ~ TASK-17)

## ğŸ“Š **ì˜ˆìƒ ì†Œìš” ì‹œê°„**
- **ì´ ì†Œìš” ì‹œê°„**: 10ì‹œê°„
- **ìš°ì„ ìˆœìœ„**: CRITICAL
- **ë³µì¡ë„**: HIGH (ì „ì²´ ì‹œìŠ¤í…œ í†µí•©)
- **ë¦¬ìŠ¤í¬**: HIGH (í˜¸í™˜ì„± ì´ìŠˆ ê°€ëŠ¥ì„±)

---
**ì‘ì„±ì¼**: 2025ë…„ 8ì›” 2ì¼  
**ë‹´ë‹¹ì**: GitHub Copilot  
**ë¬¸ì„œ íƒ€ì…**: í†µí•© í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
