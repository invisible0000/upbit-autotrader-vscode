# 단위 테스트 지침

## 테스트 파일 명명 규칙

1. 모든 테스트 파일은 `test_`로 시작해야 합니다.
2. 테스트 파일 이름은 `tasks.md`의 태스크 번호와 일치하도록 명명합니다.
   - 예: `test_02_1_upbit_api.py` - 태스크 2.1 업비트 REST API 기본 클라이언트 구현에 대한 테스트
   - 예: `test_03_2_data_collector.py` - 태스크 3.2 데이터 수집기 구현에 대한 테스트
3. 테스트 파일 이름 형식: `test_[태스크번호]_[기능명].py`
   - 태스크 번호는 `tasks.md`의 번호와 정확히 일치해야 합니다.
   - 기능명은 테스트 대상 모듈/클래스의 이름을 snake_case로 작성합니다.

## 테스트 개발 및 실행 프로세스

1. **테스트 우선 개발 (TDD) 권장**
   - 기능 구현 전에 테스트를 먼저 작성하여 요구사항을 명확히 이해합니다.
   - 테스트가 실패하는 것을 확인한 후 기능을 구현합니다.
   - 기능 구현 후 테스트가 통과하는지 확인합니다.

2. **기능 개발 완료 후 필수 테스트 실행**
   - 모든 기능 개발이 완료되면 반드시 해당 기능에 대한 단위 테스트를 실행해야 합니다.
   - `run_tests_in_order.py` 스크립트를 사용하여 테스트를 순서대로 실행합니다.
   - 모든 테스트가 통과하는지 확인합니다.
   - 기능 간 암묵적인 공통 프로세스가 존재할 수 있으므로, 테스트가 너무 오래 걸려 개발 지연을 초래하지 않는다면 처음부터 현재 테스트까지 또는 개발 블록 단위로 현재까지 테스트를 실행합니다.
   - 새로운 기능 개발이 이전에 개발된 기능에 영향을 미칠 가능성이 있으므로, 회귀 테스트를 통해 전체 시스템의 안정성을 확인합니다.

3. **테스트 커버리지 목표**
   - 핵심 비즈니스 로직: 최소 80% 이상의 코드 커버리지
   - 데이터 모델 및 유틸리티: 최소 70% 이상의 코드 커버리지
   - 외부 API 연동 부분: 모든 주요 시나리오에 대한 테스트 케이스 구현

## 테스트 작성 가이드라인

1. **테스트 구조**
   - 각 테스트 클래스는 하나의 모듈 또는 클래스를 테스트합니다.
   - `setUp`과 `tearDown` 메서드를 사용하여 테스트 환경을 준비하고 정리합니다.
   - 각 테스트 메서드는 하나의 기능 또는 시나리오만 테스트합니다.
   - 단위 테스트 코드 내의 하위 테스트도 각자의 단위 테스트 ID를 가지고 엄밀하게 관리되어야 합니다.
   - 하위 테스트 ID는 상위 테스트 ID에 하위 번호를 추가하는 형식으로 지정합니다(예: `test_03_3_1`, `test_03_3_2`).

2. **테스트 케이스 작성**
   - 정상 케이스: 기능이 정상적으로 동작하는 경우를 테스트합니다.
   - 경계 케이스: 입력값의 경계 조건을 테스트합니다.
   - 예외 케이스: 예외가 발생하는 경우를 테스트합니다.
   - 모든 테스트는 독립적이어야 하며, 다른 테스트에 의존해서는 안 됩니다.

3. **모의 객체(Mock) 사용**
   - 외부 의존성(API, 데이터베이스 등)은 모의 객체로 대체합니다.
   - `unittest.mock` 모듈을 사용하여 모의 객체를 생성합니다.
   - 실제 API 호출이나 데이터베이스 연결이 필요한 경우, 테스트 환경에서만 사용할 수 있는 별도의 설정을 사용합니다.

4. **테스트 주석**
   - 각 테스트 클래스와 메서드에는 명확한 docstring을 작성합니다.
   - 복잡한 테스트 로직에는 주석을 추가하여 의도를 명확히 합니다.
   - 테스트가 검증하는 요구사항 번호를 주석으로 표시합니다.

5. **테스트 출력**
   - 테스트 중에는 사용자가 확인할 수 있는 필수 정보의 출력을 print하도록 합니다.
   - 테스트 진행 상황, 중요 데이터 값, 예상 결과와 실제 결과를 출력하여 사용자가 테스트 흐름을 이해할 수 있게 합니다.
   - 특히 복잡한 데이터 처리나 계산 과정에서는 중간 결과를 출력하여 디버깅을 용이하게 합니다.

6. **수치 비교 테스트**
   - 기능의 출력이 수치(numeric)인 값이고 테스트에서 정확한 일치를 요구할 때는 연산 방식에 따른 오차/오류를 고려하여 판단해야 합니다.
   - 부동 소수점 연산의 경우 `assertEqual` 대신 `assertAlmostEqual` 또는 `numpy.testing.assert_allclose`를 사용하여 허용 오차(tolerance)를 지정합니다.
   - 데이터 구조(예: DataFrame, Series)를 비교할 때는 값뿐만 아니라 인덱스, 컬럼명, 데이터 타입 등도 고려해야 합니다.
   - 서로 다른 알고리즘이나 라이브러리를 사용하여 동일한 결과를 계산할 때는 미세한 차이가 발생할 수 있으므로 적절한 허용 오차를 설정합니다.
   - 테스트 실패 시 실제 값과 기대 값의 차이를 출력하여 디버깅을 용이하게 합니다.

## 테스트 디버깅 및 유지보수

1. **실패한 테스트 디버깅**
   - 테스트 실패 시 실패 원인을 명확히 파악합니다.
   - 디버깅 도구를 활용하여 문제를 해결합니다.
   - 수정 후 모든 테스트를 다시 실행하여 회귀 문제가 없는지 확인합니다.

2. **테스트 코드 리팩토링**
   - 테스트 코드도 프로덕션 코드와 마찬가지로 유지보수가 필요합니다.
   - 중복 코드는 헬퍼 메서드로 추출합니다.
   - 테스트 픽스처를 활용하여 테스트 데이터를 관리합니다.

3. **지속적 통합(CI)**
   - 모든 코드 변경 시 자동으로 테스트가 실행되도록 CI 환경을 구성합니다.
   - 테스트 실패 시 즉시 알림을 받고 문제를 해결합니다.

## 테스트 결과 보고

1. **테스트 결과 문서화**
   - 테스트 실행 결과를 문서화하여 보관합니다.
   - 테스트 커버리지 보고서를 생성하여 코드 품질을 모니터링합니다.

2. **테스트 결과 분석**
   - 주기적으로 테스트 결과를 분석하여 개선 사항을 식별합니다.
   - 자주 실패하는 테스트는 원인을 분석하고 안정성을 개선합니다.