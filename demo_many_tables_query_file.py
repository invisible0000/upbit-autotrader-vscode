"""
ìˆœìˆ˜ DB ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ - ì‹¤ì œ íŒŒì¼ DBë¡œ 5000+ í…Œì´ë¸” ìƒì„± í›„ ì§ì ‘ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •
"""
import time
import sqlite3
import random
import requests
import os
from datetime import datetime, timedelta
from typing import List


def test_db_table_scaling():
    """ì—…ë¹„íŠ¸ ëª¨ë“  ë§ˆì¼“ì— ëŒ€í•´ ëŒ€ëŸ‰ í…Œì´ë¸” ìƒì„± í›„ ìˆœìˆ˜ DB ì„±ëŠ¥ ì¸¡ì • (ì‹¤ì œ íŒŒì¼ DB)"""

    print("ğŸ”¥ ìˆœìˆ˜ DB í…Œì´ë¸” ìŠ¤ì¼€ì¼ë§ í…ŒìŠ¤íŠ¸ ì‹œì‘ (ì‹¤ì œ íŒŒì¼ DB)\n")

    # ì‹¤ì œ DB íŒŒì¼ ì‚¬ìš©
    db_path = "test_scaling_performance.sqlite3"

    # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ DB íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
    if os.path.exists(db_path):
        os.remove(db_path)
        print(f"   ğŸ—‘ï¸  ê¸°ì¡´ í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì‚­ì œ: {db_path}")

    try:
        # 1ë‹¨ê³„: ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì • (ë¹ˆ DB)
        print("ğŸ“Š 1ë‹¨ê³„: ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì • (ë¹ˆ íŒŒì¼ DB)")
        baseline_time = measure_baseline_query_performance(db_path)
        print(f"   ê¸°ì¤€ ì¿¼ë¦¬ ì‹œê°„: {baseline_time:.3f}ms")

        # 2ë‹¨ê³„: ì—…ë¹„íŠ¸ ëª¨ë“  ë§ˆì¼“ ì¡°íšŒ (API ì§ì ‘ í˜¸ì¶œ)
        print("\nğŸ“Š 2ë‹¨ê³„: ì—…ë¹„íŠ¸ ë§ˆì¼“ ì¡°íšŒ (API ì§ì ‘ í˜¸ì¶œ)")
        markets = get_all_upbit_markets_direct()
        all_markets = [m['market'] for m in markets]
        krw_markets = [m for m in all_markets if m.startswith('KRW-')]
        btc_markets = [m for m in all_markets if m.startswith('BTC-')]
        usdt_markets = [m for m in all_markets if m.startswith('USDT-')]

        print(f"   ì´ ë§ˆì¼“ ìˆ˜: {len(all_markets)}ê°œ")
        print(f"   KRW ë§ˆì¼“: {len(krw_markets)}ê°œ")
        print(f"   BTC ë§ˆì¼“: {len(btc_markets)}ê°œ")
        print(f"   USDT ë§ˆì¼“: {len(usdt_markets)}ê°œ")

        # 3ë‹¨ê³„: ëª¨ë“  ë§ˆì¼“ Ã— ëª¨ë“  íƒ€ì„í”„ë ˆì„ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±
        print("\nğŸ“Š 3ë‹¨ê³„: ëŒ€ëŸ‰ í…Œì´ë¸” ìƒì„±")
        timeframes = ["1m", "3m", "5m", "15m", "30m", "1h", "4h", "1d", "1w", "1M"]

        total_tables_to_create = len(all_markets) * len(timeframes)
        print(f"   ìƒì„±í•  í…Œì´ë¸” ìˆ˜: {total_tables_to_create}ê°œ ({len(all_markets)} ë§ˆì¼“ Ã— {len(timeframes)} íƒ€ì„í”„ë ˆì„)")

        # í…Œì´ë¸” ìƒì„± ì‹¤í–‰
        conn = sqlite3.connect(db_path)
        created_count = create_all_market_tables(conn, all_markets, timeframes)
        print(f"   ì‹¤ì œ ìƒì„±ëœ í…Œì´ë¸”: {created_count}ê°œ")

        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size_mb = os.path.getsize(db_path) / (1024 * 1024)
        print(f"   DB íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB")

        # 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •
        print("\nğŸ“Š 4ë‹¨ê³„: ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •")
        measure_metadata_query_performance(conn)

        # 5ë‹¨ê³„: ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •
        print("\nğŸ“Š 5ë‹¨ê³„: ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •")
        measure_data_query_performance(conn)

        # 6ë‹¨ê³„: ë³µì¡í•œ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •
        print("\nğŸ“Š 6ë‹¨ê³„: ë³µì¡í•œ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •")
        measure_complex_query_performance(conn)

        # 7ë‹¨ê³„: DB ë™ê¸°í™” ì„±ëŠ¥ ì¸¡ì •
        print("\nğŸ“Š 7ë‹¨ê³„: DB ë™ê¸°í™” ì„±ëŠ¥ ì¸¡ì •")
        measure_sync_performance(conn)

        # 8ë‹¨ê³„: í™•ì¥ëœ í™˜ê²½ì—ì„œ ê¸°ì¤€ ì¿¼ë¦¬ ì¬ì¸¡ì •
        print("\nğŸ“Š 8ë‹¨ê³„: ëŒ€ëŸ‰ í…Œì´ë¸” í™˜ê²½ì—ì„œ ê¸°ì¤€ ì¿¼ë¦¬ ì¬ì¸¡ì •")
        scaled_time = measure_baseline_query_performance_on_db(conn)
        print(f"   í™•ì¥ í™˜ê²½ ì¿¼ë¦¬ ì‹œê°„: {scaled_time:.3f}ms")

        # 9ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„
        print("\nğŸ“Š 9ë‹¨ê³„: ì„±ëŠ¥ ë¶„ì„")
        analyze_performance_impact(baseline_time, scaled_time, 0, created_count)

        # ì—°ê²° ì¢…ë£Œ
        conn.close()

        # ìµœì¢… íŒŒì¼ í¬ê¸° í™•ì¸
        final_file_size_mb = os.path.getsize(db_path) / (1024 * 1024)
        print("\nğŸ“ ìµœì¢… DB íŒŒì¼ ì •ë³´:")
        print(f"   íŒŒì¼ ê²½ë¡œ: {os.path.abspath(db_path)}")
        print(f"   íŒŒì¼ í¬ê¸°: {final_file_size_mb:.2f} MB")
        print(f"   í…Œì´ë¸” ìˆ˜: {created_count}ê°œ")

    finally:
        # 10ë‹¨ê³„: í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì •ë¦¬
        print("\nğŸ“Š 10ë‹¨ê³„: í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì •ë¦¬")
        cleanup_test_db(db_path)


def get_all_upbit_markets_direct() -> List:
    """ì—…ë¹„íŠ¸ API ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ëª¨ë“  ë§ˆì¼“ ì¡°íšŒ"""
    try:
        response = requests.get("https://api.upbit.com/v1/market/all", timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        print(f"   âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
        return [
            {"market": "KRW-BTC"}, {"market": "KRW-ETH"}, {"market": "KRW-XRP"},
            {"market": "BTC-ETH"}, {"market": "USDT-BTC"}
        ]


def create_all_market_tables(conn: sqlite3.Connection, markets: List[str], timeframes: List[str]) -> int:
    """ëª¨ë“  ë§ˆì¼“ê³¼ íƒ€ì„í”„ë ˆì„ ì¡°í•©ìœ¼ë¡œ í…Œì´ë¸” ìƒì„±"""
    created_count = 0
    cursor = conn.cursor()

    print(f"   ì‹œì‘: {len(markets)} ë§ˆì¼“ Ã— {len(timeframes)} íƒ€ì„í”„ë ˆì„")

    for i, market in enumerate(markets):
        if i % 50 == 0:
            print(f"   ì§„í–‰ë¥ : {i + 1}/{len(markets)} - {market}")

        for timeframe in timeframes:
            try:
                table_name = f"candles_{market.replace('-', '_')}_{timeframe}"

                # í‘œì¤€í™”ëœ ìº”ë“¤ í…Œì´ë¸” ìƒì„±
                cursor.execute(f"""
                    CREATE TABLE IF NOT EXISTS {table_name} (
                        candle_date_time_utc TEXT NOT NULL PRIMARY KEY,
                        market TEXT NOT NULL,
                        candle_date_time_kst TEXT NOT NULL,
                        opening_price REAL NOT NULL,
                        high_price REAL NOT NULL,
                        low_price REAL NOT NULL,
                        trade_price REAL NOT NULL,
                        timestamp INTEGER NOT NULL,
                        candle_acc_trade_price REAL NOT NULL,
                        candle_acc_trade_volume REAL NOT NULL,
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP
                    )
                """)

                # ìƒ˜í”Œ ë°ì´í„° ì‚½ì… (3ê°œë§Œ)
                insert_sample_candle_data(cursor, table_name, market)
                created_count += 1

            except Exception as e:
                print(f"      âš ï¸  {market} {timeframe} ì‹¤íŒ¨: {e}")

    conn.commit()
    print(f"   âœ… ì´ {created_count}ê°œ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")
    return created_count


def insert_sample_candle_data(cursor: sqlite3.Cursor, table_name: str, market: str):
    """ê°€ìƒì˜ ìº”ë“¤ ë°ì´í„° ì‚½ì…"""
    base_time = datetime.now()
    base_price = random.uniform(1000, 100000)

    for i in range(3):  # 3ê°œë§Œ ì‚½ì…
        candle_time = base_time - timedelta(minutes=i)

        # ê°„ë‹¨í•œ ê°€ê²© ë³€ë™
        price_variation = random.uniform(0.98, 1.02)
        opening_price = base_price * price_variation
        high_price = opening_price * random.uniform(1.0, 1.01)
        low_price = opening_price * random.uniform(0.99, 1.0)
        trade_price = random.uniform(low_price, high_price)

        cursor.execute(f"""
            INSERT OR IGNORE INTO {table_name}
            (candle_date_time_utc, market, candle_date_time_kst, opening_price,
             high_price, low_price, trade_price, timestamp,
             candle_acc_trade_price, candle_acc_trade_volume)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            candle_time.strftime('%Y-%m-%dT%H:%M:%S'),
            market,
            candle_time.strftime('%Y-%m-%dT%H:%M:%S'),
            opening_price,
            high_price,
            low_price,
            trade_price,
            int(candle_time.timestamp() * 1000),
            trade_price * random.uniform(100, 1000),  # ê±°ë˜ëŒ€ê¸ˆ
            random.uniform(1, 100)  # ê±°ë˜ëŸ‰
        ))


def measure_baseline_query_performance(db_path: str) -> float:
    """ê¸°ì¤€ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì • (ë¹ˆ DB)"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    # ê°„ë‹¨í•œ ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬
    start_time = time.perf_counter()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
    cursor.fetchall()
    query_time = (time.perf_counter() - start_time) * 1000

    conn.close()
    return query_time


def measure_baseline_query_performance_on_db(conn: sqlite3.Connection) -> float:
    """ê¸°ì¡´ DBì—ì„œ ê¸°ì¤€ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
    cursor = conn.cursor()

    start_time = time.perf_counter()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' LIMIT 10")
    cursor.fetchall()
    query_time = (time.perf_counter() - start_time) * 1000

    return query_time


def measure_metadata_query_performance(conn: sqlite3.Connection):
    """ë©”íƒ€ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
    cursor = conn.cursor()

    # 1. ì „ì²´ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
    times = []
    tables_count = 0
    for i in range(10):
        start_time = time.perf_counter()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' ORDER BY name")
        tables = cursor.fetchall()
        tables_count = len(tables)
        query_time = (time.perf_counter() - start_time) * 1000
        times.append(query_time)

    avg_time = sum(times) / len(times)
    print(f"   ì „ì²´ í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ({tables_count}ê°œ í…Œì´ë¸”):")
    print(f"      í‰ê· : {avg_time:.2f}ms, ë²”ìœ„: {min(times):.2f}-{max(times):.2f}ms")

    # 2. ìº”ë“¤ í…Œì´ë¸”ë§Œ ì¡°íšŒ
    times = []
    candle_tables_count = 0
    for i in range(10):
        start_time = time.perf_counter()
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'candles_%'")
        candle_tables = cursor.fetchall()
        candle_tables_count = len(candle_tables)
        query_time = (time.perf_counter() - start_time) * 1000
        times.append(query_time)

    avg_time = sum(times) / len(times)
    print(f"   ìº”ë“¤ í…Œì´ë¸”ë§Œ ì¡°íšŒ ({candle_tables_count}ê°œ í…Œì´ë¸”):")
    print(f"      í‰ê· : {avg_time:.2f}ms, ë²”ìœ„: {min(times):.2f}-{max(times):.2f}ms")

    # 3. íŠ¹ì • í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    times = []
    test_table = "candles_KRW_BTC_1m"
    for i in range(100):
        start_time = time.perf_counter()
        cursor.execute("SELECT 1 FROM sqlite_master WHERE type='table' AND name = ?", (test_table,))
        cursor.fetchone()
        query_time = (time.perf_counter() - start_time) * 1000
        times.append(query_time)

    avg_time = sum(times) / len(times)
    print("   í…Œì´ë¸” ì¡´ì¬ í™•ì¸ (100íšŒ):")
    print(f"      í‰ê· : {avg_time:.3f}ms")


def measure_data_query_performance(conn: sqlite3.Connection):
    """ë°ì´í„° ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
    cursor = conn.cursor()

    # 1. ë‹¨ì¼ í…Œì´ë¸” ë°ì´í„° ì¡°íšŒ
    table_name = "candles_KRW_BTC_1m"
    times = []
    rows_count = 0
    for i in range(20):
        start_time = time.perf_counter()
        cursor.execute(f"SELECT * FROM {table_name} ORDER BY candle_date_time_utc DESC LIMIT 5")
        rows = cursor.fetchall()
        rows_count = len(rows)
        query_time = (time.perf_counter() - start_time) * 1000
        times.append(query_time)

    avg_time = sum(times) / len(times)
    print(f"   ë‹¨ì¼ í…Œì´ë¸” ë°ì´í„° ì¡°íšŒ ({rows_count}ê°œ ë ˆì½”ë“œ):")
    print(f"      í‰ê· : {avg_time:.3f}ms")

    # 2. COUNT ì¿¼ë¦¬
    times = []
    record_count = 0
    for i in range(10):
        start_time = time.perf_counter()
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        record_count = cursor.fetchone()[0]
        query_time = (time.perf_counter() - start_time) * 1000
        times.append(query_time)

    avg_time = sum(times) / len(times)
    print(f"   COUNT ì¿¼ë¦¬ ({record_count}ê°œ ë ˆì½”ë“œ):")
    print(f"      í‰ê· : {avg_time:.3f}ms")


def measure_complex_query_performance(conn: sqlite3.Connection):
    """ë³µì¡í•œ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì •"""
    cursor = conn.cursor()

    # 1. ì—¬ëŸ¬ í…Œì´ë¸” ì¡´ì¬ í™•ì¸
    test_tables = [
        "candles_KRW_BTC_1m", "candles_KRW_ETH_1m", "candles_KRW_XRP_1m",
        "candles_BTC_ETH_1h", "candles_USDT_BTC_1d"
    ]

    times = []
    existing_count = 0
    for i in range(10):
        start_time = time.perf_counter()
        placeholders = ','.join(['?' for _ in test_tables])
        cursor.execute(f"""
            SELECT name FROM sqlite_master
            WHERE type='table' AND name IN ({placeholders})
        """, test_tables)
        existing_tables = cursor.fetchall()
        existing_count = len(existing_tables)
        query_time = (time.perf_counter() - start_time) * 1000
        times.append(query_time)

    avg_time = sum(times) / len(times)
    print(f"   ë³µìˆ˜ í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ({existing_count}/{len(test_tables)}ê°œ):")
    print(f"      í‰ê· : {avg_time:.3f}ms")

    # 2. í…Œì´ë¸”ë³„ ë ˆì½”ë“œ ìˆ˜ ì§‘ê³„
    start_time = time.perf_counter()
    cursor.execute("""
        SELECT name FROM sqlite_master
        WHERE type='table' AND name LIKE 'candles_KRW_%_1m'
        LIMIT 10
    """)
    krw_1m_tables = cursor.fetchall()

    total_records = 0
    for table_row in krw_1m_tables:
        table_name = table_row[0]
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count = cursor.fetchone()[0]
        total_records += count

    query_time = (time.perf_counter() - start_time) * 1000
    print(f"   KRW 1ë¶„ë´‰ í…Œì´ë¸” ì§‘ê³„ ({len(krw_1m_tables)}ê°œ í…Œì´ë¸”, {total_records}ê°œ ë ˆì½”ë“œ):")
    print(f"      ì´ ì‹œê°„: {query_time:.2f}ms")


def measure_sync_performance(conn: sqlite3.Connection):
    """DB ë™ê¸°í™” ì„±ëŠ¥ ì¸¡ì • (íŒŒì¼ DB ì „ìš©)"""
    cursor = conn.cursor()

    # 1. ìˆ˜ë™ COMMIT ì„±ëŠ¥
    start_time = time.perf_counter()
    cursor.execute("BEGIN TRANSACTION")
    cursor.execute("CREATE TEMP TABLE test_sync_table (id INTEGER, data TEXT)")
    cursor.execute("INSERT INTO test_sync_table VALUES (1, 'test')")
    cursor.execute("COMMIT")
    sync_time = (time.perf_counter() - start_time) * 1000

    print(f"   ìˆ˜ë™ íŠ¸ëœì­ì…˜ COMMIT: {sync_time:.3f}ms")

    # 2. PRAGMA synchronous ì„¤ì • í™•ì¸
    cursor.execute("PRAGMA synchronous")
    sync_mode = cursor.fetchone()[0]
    sync_modes = {0: "OFF", 1: "NORMAL", 2: "FULL", 3: "EXTRA"}
    print(f"   ë™ê¸°í™” ëª¨ë“œ: {sync_modes.get(sync_mode, sync_mode)}")

    # 3. VACUUM ì„±ëŠ¥ (ì†Œê·œëª¨)
    start_time = time.perf_counter()
    cursor.execute("VACUUM")
    vacuum_time = (time.perf_counter() - start_time) * 1000
    print(f"   VACUUM ì‹¤í–‰: {vacuum_time:.2f}ms")

    # 4. í˜ì´ì§€ ìºì‹œ ì •ë³´
    cursor.execute("PRAGMA cache_size")
    cache_size = cursor.fetchone()[0]
    print(f"   í˜ì´ì§€ ìºì‹œ í¬ê¸°: {cache_size}ê°œ í˜ì´ì§€")


def cleanup_test_db(db_path: str):
    """í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì •ë¦¬"""
    try:
        if os.path.exists(db_path):
            os.remove(db_path)
            print(f"   âœ… í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {db_path}")
        else:
            print(f"   âš ï¸  í…ŒìŠ¤íŠ¸ DB íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {db_path}")
    except Exception as e:
        print(f"   âŒ í…ŒìŠ¤íŠ¸ DB íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")


def analyze_performance_impact(baseline: float, scaled: float, initial_tables: int, final_tables: int):
    """ì„±ëŠ¥ ì˜í–¥ ë¶„ì„"""
    if baseline > 0:
        percent_change = ((scaled - baseline) / baseline) * 100
    else:
        percent_change = 0

    print(f"   ê¸°ì¤€ ì‹œê°„: {baseline:.3f}ms ({initial_tables}ê°œ í…Œì´ë¸”)")
    print(f"   í™•ì¥ ì‹œê°„: {scaled:.3f}ms ({final_tables}ê°œ í…Œì´ë¸”)")
    print(f"   ì„±ëŠ¥ ë³€í™”: {scaled - baseline:+.3f}ms ({percent_change:+.1f}%)")
    print(f"   í…Œì´ë¸” ì¦ê°€: +{final_tables - initial_tables}ê°œ")

    if abs(percent_change) < 10:
        print("   âœ… ì„±ëŠ¥ ì˜í–¥ ë¯¸ë¯¸ (10% ì´í•˜)")
        scale_rating = "ìš°ìˆ˜"
    elif abs(percent_change) < 25:
        print("   âš ï¸  ê²½ë¯¸í•œ ì„±ëŠ¥ ë³€í™” (10-25%)")
        scale_rating = "ì–‘í˜¸"
    elif abs(percent_change) < 50:
        print("   âš ï¸  ì¤‘ê°„ ì„±ëŠ¥ ë³€í™” (25-50%)")
        scale_rating = "ë³´í†µ"
    else:
        print("   âŒ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ë³€í™” (50% ì´ìƒ)")
        scale_rating = "ê°œì„  í•„ìš”"

    print(f"   ëŒ€ëŸ‰ í…Œì´ë¸” í™•ì¥ì„±: {scale_rating}")

    # ì˜ˆìƒ ì„±ëŠ¥ ì¶”ì •
    if final_tables > 0:
        per_table_impact = percent_change / final_tables
        projected_10k = baseline * (1 + (per_table_impact * 10000 / 100))
        print(f"   10,000ê°œ í…Œì´ë¸” ì˜ˆìƒ: {projected_10k:.3f}ms")


if __name__ == "__main__":
    test_db_table_scaling()
