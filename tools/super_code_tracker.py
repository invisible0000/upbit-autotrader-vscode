#!/usr/bin/env python3
"""
ğŸ” Super Code Tracker
ê°œë°œ ì¶”ì  ë° ê¸°ëŠ¥ ë°œê²¬ ë„êµ¬ - DDD ê³„ì¸µë³„ ê¸°ëŠ¥ ë¶„ì„ ë° ì¤‘ë³µ ë°©ì§€

ğŸ¤– LLM ì‚¬ìš© ê°€ì´ë“œ:
===================
ì´ ë„êµ¬ëŠ” ê°œë°œ ì¤‘ ê¸°ì¡´ ê¸°ëŠ¥ ë°œê²¬ê³¼ ì½”ë“œ ì¶”ì ì„ ìœ„í•œ ì „ë¬¸ ë„êµ¬ì…ë‹ˆë‹¤.

ğŸ“‹ ì£¼ìš” ëª…ë ¹ì–´ (tools í´ë”ì—ì„œ ì‹¤í–‰):
1. python super_code_tracker.py --feature-discovery <í‚¤ì›Œë“œ>      # ê¸°ëŠ¥ ë¹ ë¥¸ íƒì§€ â­
2. python super_code_tracker.py --layer-analysis                  # DDD ê³„ì¸µë³„ ê¸°ëŠ¥ ë¶„í¬ â­
3. python super_code_tracker.py --duplicate-detection             # ì¤‘ë³µ ê¸°ëŠ¥ íƒì§€
4. python super_code_tracker.py --feature-map                     # ê¸°ëŠ¥ë³„ ì½”ë“œ ìœ„ì¹˜ ë§¤í•‘
5. python super_code_tracker.py --quick-search <í‚¤ì›Œë“œ>           # í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ê²€ìƒ‰
6. python super_code_tracker.py --unused-cleanup                  # ë¯¸ì‚¬ìš© ì½”ë“œ ì •ë¦¬ ì œì•ˆ

ğŸ¯ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤:
- ğŸ” ìƒˆ ê¸°ëŠ¥ ê°œë°œ ì „: --feature-discovery "trading strategy" (ì¤‘ë³µ ê°œë°œ ë°©ì§€)
- ğŸ“Š ì•„í‚¤í…ì²˜ ì ê²€: --layer-analysis (DDD ì¤€ìˆ˜ í™•ì¸)
- ğŸ§¹ ì½”ë“œ ì •ë¦¬: --unused-cleanup (ë¯¸ì‚¬ìš© íŒŒì¼ ì œê±°)
- ğŸ—ºï¸ ì½”ë“œ ë„¤ë¹„ê²Œì´ì…˜: --feature-map (ê¸°ëŠ¥ ìœ„ì¹˜ íŒŒì•…)

ğŸ’¡ ê¸°ì¡´ ë„êµ¬ í†µí•©:
- super_db_table_reference_code_analyzer.py â†’ í…Œì´ë¸” ì°¸ì¡° ì¶”ì 
- super_file_dependency_analyzer.py â†’ ì˜ì¡´ì„± ë¶„ì„
- super_files_unused_detector.py â†’ ë¯¸ì‚¬ìš© ì½”ë“œ íƒì§€
- super_import_tracker.py â†’ import ê´€ê³„ ì¶”ì 

ì‘ì„±ì¼: 2025-08-16
ì‘ì„±ì: Upbit Auto Trading Team
"""

import re
import ast
import time
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass
from collections import defaultdict
import argparse
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ íŒ¨ìŠ¤ì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


@dataclass
class FeatureInfo:
    """ê¸°ëŠ¥ ì •ë³´"""
    name: str
    file_path: str
    layer: str  # Domain, Application, Infrastructure, Presentation
    function_count: int
    class_count: int
    line_count: int
    dependencies: List[str]
    description: str = ""


@dataclass
class DuplicatePattern:
    """ì¤‘ë³µ íŒ¨í„´ ì •ë³´"""
    pattern_type: str  # 'function', 'class', 'logic'
    similarity_score: float
    locations: List[str]
    suggested_action: str


@dataclass
class LayerStats:
    """ê³„ì¸µë³„ í†µê³„"""
    layer_name: str
    file_count: int
    function_count: int
    class_count: int
    total_lines: int
    main_features: List[str]


class SuperCodeTracker:
    """
    ğŸ” ê°œë°œ ì¶”ì  ë° ê¸°ëŠ¥ ë°œê²¬ ë„êµ¬

    ì£¼ìš” ê¸°ëŠ¥:
    1. ê¸°ì¡´ ê¸°ëŠ¥ ë¹ ë¥¸ íƒì§€
    2. DDD ê³„ì¸µë³„ ë¶„ì„
    3. ì¤‘ë³µ ê¸°ëŠ¥ íƒì§€
    4. ì½”ë“œ ìœ„ì¹˜ ë§¤í•‘
    5. ë¯¸ì‚¬ìš© ì½”ë“œ ì •ë¦¬
    """

    def __init__(self):
        self.project_root = PROJECT_ROOT
        self.upbit_path = self.project_root / "upbit_auto_trading"
        self.start_time = time.time()

        # DDD ê³„ì¸µ ì •ì˜
        self.layer_mapping = {
            'domain': 'upbit_auto_trading/domain',
            'application': 'upbit_auto_trading/application',
            'infrastructure': 'upbit_auto_trading/infrastructure',
            'presentation': 'upbit_auto_trading/ui'
        }

        # ìºì‹œ
        self._file_cache = {}
        self._ast_cache = {}

    def feature_discovery(self, keyword: str) -> None:
        """ğŸ” ê¸°ëŠ¥ ë¹ ë¥¸ íƒì§€"""
        print(f"ğŸ” === ê¸°ëŠ¥ íƒì§€: '{keyword}' ===\n")

        if not keyword or len(keyword) < 2:
            print("âŒ í‚¤ì›Œë“œëŠ” 2ê¸€ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return

        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
        search_patterns = [
            keyword.lower(),
            keyword.replace(' ', '_').lower(),
            keyword.replace(' ', '').lower(),
            keyword.upper(),
            keyword.title().replace(' ', '')
        ]

        findings = []
        processed_files = set()

        # Python íŒŒì¼ ê²€ìƒ‰
        for py_file in self.upbit_path.rglob("*.py"):
            if py_file in processed_files:
                continue
            processed_files.add(py_file)

            try:
                content = self._get_file_content(py_file)
                layer = self._get_file_layer(py_file)

                # íŒŒì¼ëª…ì—ì„œ ê²€ìƒ‰
                filename_matches = any(pattern in py_file.stem.lower() for pattern in search_patterns)

                # ë‚´ìš©ì—ì„œ ê²€ìƒ‰
                content_matches = []
                for line_num, line in enumerate(content.split('\n'), 1):
                    for pattern in search_patterns:
                        if pattern in line.lower():
                            content_matches.append((line_num, line.strip()))

                if filename_matches or content_matches:
                    # AST ë¶„ì„ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
                    functions, classes = self._analyze_ast(py_file)

                    feature = FeatureInfo(
                        name=py_file.stem,
                        file_path=str(py_file.relative_to(self.project_root)),
                        layer=layer,
                        function_count=len(functions),
                        class_count=len(classes),
                        line_count=len(content.split('\n')),
                        dependencies=self._get_file_dependencies(py_file),
                        description=self._extract_description(content)
                    )

                    findings.append((feature, filename_matches, content_matches))

            except Exception:
                continue

        # ê²°ê³¼ ì¶œë ¥
        if not findings:
            print(f"âŒ '{keyword}' ê´€ë ¨ ê¸°ëŠ¥ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
            return

        print(f"ğŸ¯ '{keyword}' ê´€ë ¨ ê¸°ëŠ¥ {len(findings)}ê°œ ë°œê²¬:")

        # ê³„ì¸µë³„ ê·¸ë£¹í™”
        by_layer = defaultdict(list)
        for feature, filename_match, content_matches in findings:
            by_layer[feature.layer].append((feature, filename_match, content_matches))

        for layer, layer_findings in by_layer.items():
            print(f"\nğŸ“ {layer.upper()} ê³„ì¸µ ({len(layer_findings)}ê°œ):")

            for feature, filename_match, content_matches in layer_findings:
                match_type = "ğŸ“" if filename_match else "ğŸ“"
                print(f"   {match_type} {feature.name}")
                print(f"      ğŸ“‚ {feature.file_path}")
                print(f"      ğŸ“Š í•¨ìˆ˜: {feature.function_count}ê°œ, í´ë˜ìŠ¤: {feature.class_count}ê°œ, ë¼ì¸: {feature.line_count}ê°œ")

                if feature.description:
                    print(f"      ğŸ“ {feature.description[:100]}...")

                if content_matches:
                    print(f"      ğŸ” ë§¤ì¹­ ë¼ì¸: {len(content_matches)}ê°œ")
                    for line_num, line in content_matches[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                        print(f"         L{line_num}: {line[:60]}...")
                    if len(content_matches) > 3:
                        print(f"         ... ì™¸ {len(content_matches) - 3}ê°œ")
                print()

        # ì¤‘ë³µ ê°€ëŠ¥ì„± ê²€ì‚¬
        similar_features = self._find_similar_features([f[0] for f in findings])
        if similar_features:
            print("âš ï¸ ìœ ì‚¬í•œ ê¸°ëŠ¥ì´ ì´ë¯¸ ì¡´ì¬í•  ê°€ëŠ¥ì„±:")
            for feature1, feature2, similarity in similar_features:
                print(f"   â€¢ {feature1.name} â†” {feature2.name} (ìœ ì‚¬ë„: {similarity:.0f}%)")

    def layer_analysis(self) -> None:
        """ğŸ“Š DDD ê³„ì¸µë³„ ê¸°ëŠ¥ ë¶„í¬ ë¶„ì„"""
        print("ğŸ“Š === DDD ê³„ì¸µë³„ ê¸°ëŠ¥ ë¶„í¬ ë¶„ì„ ===\n")

        layer_stats = {}

        for layer_name, layer_path in self.layer_mapping.items():
            full_path = self.project_root / layer_path

            if not full_path.exists():
                continue

            print(f"ğŸ” {layer_name.upper()} ê³„ì¸µ ë¶„ì„ ì¤‘...")

            py_files = list(full_path.rglob("*.py"))
            total_functions = 0
            total_classes = 0
            total_lines = 0
            main_features = []

            for py_file in py_files:
                try:
                    content = self._get_file_content(py_file)
                    functions, classes = self._analyze_ast(py_file)

                    total_functions += len(functions)
                    total_classes += len(classes)
                    total_lines += len(content.split('\n'))

                    # ì£¼ìš” ê¸°ëŠ¥ ì‹ë³„ (íŒŒì¼ëª… ê¸°ì¤€)
                    feature_name = py_file.stem
                    if not feature_name.startswith('__') and len(functions) + len(classes) > 2:
                        main_features.append(feature_name)

                except Exception:
                    continue

            layer_stats[layer_name] = LayerStats(
                layer_name=layer_name,
                file_count=len(py_files),
                function_count=total_functions,
                class_count=total_classes,
                total_lines=total_lines,
                main_features=main_features[:10]  # ìƒìœ„ 10ê°œ
            )

            stats = layer_stats[layer_name]
            print(f"   ğŸ“ íŒŒì¼: {stats.file_count}ê°œ")
            print(f"   ğŸ”§ í•¨ìˆ˜: {stats.function_count}ê°œ")
            print(f"   ğŸ—ï¸ í´ë˜ìŠ¤: {stats.class_count}ê°œ")
            print(f"   ğŸ“ ì´ ë¼ì¸: {stats.total_lines:,}ê°œ")
            print(f"   ğŸ¯ ì£¼ìš” ê¸°ëŠ¥: {len(stats.main_features)}ê°œ")
            print()

        # ê³„ì¸µë³„ ë¹„êµ ë¶„ì„
        total_files = sum(stats.file_count for stats in layer_stats.values())
        total_functions = sum(stats.function_count for stats in layer_stats.values())

        print("ğŸ“ˆ === ê³„ì¸µë³„ ë¹„êµ ===")
        for layer_name, stats in layer_stats.items():
            file_ratio = (stats.file_count / total_files * 100) if total_files > 0 else 0
            func_ratio = (stats.function_count / total_functions * 100) if total_functions > 0 else 0

            print(f"ğŸ—ï¸ {layer_name.upper()}:")
            print(f"   ğŸ“ íŒŒì¼ ë¹„ìœ¨: {file_ratio:.1f}%")
            print(f"   ğŸ”§ í•¨ìˆ˜ ë¹„ìœ¨: {func_ratio:.1f}%")
            print(f"   ğŸ“Š íŒŒì¼ë‹¹ í‰ê·  í•¨ìˆ˜: {stats.function_count / max(stats.file_count, 1):.1f}ê°œ")

            # ì£¼ìš” ê¸°ëŠ¥ ë‚˜ì—´
            if stats.main_features:
                features_text = ", ".join(stats.main_features[:5])
                if len(stats.main_features) > 5:
                    features_text += f" ì™¸ {len(stats.main_features) - 5}ê°œ"
                print(f"   ğŸ¯ ì£¼ìš” ê¸°ëŠ¥: {features_text}")
            print()

        # DDD ì›ì¹™ ì¤€ìˆ˜ í‰ê°€
        self._evaluate_ddd_compliance(layer_stats)

    def duplicate_detection(self) -> None:
        """ğŸ”„ ì¤‘ë³µ ê¸°ëŠ¥ íƒì§€"""
        print("ğŸ”„ === ì¤‘ë³µ ê¸°ëŠ¥ íƒì§€ ===\n")

        # í•¨ìˆ˜ëª… ê¸°ë°˜ ì¤‘ë³µ ê²€ì‚¬
        function_names = defaultdict(list)
        class_names = defaultdict(list)

        for py_file in self.upbit_path.rglob("*.py"):
            try:
                functions, classes = self._analyze_ast(py_file)

                for func_name in functions:
                    function_names[func_name].append(str(py_file.relative_to(self.project_root)))

                for class_name in classes:
                    class_names[class_name].append(str(py_file.relative_to(self.project_root)))

            except Exception:
                continue

        # ì¤‘ë³µ í•¨ìˆ˜ ì°¾ê¸°
        duplicate_functions = {name: files for name, files in function_names.items() if len(files) > 1}
        duplicate_classes = {name: files for name, files in class_names.items() if len(files) > 1}

        print(f"ğŸ”§ ì¤‘ë³µ í•¨ìˆ˜: {len(duplicate_functions)}ê°œ")
        for func_name, files in list(duplicate_functions.items())[:10]:  # ìƒìœ„ 10ê°œ
            print(f"   â€¢ {func_name}:")
            for file_path in files:
                layer = self._get_file_layer(self.project_root / file_path)
                print(f"     ğŸ“ {layer}: {file_path}")
            print()

        print(f"ğŸ—ï¸ ì¤‘ë³µ í´ë˜ìŠ¤: {len(duplicate_classes)}ê°œ")
        for class_name, files in list(duplicate_classes.items())[:10]:  # ìƒìœ„ 10ê°œ
            print(f"   â€¢ {class_name}:")
            for file_path in files:
                layer = self._get_file_layer(self.project_root / file_path)
                print(f"     ğŸ“ {layer}: {file_path}")
            print()

        # ë¡œì§ ìœ ì‚¬ì„± ê²€ì‚¬ (ê°„ë‹¨ ë²„ì „)
        print("ğŸ§  ë¡œì§ ìœ ì‚¬ì„± ê²€ì‚¬:")
        similar_patterns = self._detect_similar_logic()

        if similar_patterns:
            for pattern in similar_patterns[:5]:  # ìƒìœ„ 5ê°œ
                print(f"   ğŸ” {pattern.pattern_type}: {pattern.similarity_score:.0f}% ìœ ì‚¬")
                for location in pattern.locations:
                    print(f"     ğŸ“ {location}")
                print(f"   ğŸ’¡ ì œì•ˆ: {pattern.suggested_action}")
                print()
        else:
            print("   âœ… ìœ ì‚¬í•œ ë¡œì§ íŒ¨í„´ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def feature_map(self) -> None:
        """ğŸ—ºï¸ ê¸°ëŠ¥ë³„ ì½”ë“œ ìœ„ì¹˜ ë§¤í•‘"""
        print("ğŸ—ºï¸ === ê¸°ëŠ¥ë³„ ì½”ë“œ ìœ„ì¹˜ ë§¤í•‘ ===\n")

        feature_map = defaultdict(lambda: defaultdict(list))

        # ê¸°ëŠ¥ ì¹´í…Œê³ ë¦¬ ì •ì˜
        feature_categories = {
            'trading': ['trade', 'order', 'buy', 'sell', 'strategy'],
            'indicator': ['rsi', 'sma', 'ema', 'bollinger', 'macd'],
            'data': ['market', 'price', 'candle', 'volume'],
            'ui': ['window', 'widget', 'view', 'presenter'],
            'config': ['config', 'setting', 'parameter'],
            'database': ['db', 'sqlite', 'repository', 'dao'],
            'api': ['upbit', 'request', 'response', 'client'],
            'validation': ['validate', 'check', 'verify'],
            'logging': ['log', 'logger', 'debug'],
            'util': ['util', 'helper', 'tool', 'common']
        }

        # íŒŒì¼ ë¶„ë¥˜
        for py_file in self.upbit_path.rglob("*.py"):
            try:
                content = self._get_file_content(py_file)
                file_path = str(py_file.relative_to(self.project_root))
                layer = self._get_file_layer(py_file)

                # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ë¥˜
                for category, keywords in feature_categories.items():
                    for keyword in keywords:
                        if (keyword in py_file.stem.lower() or
                            keyword in content.lower()):

                            functions, classes = self._analyze_ast(py_file)

                            feature_map[category][layer].append({
                                'file': file_path,
                                'functions': len(functions),
                                'classes': len(classes),
                                'lines': len(content.split('\n'))
                            })
                            break  # ì²« ë²ˆì§¸ ë§¤ì¹­ìœ¼ë¡œ ë¶„ë¥˜

            except Exception:
                continue

        # ê²°ê³¼ ì¶œë ¥
        for category, layers in feature_map.items():
            total_files = sum(len(files) for files in layers.values())
            if total_files == 0:
                continue

            print(f"ğŸ¯ {category.upper()} ê¸°ëŠ¥ ({total_files}ê°œ íŒŒì¼):")

            for layer, files in layers.items():
                if not files:
                    continue

                total_functions = sum(f['functions'] for f in files)
                total_classes = sum(f['classes'] for f in files)

                print(f"   ğŸ“ {layer}: {len(files)}ê°œ íŒŒì¼")
                print(f"      ğŸ”§ í•¨ìˆ˜: {total_functions}ê°œ, í´ë˜ìŠ¤: {total_classes}ê°œ")

                # ì£¼ìš” íŒŒì¼ ë‚˜ì—´
                top_files = sorted(files, key=lambda x: x['functions'] + x['classes'], reverse=True)[:3]
                for file_info in top_files:
                    print(f"      ğŸ“„ {Path(file_info['file']).name}: {file_info['functions']}í•¨ìˆ˜, {file_info['classes']}í´ë˜ìŠ¤")
            print()

    def quick_search(self, keyword: str) -> None:
        """âš¡ í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ê²€ìƒ‰"""
        print(f"âš¡ === ë¹ ë¥¸ ê²€ìƒ‰: '{keyword}' ===\n")

        if not keyword or len(keyword) < 2:
            print("âŒ í‚¤ì›Œë“œëŠ” 2ê¸€ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
            return

        matches = []
        search_time = time.time()

        # íŒŒì¼ëª…, í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª…ì—ì„œ ê²€ìƒ‰
        for py_file in self.upbit_path.rglob("*.py"):
            try:
                # íŒŒì¼ëª… ê²€ìƒ‰
                if keyword.lower() in py_file.stem.lower():
                    matches.append(('file', str(py_file.relative_to(self.project_root)), py_file.stem))

                # í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ê²€ìƒ‰
                functions, classes = self._analyze_ast(py_file)

                for func_name in functions:
                    if keyword.lower() in func_name.lower():
                        matches.append(('function', str(py_file.relative_to(self.project_root)), func_name))

                for class_name in classes:
                    if keyword.lower() in class_name.lower():
                        matches.append(('class', str(py_file.relative_to(self.project_root)), class_name))

            except Exception:
                continue

        search_elapsed = time.time() - search_time

        if not matches:
            print(f"âŒ '{keyword}' ê´€ë ¨ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"ğŸ¯ {len(matches)}ê°œ í•­ëª© ë°œê²¬ (ê²€ìƒ‰ ì‹œê°„: {search_elapsed:.2f}ì´ˆ):")

        # íƒ€ì…ë³„ ê·¸ë£¹í™”
        by_type = defaultdict(list)
        for match_type, file_path, name in matches:
            by_type[match_type].append((file_path, name))

        for match_type, items in by_type.items():
            type_emoji = {'file': 'ğŸ“', 'function': 'ğŸ”§', 'class': 'ğŸ—ï¸'}
            print(f"\n{type_emoji[match_type]} {match_type.upper()} ({len(items)}ê°œ):")

            for file_path, name in items[:10]:  # ìƒìœ„ 10ê°œ
                layer = self._get_file_layer(self.project_root / file_path)
                print(f"   â€¢ {name} ({layer})")
                print(f"     ğŸ“‚ {file_path}")

            if len(items) > 10:
                print(f"   ... ì™¸ {len(items) - 10}ê°œ")

    def unused_cleanup(self) -> None:
        """ğŸ§¹ ë¯¸ì‚¬ìš© ì½”ë“œ ì •ë¦¬ ì œì•ˆ"""
        print("ğŸ§¹ === ë¯¸ì‚¬ìš© ì½”ë“œ ì •ë¦¬ ì œì•ˆ ===\n")

        # ëª¨ë“  Python íŒŒì¼ì˜ import ê´€ê³„ ë¶„ì„
        all_imports = set()
        all_modules = set()

        for py_file in self.upbit_path.rglob("*.py"):
            try:
                content = self._get_file_content(py_file)

                # ëª¨ë“ˆëª… ì¶”ê°€
                relative_path = py_file.relative_to(self.upbit_path)
                module_name = str(relative_path.with_suffix('')).replace('/', '.').replace('\\', '.')
                all_modules.add(module_name)

                # import ë¬¸ ì¶”ì¶œ
                imports = self._extract_imports(content)
                all_imports.update(imports)

            except Exception:
                continue

        # ë¯¸ì‚¬ìš© ëª¨ë“ˆ ì°¾ê¸°
        unused_modules = []
        for module in all_modules:
            if module not in all_imports and not module.endswith('__init__'):
                module_path = self.upbit_path / f"{module.replace('.', '/')}.py"
                if module_path.exists():
                    unused_modules.append(str(module_path.relative_to(self.project_root)))

        print(f"ğŸ“ ë¯¸ì‚¬ìš© ê°€ëŠ¥ì„±ì´ ìˆëŠ” ëª¨ë“ˆ: {len(unused_modules)}ê°œ")
        for module_path in unused_modules[:15]:  # ìƒìœ„ 15ê°œ
            print(f"   ğŸ“„ {module_path}")
        if len(unused_modules) > 15:
            print(f"   ... ì™¸ {len(unused_modules) - 15}ê°œ")

        # ë¹ˆ íŒŒì¼ ì°¾ê¸°
        empty_files = []
        for py_file in self.upbit_path.rglob("*.py"):
            try:
                content = self._get_file_content(py_file)
                lines = [line.strip() for line in content.split('\n') if line.strip()]

                # ì£¼ì„ê³¼ importë§Œ ìˆëŠ” íŒŒì¼
                non_trivial_lines = [line for line in lines if not (
                    line.startswith('#') or
                    line.startswith('"""') or
                    line.startswith("'''") or
                    line.startswith('import ') or
                    line.startswith('from ')
                )]

                if len(non_trivial_lines) <= 2:  # ê±°ì˜ ë¹ˆ íŒŒì¼
                    empty_files.append(str(py_file.relative_to(self.project_root)))

            except Exception:
                continue

        print(f"\nğŸ“„ ê±°ì˜ ë¹ˆ íŒŒì¼: {len(empty_files)}ê°œ")
        for file_path in empty_files[:10]:  # ìƒìœ„ 10ê°œ
            print(f"   ğŸ“„ {file_path}")
        if len(empty_files) > 10:
            print(f"   ... ì™¸ {len(empty_files) - 10}ê°œ")

        # ì •ë¦¬ ì œì•ˆ
        print(f"\nğŸ’¡ === ì •ë¦¬ ì œì•ˆ ===")
        if unused_modules or empty_files:
            print("ğŸ—‚ï¸ ì•ˆì „í•œ ì •ë¦¬ ë‹¨ê³„:")
            print("1. ğŸ“ ë¯¸ì‚¬ìš© ëª¨ë“ˆì„ legacy í´ë”ë¡œ ì´ë™")
            print("2. ğŸ“„ ë¹ˆ íŒŒì¼ë“¤ì„ í†µí•© ë˜ëŠ” ì œê±°")
            print("3. ğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰ìœ¼ë¡œ ê²€ì¦")
            print("4. ğŸ”„ import ê´€ê³„ ì¬ê²€í† ")
        else:
            print("âœ… ì •ë¦¬ê°€ í•„ìš”í•œ íŒŒì¼ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    def _get_file_content(self, file_path: Path) -> str:
        """íŒŒì¼ ë‚´ìš© ì¡°íšŒ (ìºì‹œ ì‚¬ìš©)"""
        if file_path not in self._file_cache:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    self._file_cache[file_path] = f.read()
            except Exception:
                self._file_cache[file_path] = ""

        return self._file_cache[file_path]

    def _get_file_layer(self, file_path: Path) -> str:
        """íŒŒì¼ì˜ DDD ê³„ì¸µ íŒë‹¨"""
        try:
            relative_path = file_path.relative_to(self.project_root)
            path_str = str(relative_path)

            if 'domain' in path_str:
                return 'domain'
            elif 'application' in path_str:
                return 'application'
            elif 'infrastructure' in path_str:
                return 'infrastructure'
            elif 'ui' in path_str or 'presentation' in path_str:
                return 'presentation'
            else:
                return 'other'
        except Exception:
            return 'unknown'

    def _analyze_ast(self, file_path: Path) -> Tuple[List[str], List[str]]:
        """AST ë¶„ì„ìœ¼ë¡œ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ì¶”ì¶œ (ìºì‹œ ì‚¬ìš©)"""
        if file_path not in self._ast_cache:
            functions = []
            classes = []

            try:
                content = self._get_file_content(file_path)
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        functions.append(node.name)
                    elif isinstance(node, ast.ClassDef):
                        classes.append(node.name)

                self._ast_cache[file_path] = (functions, classes)
            except Exception:
                self._ast_cache[file_path] = ([], [])

        return self._ast_cache[file_path]

    def _get_file_dependencies(self, file_path: Path) -> List[str]:
        """íŒŒì¼ì˜ ì˜ì¡´ì„± ì¶”ì¶œ"""
        try:
            content = self._get_file_content(file_path)
            return self._extract_imports(content)
        except Exception:
            return []

    def _extract_imports(self, content: str) -> List[str]:
        """import ë¬¸ ì¶”ì¶œ"""
        imports = []

        try:
            tree = ast.parse(content)
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)
        except Exception:
            # ì •ê·œì‹ fallback
            import_patterns = [
                r'import\s+([a-zA-Z_][a-zA-Z0-9_.]*)',
                r'from\s+([a-zA-Z_][a-zA-Z0-9_.]*)\s+import'
            ]

            for pattern in import_patterns:
                matches = re.findall(pattern, content)
                imports.extend(matches)

        return imports

    def _extract_description(self, content: str) -> str:
        """íŒŒì¼ ì„¤ëª… ì¶”ì¶œ"""
        lines = content.split('\n')

        # docstring ì°¾ê¸°
        for i, line in enumerate(lines[:20]):  # ìƒìœ„ 20ì¤„ë§Œ ê²€ì‚¬
            if '"""' in line or "'''" in line:
                # ë‹¤ìŒ ì¤„ë“¤ì—ì„œ ì„¤ëª… ì°¾ê¸°
                for j in range(i + 1, min(i + 10, len(lines))):
                    desc_line = lines[j].strip()
                    if desc_line and not desc_line.startswith('"""') and not desc_line.startswith("'''"):
                        return desc_line

        # ì£¼ì„ì—ì„œ ì°¾ê¸°
        for line in lines[:10]:
            if line.strip().startswith('#') and len(line.strip()) > 5:
                return line.strip()[1:].strip()

        return ""

    def _find_similar_features(self, features: List[FeatureInfo]) -> List[Tuple[FeatureInfo, FeatureInfo, float]]:
        """ìœ ì‚¬í•œ ê¸°ëŠ¥ ì°¾ê¸°"""
        similar_pairs = []

        for i, feature1 in enumerate(features):
            for feature2 in features[i+1:]:
                # ì´ë¦„ ìœ ì‚¬ë„
                name_similarity = self._calculate_string_similarity(feature1.name, feature2.name)

                # êµ¬ì¡° ìœ ì‚¬ë„
                func_diff = abs(feature1.function_count - feature2.function_count)
                class_diff = abs(feature1.class_count - feature2.class_count)
                structure_similarity = 100 - min(100, (func_diff + class_diff) * 10)

                # ì „ì²´ ìœ ì‚¬ë„
                overall_similarity = (name_similarity + structure_similarity) / 2

                if overall_similarity > 70:  # 70% ì´ìƒ ìœ ì‚¬
                    similar_pairs.append((feature1, feature2, overall_similarity))

        return sorted(similar_pairs, key=lambda x: x[2], reverse=True)

    def _calculate_string_similarity(self, str1: str, str2: str) -> float:
        """ë¬¸ìì—´ ìœ ì‚¬ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        if not str1 or not str2:
            return 0

        # ê³µí†µ ë¶€ë¶„ ë¬¸ìì—´ ë¹„ìœ¨
        common_chars = set(str1.lower()) & set(str2.lower())
        total_chars = set(str1.lower()) | set(str2.lower())

        if not total_chars:
            return 0

        return (len(common_chars) / len(total_chars)) * 100

    def _detect_similar_logic(self) -> List[DuplicatePattern]:
        """ìœ ì‚¬í•œ ë¡œì§ íŒ¨í„´ íƒì§€ (ê°„ë‹¨í•œ ë²„ì „)"""
        patterns = []

        # í•¨ìˆ˜ ê¸¸ì´ ê¸°ë°˜ ìœ ì‚¬ì„± ê²€ì‚¬
        function_lengths = defaultdict(list)

        for py_file in self.upbit_path.rglob("*.py"):
            try:
                content = self._get_file_content(py_file)
                tree = ast.parse(content)

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        # í•¨ìˆ˜ ê¸¸ì´ ê³„ì‚°
                        func_lines = node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 10
                        function_lengths[func_lines].append(f"{py_file.stem}.{node.name}")

            except Exception:
                continue

        # ë™ì¼í•œ ê¸¸ì´ì˜ í•¨ìˆ˜ë“¤ ì¤‘ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ê²ƒë“¤
        for length, functions in function_lengths.items():
            if length > 10 and len(functions) > 1:  # 10ì¤„ ì´ìƒ, 2ê°œ ì´ìƒ
                patterns.append(DuplicatePattern(
                    pattern_type=f"{length}ì¤„ í•¨ìˆ˜ íŒ¨í„´",
                    similarity_score=80.0,
                    locations=functions,
                    suggested_action="í•¨ìˆ˜ ë‚´ìš©ì„ ë¹„êµí•˜ì—¬ ê³µí†µ ë¡œì§ ì¶”ì¶œ ê²€í† "
                ))

        return patterns

    def _evaluate_ddd_compliance(self, layer_stats: Dict[str, LayerStats]) -> None:
        """DDD ì›ì¹™ ì¤€ìˆ˜ í‰ê°€"""
        print("ğŸ—ï¸ === DDD ì›ì¹™ ì¤€ìˆ˜ í‰ê°€ ===")

        total_files = sum(stats.file_count for stats in layer_stats.values())

        # ê³„ì¸µë³„ ê· í˜• ê²€ì‚¬
        domain_ratio = layer_stats.get('domain', LayerStats('domain', 0, 0, 0, 0, [])).file_count / max(total_files, 1) * 100
        infra_ratio = layer_stats.get('infrastructure', LayerStats('infrastructure', 0, 0, 0, 0, [])).file_count / max(total_files, 1) * 100

        print(f"ğŸ“Š Domain ê³„ì¸µ ë¹„ìœ¨: {domain_ratio:.1f}%", end="")
        if domain_ratio < 20:
            print(" âš ï¸ ë„ˆë¬´ ì ìŒ (20% ì´ìƒ ê¶Œì¥)")
        elif domain_ratio > 50:
            print(" âš ï¸ ë„ˆë¬´ ë§ìŒ (50% ì´í•˜ ê¶Œì¥)")
        else:
            print(" âœ… ì ì ˆí•¨")

        print(f"ğŸ“Š Infrastructure ê³„ì¸µ ë¹„ìœ¨: {infra_ratio:.1f}%", end="")
        if infra_ratio > 40:
            print(" âš ï¸ ë„ˆë¬´ ë§ìŒ (40% ì´í•˜ ê¶Œì¥)")
        else:
            print(" âœ… ì ì ˆí•¨")

        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        balance_score = 100 - abs(domain_ratio - 30) - abs(infra_ratio - 25)  # ì´ìƒì : Domain 30%, Infra 25%
        complexity_score = min(100, 100 - (total_files / 10))  # íŒŒì¼ ìˆ˜ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ

        overall_score = (balance_score + complexity_score) / 2

        print(f"ğŸ¯ DDD ì¤€ìˆ˜ ì ìˆ˜: {overall_score:.1f}/100")
        if overall_score >= 80:
            print("   âœ… í›Œë¥­í•œ ì•„í‚¤í…ì²˜")
        elif overall_score >= 60:
            print("   ğŸŸ¡ ê°œì„  ì—¬ì§€ ìˆìŒ")
        else:
            print("   ğŸ”´ êµ¬ì¡° ê°œì„  í•„ìš”")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Super Code Tracker - ê°œë°œ ì¶”ì  ë° ê¸°ëŠ¥ ë°œê²¬ ë„êµ¬')
    parser.add_argument('--feature-discovery', type=str, help='ê¸°ëŠ¥ ë¹ ë¥¸ íƒì§€ (í‚¤ì›Œë“œ)')
    parser.add_argument('--layer-analysis', action='store_true', help='DDD ê³„ì¸µë³„ ê¸°ëŠ¥ ë¶„í¬')
    parser.add_argument('--duplicate-detection', action='store_true', help='ì¤‘ë³µ ê¸°ëŠ¥ íƒì§€')
    parser.add_argument('--feature-map', action='store_true', help='ê¸°ëŠ¥ë³„ ì½”ë“œ ìœ„ì¹˜ ë§¤í•‘')
    parser.add_argument('--quick-search', type=str, help='í‚¤ì›Œë“œ ê¸°ë°˜ ë¹ ë¥¸ ê²€ìƒ‰')
    parser.add_argument('--unused-cleanup', action='store_true', help='ë¯¸ì‚¬ìš© ì½”ë“œ ì •ë¦¬ ì œì•ˆ')

    args = parser.parse_args()

    tracker = SuperCodeTracker()

    # ì•„ë¬´ ì˜µì…˜ì´ ì—†ìœ¼ë©´ layer-analysis ì‹¤í–‰
    if not any(vars(args).values()):
        tracker.layer_analysis()
        return

    if args.feature_discovery:
        tracker.feature_discovery(args.feature_discovery)

    if args.layer_analysis:
        tracker.layer_analysis()

    if args.duplicate_detection:
        tracker.duplicate_detection()

    if args.feature_map:
        tracker.feature_map()

    if args.quick_search:
        tracker.quick_search(args.quick_search)

    if args.unused_cleanup:
        tracker.unused_cleanup()


if __name__ == "__main__":
    main()
