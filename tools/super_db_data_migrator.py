#!/usr/bin/env python3
"""
Super DB Data Migrator v1.0 - ë°ì´í„°ë² ì´ìŠ¤ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬
================================================================================
ğŸ¯ ëª©ì : DB ê°„ í…Œì´ë¸” ë°ì´í„° ë³µì‚¬, ìŠ¤í‚¤ë§ˆ ë§¤í•‘, ë°ì´í„° ë³€í™˜
ğŸ”§ ê¸°ëŠ¥: 
- í…Œì´ë¸” ê°„ ë°ì´í„° ë³µì‚¬
- ì»¬ëŸ¼ëª… ë§¤í•‘ (ì†ŒìŠ¤ â†’ íƒ€ê²Ÿ)
- ë°ì´í„° ë³€í™˜ (JSON, íƒ€ì… ë³€í™˜ ë“±)
- ì•ˆì „í•œ íŠ¸ëœì­ì…˜ ì²˜ë¦¬
- ì¤‘ë³µ ë°ì´í„° ì²˜ë¦¬ ì˜µì…˜
================================================================================
"""

import sqlite3
import json
import argparse
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
import os

class SuperDBDataMigrator:
    """ë°ì´í„°ë² ì´ìŠ¤ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬"""
    
    def __init__(self):
        self.version = "1.0"
        self.session_log = []
    
    def migrate_data(self, 
                    source_db: str, 
                    target_db: str, 
                    source_table: str, 
                    target_table: str,
                    column_mapping: Optional[Dict[str, str]] = None,
                    where_condition: Optional[str] = None,
                    skip_duplicates: bool = True,
                    dry_run: bool = False) -> Tuple[bool, str, int]:
        """
        ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
        
        Args:
            source_db: ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            target_db: íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ
            source_table: ì†ŒìŠ¤ í…Œì´ë¸”ëª…
            target_table: íƒ€ê²Ÿ í…Œì´ë¸”ëª… (ê¸°ë³¸ê°’: source_tableê³¼ ë™ì¼)
            column_mapping: ì»¬ëŸ¼ ë§¤í•‘ (ì†ŒìŠ¤ì»¬ëŸ¼ -> íƒ€ê²Ÿì»¬ëŸ¼)
            where_condition: WHERE ì¡°ê±´ (ì˜ˆ: "id > 0")
            skip_duplicates: ì¤‘ë³µ ë°ì´í„° ê±´ë„ˆë›°ê¸°
            dry_run: ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ë¯¸ë¦¬ë³´ê¸°ë§Œ
        
        Returns:
            (ì„±ê³µì—¬ë¶€, ë©”ì‹œì§€, ì²˜ë¦¬ëœ_í–‰ìˆ˜)
        """
        try:
            print(f"ğŸš€ Super DB Data Migrator v{self.version} ì‹œì‘")
            print("=" * 60)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ê²€ì¦
            if not os.path.exists(source_db):
                return False, f"ì†ŒìŠ¤ DBë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {source_db}", 0
            
            # íƒ€ê²Ÿ í…Œì´ë¸”ëª… ê¸°ë³¸ê°’ ì„¤ì •
            if not target_table:
                target_table = source_table
            
            # ì†ŒìŠ¤ ë°ì´í„° ì¡°íšŒ
            source_data = self._fetch_source_data(source_db, source_table, where_condition)
            if not source_data:
                return False, "ì†ŒìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", 0
            
            print(f"ğŸ“Š ì†ŒìŠ¤ ë°ì´í„°: {len(source_data)}ê°œ í–‰ ë°œê²¬")
            
            # íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ í™•ì¸
            target_schema = self._get_table_schema(target_db, target_table)
            if not target_schema:
                return False, f"íƒ€ê²Ÿ í…Œì´ë¸” '{target_table}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 0
            
            print(f"ğŸ¯ íƒ€ê²Ÿ ìŠ¤í‚¤ë§ˆ: {len(target_schema)}ê°œ ì»¬ëŸ¼")
            
            # ì»¬ëŸ¼ ë§¤í•‘ ì²˜ë¦¬
            if not column_mapping:
                column_mapping = self._auto_generate_mapping(source_data[0], target_schema)
            
            print(f"ğŸ”— ì»¬ëŸ¼ ë§¤í•‘: {len(column_mapping)}ê°œ ë§¤í•‘")
            for src, tgt in column_mapping.items():
                print(f"   {src} â†’ {tgt}")
            
            # ë°ì´í„° ë³€í™˜
            transformed_data = self._transform_data(source_data, column_mapping, target_schema)
            print(f"ğŸ”„ ë°ì´í„° ë³€í™˜: {len(transformed_data)}ê°œ í–‰ ì²˜ë¦¬ë¨")
            
            if dry_run:
                print("ğŸ” DRY RUN ëª¨ë“œ - ì‹¤ì œ ë°ì´í„°ëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                self._preview_data(transformed_data[:3])  # ì²˜ìŒ 3ê°œ í–‰ë§Œ ë¯¸ë¦¬ë³´ê¸°
                return True, f"DRY RUN ì™„ë£Œ: {len(transformed_data)}ê°œ í–‰ ì²˜ë¦¬ ì˜ˆì •", len(transformed_data)
            
            # ì‹¤ì œ ë°ì´í„° ì‚½ì…
            inserted_count = self._insert_data(target_db, target_table, transformed_data, skip_duplicates)
            
            print(f"âœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {inserted_count}ê°œ í–‰ ì‚½ì…ë¨")
            return True, f"ì„±ê³µì ìœ¼ë¡œ {inserted_count}ê°œ í–‰ì„ ë§ˆì´ê·¸ë ˆì´ì…˜í–ˆìŠµë‹ˆë‹¤.", inserted_count
            
        except Exception as e:
            error_msg = f"ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {str(e)}"
            print(f"âŒ {error_msg}")
            return False, error_msg, 0
    
    def _fetch_source_data(self, db_path: str, table_name: str, where_condition: Optional[str] = None) -> List[Dict[str, Any]]:
        """ì†ŒìŠ¤ ë°ì´í„° ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            
            query = f"SELECT * FROM {table_name}"
            if where_condition:
                query += f" WHERE {where_condition}"
            
            cursor.execute(query)
            rows = cursor.fetchall()
            conn.close()
            
            return [dict(row) for row in rows]
            
        except Exception as e:
            print(f"âŒ ì†ŒìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_table_schema(self, db_path: str, table_name: str) -> List[Dict[str, Any]]:
        """í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ"""
        try:
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            cursor.execute(f"PRAGMA table_info({table_name})")
            schema = cursor.fetchall()
            conn.close()
            
            return [
                {
                    'name': row[1],
                    'type': row[2],
                    'notnull': row[3],
                    'default': row[4],
                    'pk': row[5]
                }
                for row in schema
            ]
            
        except Exception as e:
            print(f"âŒ ìŠ¤í‚¤ë§ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def _auto_generate_mapping(self, sample_row: Dict[str, Any], target_schema: List[Dict[str, Any]]) -> Dict[str, str]:
        """ìë™ ì»¬ëŸ¼ ë§¤í•‘ ìƒì„±"""
        source_columns = set(sample_row.keys())
        target_columns = {col['name'] for col in target_schema}
        
        mapping = {}
        
        # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì»¬ëŸ¼ë“¤
        for col in source_columns.intersection(target_columns):
            mapping[col] = col
        
        # ìœ ì‚¬í•œ ì»¬ëŸ¼ëª… ë§¤í•‘ (ê¸°ë³¸ ê·œì¹™ë“¤)
        similarity_rules = {
            'condition_name': 'name',  # settings â†’ strategies ë§¤í•‘
            'description': 'description',
            'created_at': 'created_at',
            'updated_at': 'updated_at',
            'is_active': 'is_active'
        }
        
        for src, tgt in similarity_rules.items():
            if src in source_columns and tgt in target_columns and src not in mapping:
                mapping[src] = tgt
        
        return mapping
    
    def _transform_data(self, source_data: List[Dict[str, Any]], 
                       column_mapping: Dict[str, str], 
                       target_schema: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ë°ì´í„° ë³€í™˜"""
        transformed = []
        target_columns = {col['name']: col for col in target_schema}
        
        for row in source_data:
            new_row = {}
            
            # ë§¤í•‘ëœ ì»¬ëŸ¼ë“¤ ì²˜ë¦¬
            for src_col, tgt_col in column_mapping.items():
                if src_col in row and tgt_col in target_columns:
                    value = row[src_col]
                    
                    # íƒ€ì… ë³€í™˜ ì²˜ë¦¬
                    target_type = target_columns[tgt_col]['type']
                    new_row[tgt_col] = self._convert_value(value, target_type)
            
            # variable_mappings ë°ì´í„° íŠ¹ë³„ ì²˜ë¦¬
            if 'variable_mappings' in row:
                self._parse_variable_mappings(row['variable_mappings'], new_row)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ê¸°ë³¸ê°’ ì„¤ì •
            for col_info in target_schema:
                col_name = col_info['name']
                if col_name not in new_row:
                    if col_info['notnull'] and col_info['default'] is None:
                        # NOT NULL ì»¬ëŸ¼ì— ê¸°ë³¸ê°’ ì„¤ì •
                        new_row[col_name] = self._get_default_value(col_info['type'])
                    elif col_info['default'] is not None:
                        new_row[col_name] = col_info['default']
            
            transformed.append(new_row)
        
        return transformed
    
    def _parse_variable_mappings(self, json_data: str, target_row: Dict[str, Any]) -> None:
        """variable_mappings JSONì„ strategies í…Œì´ë¸” ì»¬ëŸ¼ë“¤ë¡œ ë¶„ë¦¬"""
        try:
            if not json_data:
                return
            
            # JSON íŒŒì‹±
            mapping_data = json.loads(json_data) if isinstance(json_data, str) else json_data
            
            # strategies í…Œì´ë¸” ì»¬ëŸ¼ë“¤ë¡œ ë§¤í•‘
            if 'variable_id' in mapping_data:
                target_row['variable_id'] = mapping_data['variable_id']
            if 'variable_name' in mapping_data:
                target_row['variable_name'] = mapping_data['variable_name']
            if 'variable_params' in mapping_data:
                target_row['variable_params'] = json.dumps(mapping_data['variable_params'], ensure_ascii=False)
            if 'operator' in mapping_data:
                target_row['operator'] = mapping_data['operator']
            if 'comparison_type' in mapping_data:
                target_row['comparison_type'] = mapping_data['comparison_type']
            if 'target_value' in mapping_data:
                target_row['target_value'] = mapping_data['target_value']
            if 'external_variable' in mapping_data:
                ext_var = mapping_data['external_variable']
                target_row['external_variable'] = json.dumps(ext_var, ensure_ascii=False) if ext_var else None
            if 'trend_direction' in mapping_data:
                target_row['trend_direction'] = mapping_data['trend_direction']
            if 'chart_category' in mapping_data:
                target_row['chart_category'] = mapping_data['chart_category']
                
        except (json.JSONDecodeError, TypeError) as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •
            target_row['variable_id'] = ''
            target_row['variable_name'] = ''
            target_row['operator'] = ''
    
    def _convert_value(self, value: Any, target_type: str) -> Any:
        """ê°’ íƒ€ì… ë³€í™˜"""
        if value is None:
            return None
        
        target_type = target_type.upper()
        
        try:
            if target_type in ['INTEGER', 'INT']:
                return int(value) if value != '' else None
            elif target_type in ['REAL', 'FLOAT', 'DOUBLE']:
                return float(value) if value != '' else None
            elif target_type in ['TEXT', 'VARCHAR', 'CHAR']:
                return str(value)
            elif target_type == 'BOOLEAN':
                if isinstance(value, bool):
                    return value
                if isinstance(value, (int, str)):
                    return bool(int(value))
            elif target_type in ['DATETIME', 'TIMESTAMP']:
                if isinstance(value, str):
                    return value
                return str(value)
            else:
                return value
        except (ValueError, TypeError):
            return None
    
    def _get_default_value(self, column_type: str) -> Any:
        """ì»¬ëŸ¼ íƒ€ì…ë³„ ê¸°ë³¸ê°’ ë°˜í™˜"""
        column_type = column_type.upper()
        
        if column_type in ['INTEGER', 'INT']:
            return 0
        elif column_type in ['REAL', 'FLOAT', 'DOUBLE']:
            return 0.0
        elif column_type in ['TEXT', 'VARCHAR', 'CHAR']:
            return ''
        elif column_type == 'BOOLEAN':
            return False
        elif column_type in ['DATETIME', 'TIMESTAMP']:
            return datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        else:
            return None
    
    def _insert_data(self, db_path: str, table_name: str, data: List[Dict[str, Any]], skip_duplicates: bool) -> int:
        """ë°ì´í„° ì‚½ì…"""
        if not data:
            return 0
        
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        inserted_count = 0
        
        try:
            # ì»¬ëŸ¼ ëª©ë¡ êµ¬ì„±
            columns = list(data[0].keys())
            placeholders = ', '.join(['?' for _ in columns])
            columns_str = ', '.join(columns)
            
            if skip_duplicates:
                query = f"INSERT OR IGNORE INTO {table_name} ({columns_str}) VALUES ({placeholders})"
            else:
                query = f"INSERT INTO {table_name} ({columns_str}) VALUES ({placeholders})"
            
            for row in data:
                values = [row.get(col) for col in columns]
                cursor.execute(query, values)
                if cursor.rowcount > 0:
                    inserted_count += 1
            
            conn.commit()
            
        except Exception as e:
            conn.rollback()
            print(f"âŒ ë°ì´í„° ì‚½ì… ì‹¤íŒ¨: {e}")
            raise
        finally:
            conn.close()
        
        return inserted_count
    
    def _preview_data(self, sample_data: List[Dict[str, Any]]):
        """ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"""
        print("\nğŸ” ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3ê°œ í–‰):")
        print("-" * 80)
        
        for i, row in enumerate(sample_data, 1):
            print(f"í–‰ {i}:")
            for key, value in row.items():
                print(f"  {key}: {value}")
            print("-" * 40)


def main():
    parser = argparse.ArgumentParser(description='Super DB Data Migrator - ë°ì´í„°ë² ì´ìŠ¤ ê°„ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜')
    
    parser.add_argument('--source-db', required=True, help='ì†ŒìŠ¤ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ')
    parser.add_argument('--target-db', required=True, help='íƒ€ê²Ÿ ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ')
    parser.add_argument('--source-table', required=True, help='ì†ŒìŠ¤ í…Œì´ë¸”ëª…')
    parser.add_argument('--target-table', help='íƒ€ê²Ÿ í…Œì´ë¸”ëª… (ê¸°ë³¸ê°’: source-tableê³¼ ë™ì¼)')
    parser.add_argument('--where', help='WHERE ì¡°ê±´ (ì˜ˆ: "id > 0")')
    parser.add_argument('--mapping', help='ì»¬ëŸ¼ ë§¤í•‘ JSON (ì˜ˆ: {"old_name":"new_name"})')
    parser.add_argument('--allow-duplicates', action='store_true', help='ì¤‘ë³µ ë°ì´í„° í—ˆìš©')
    parser.add_argument('--dry-run', action='store_true', help='ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ë¯¸ë¦¬ë³´ê¸°ë§Œ')
    
    args = parser.parse_args()
    
    # ì»¬ëŸ¼ ë§¤í•‘ íŒŒì‹±
    column_mapping = None
    if args.mapping:
        try:
            column_mapping = json.loads(args.mapping)
        except json.JSONDecodeError:
            print("âŒ ì˜ëª»ëœ ë§¤í•‘ JSON í˜•ì‹ì…ë‹ˆë‹¤.")
            return
    
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    migrator = SuperDBDataMigrator()
    success, message, count = migrator.migrate_data(
        source_db=args.source_db,
        target_db=args.target_db,
        source_table=args.source_table,
        target_table=args.target_table,
        column_mapping=column_mapping,
        where_condition=args.where,
        skip_duplicates=not args.allow_duplicates,
        dry_run=args.dry_run
    )
    
    if success:
        print(f"\nâœ… {message}")
    else:
        print(f"\nâŒ {message}")
        exit(1)


if __name__ == "__main__":
    main()
