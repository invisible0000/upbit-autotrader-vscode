#!/usr/bin/env python3
"""
ğŸ”„ Super DB Rollback Manager
ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ë¡¤ë°± ë° ë³µêµ¬ ê´€ë¦¬ ë„êµ¬

ğŸ¤– LLM ì‚¬ìš© ê°€ì´ë“œ:
===================
ì´ ë„êµ¬ëŠ” DB ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘ ë¬¸ì œ ë°œìƒ ì‹œ ì•ˆì „í•œ ë¡¤ë°±ì„ ì œê³µí•©ë‹ˆë‹¤.

ğŸ“‹ ì£¼ìš” ëª…ë ¹ì–´ (tools í´ë”ì—ì„œ ì‹¤í–‰):
1. python super_db_rollback_manager.py --create-checkpoint "pre_migration_phase1"
2. python super_db_rollback_manager.py --rollback "pre_migration_phase1" --verify
3. python super_db_rollback_manager.py --list-checkpoints
4. python super_db_rollback_manager.py --auto-backup --schedule daily

ğŸ¯ ì–¸ì œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ê°€:
- ì¤‘ìš”í•œ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
- ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨ ì‹œ ì´ì „ ìƒíƒœë¡œ ë¡¤ë°±
- ì •ê¸°ì ì¸ ë°±ì—… ë° ë³µêµ¬ ì ê²€
- ì‹œìŠ¤í…œ ì¥ì•  ì‹œ ë¹ ë¥¸ ë³µêµ¬

ğŸ’¡ ì¶œë ¥ í•´ì„:
- ğŸŸ¢ ì„±ê³µ: ì²´í¬í¬ì¸íŠ¸ ìƒì„±/ë¡¤ë°± ì„±ê³µ
- ğŸŸ¡ ì£¼ì˜: ì¼ë¶€ íŒŒì¼ ë°±ì—… ì‹¤íŒ¨ (ë¶€ë¶„ ë°±ì—…)
- ğŸ”´ ì‹¤íŒ¨: ì²´í¬í¬ì¸íŠ¸ ìƒì„±/ë¡¤ë°± ì‹¤íŒ¨
- ğŸ“¦ ë°±ì—…: ì „ì²´/ì¦ë¶„/êµ¬ì¡°ë§Œ ë°±ì—… ìœ í˜•

ê¸°ëŠ¥:
1. ì „ì²´ ì‹œìŠ¤í…œ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
2. ë‹¨ê³„ë³„ ì•ˆì „ ë¡¤ë°±
3. ë°±ì—… ë¬´ê²°ì„± ê²€ì¦
4. ê¸°ì¡´ Super DB ë„êµ¬ì™€ ì—°ë™

ì‘ì„±ì¼: 2025-08-01
ì‘ì„±ì: Upbit Auto Trading Team
"""
import shutil
import json
import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple
from dataclasses import dataclass, asdict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ íŒŒì´ì¬ íŒ¨ìŠ¤ì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/super_db_rollback_manager.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class CheckpointMetadata:
    """ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„°"""
    checkpoint_id: str
    created_at: str
    description: str
    backup_type: str  # 'full', 'incremental', 'structure'
    db_files: List[str]
    yaml_files: List[str]
    backup_size_mb: float
    verification_status: str  # 'verified', 'partial', 'failed'
    tool_versions: Dict[str, str]
    
    
@dataclass
class RollbackResult:
    """ë¡¤ë°± ê²°ê³¼"""
    checkpoint_id: str
    rollback_time: str
    success: bool
    restored_files: List[str]
    failed_files: List[str]
    verification_passed: bool
    issues: List[str]
    recommendations: List[str]


class SuperDBRollbackManager:
    """
    ğŸ”„ Super DB Rollback Manager - ì•ˆì „í•œ ë¡¤ë°± ë° ë³µêµ¬ ê´€ë¦¬
    
    ğŸ¤– LLM ì‚¬ìš© íŒ¨í„´:
    manager = SuperDBRollbackManager()
    manager.create_migration_checkpoint("pre_major_update", "ì „ì²´ ë°±ì—…")
    manager.rollback_to_checkpoint("pre_major_update", verify=True)
    manager.list_available_checkpoints()
    
    ğŸ’¡ í•µì‹¬ ê¸°ëŠ¥: ì™„ì „ ë°±ì—… + ì•ˆì „ ë¡¤ë°± + ë¬´ê²°ì„± ê²€ì¦
    """
    
    def __init__(self):
        """ì´ˆê¸°í™” - ê²½ë¡œ ë° ë°±ì—… ì„¤ì • ì¤€ë¹„"""
        self.project_root = PROJECT_ROOT
        self.db_path = self.project_root / "upbit_auto_trading" / "data"
        self.data_info_path = (
            self.project_root / "upbit_auto_trading" / "utils" /
            "trading_variables" / "gui_variables_DB_migration_util" / "data_info"
        )
        
        # ë°±ì—… ê¸°ë³¸ ë””ë ‰í† ë¦¬
        self.backup_base = self.project_root / "backups" / "rollback_checkpoints"
        self.backup_base.mkdir(parents=True, exist_ok=True)
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼
        self.metadata_file = self.backup_base / "checkpoint_metadata.json"
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = self.project_root / "logs"
        log_dir.mkdir(exist_ok=True)
        
        # ë°±ì—… ëŒ€ìƒ íŒŒì¼ë“¤
        self.target_dbs = {
            'settings': self.db_path / 'settings.sqlite3',
            'strategies': self.db_path / 'strategies.sqlite3',
            'market_data': self.db_path / 'market_data.sqlite3'
        }
        
        # ë°±ì—… ë ˆë²¨ ì„¤ì •
        self.backup_levels = {
            'full': {
                'include_db': True,
                'include_yaml': True,
                'include_merged': True,
                'include_backups': True,
                'include_logs': False
            },
            'incremental': {
                'include_db': True,
                'include_yaml': True,
                'include_merged': False,
                'include_backups': False,
                'include_logs': False
            },
            'structure': {
                'include_db': True,
                'include_yaml': False,
                'include_merged': False,
                'include_backups': False,
                'include_logs': False
            }
        }
        
        logger.info("ğŸ”„ Super DB Rollback Manager ì´ˆê¸°í™”")
        logger.info(f"ğŸ“‚ DB Path: {self.db_path}")
        logger.info(f"ğŸ’¾ ë°±ì—… ê²½ë¡œ: {self.backup_base}")
        logger.info(f"ğŸ—„ï¸ ëŒ€ìƒ DB: {list(self.target_dbs.keys())}")
    
    def load_checkpoint_metadata(self) -> Dict[str, CheckpointMetadata]:
        """ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if not self.metadata_file.exists():
            return {}
        
        try:
            with open(self.metadata_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            checkpoints = {}
            for checkpoint_id, metadata_dict in data.items():
                checkpoints[checkpoint_id] = CheckpointMetadata(**metadata_dict)
            
            return checkpoints
            
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}
    
    def save_checkpoint_metadata(self, checkpoints: Dict[str, CheckpointMetadata]) -> None:
        """ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            data = {}
            for checkpoint_id, metadata in checkpoints.items():
                data[checkpoint_id] = asdict(metadata)
            
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"âŒ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def get_tool_versions(self) -> Dict[str, str]:
        """í˜„ì¬ Super DB ë„êµ¬ë“¤ì˜ ë²„ì „ ì •ë³´ ìˆ˜ì§‘"""
        tools_dir = self.project_root / "tools"
        tool_versions = {}
        
        super_db_tools = [
            'super_db_structure_generator.py',
            'super_db_extraction_db_to_yaml.py',
            'super_db_migration_yaml_to_db.py',
            'super_db_yaml_editor_workflow.py',
            'super_db_yaml_merger.py',
            'super_db_schema_extractor.py',
            'super_db_health_monitor.py',
            'super_db_schema_validator.py'
        ]
        
        for tool in super_db_tools:
            tool_path = tools_dir / tool
            if tool_path.exists():
                try:
                    mtime = tool_path.stat().st_mtime
                    version = datetime.fromtimestamp(mtime).strftime("%Y%m%d_%H%M%S")
                    tool_versions[tool] = version
                except Exception:
                    tool_versions[tool] = "unknown"
            else:
                tool_versions[tool] = "missing"
        
        return tool_versions
    
    def calculate_backup_size(self, backup_dir: Path) -> float:
        """ë°±ì—… ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (MB)"""
        if not backup_dir.exists():
            return 0.0
        
        total_size = 0
        try:
            for file_path in backup_dir.rglob('*'):
                if file_path.is_file():
                    total_size += file_path.stat().st_size
            
            return round(total_size / (1024 * 1024), 2)
            
        except Exception as e:
            logger.error(f"âŒ ë°±ì—… í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0
    
    def backup_database_files(self, backup_dir: Path, backup_level: str) -> Tuple[List[str], List[str]]:
        """ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ë°±ì—…"""
        backed_up = []
        failed = []
        
        if not self.backup_levels[backup_level]['include_db']:
            return backed_up, failed
        
        db_backup_dir = backup_dir / "databases"
        db_backup_dir.mkdir(exist_ok=True)
        
        for db_name, db_file in self.target_dbs.items():
            if db_file.exists():
                try:
                    target_path = db_backup_dir / db_file.name
                    shutil.copy2(db_file, target_path)
                    backed_up.append(str(db_file))
                    logger.info(f"âœ… DB ë°±ì—… ì™„ë£Œ: {db_name}")
                except Exception as e:
                    failed.append(f"{db_name}: {str(e)}")
                    logger.error(f"âŒ DB ë°±ì—… ì‹¤íŒ¨ ({db_name}): {e}")
            else:
                logger.warning(f"âš ï¸ DB íŒŒì¼ ì—†ìŒ: {db_name} ({db_file})")
        
        return backed_up, failed
    
    def backup_yaml_files(self, backup_dir: Path, backup_level: str) -> Tuple[List[str], List[str]]:
        """YAML íŒŒì¼ë“¤ ë°±ì—…"""
        backed_up = []
        failed = []
        
        if not self.backup_levels[backup_level]['include_yaml']:
            return backed_up, failed
        
        yaml_backup_dir = backup_dir / "yaml_files"
        yaml_backup_dir.mkdir(exist_ok=True)
        
        # data_info ë””ë ‰í† ë¦¬ì˜ YAML íŒŒì¼ë“¤
        if self.data_info_path.exists():
            try:
                for yaml_file in self.data_info_path.glob("*.yaml"):
                    target_path = yaml_backup_dir / yaml_file.name
                    shutil.copy2(yaml_file, target_path)
                    backed_up.append(str(yaml_file))
                
                # _MERGED_ ë””ë ‰í† ë¦¬ (í•„ìš”í•œ ê²½ìš°)
                if self.backup_levels[backup_level]['include_merged']:
                    merged_dir = self.data_info_path / "_MERGED_"
                    if merged_dir.exists():
                        target_merged_dir = yaml_backup_dir / "_MERGED_"
                        shutil.copytree(merged_dir, target_merged_dir, dirs_exist_ok=True)
                        backed_up.extend([str(f) for f in merged_dir.rglob("*.yaml")])
                
                # _BACKUPS_ ë””ë ‰í† ë¦¬ (í•„ìš”í•œ ê²½ìš°)
                if self.backup_levels[backup_level]['include_backups']:
                    backups_dir = self.data_info_path / "_BACKUPS_"
                    if backups_dir.exists():
                        target_backups_dir = yaml_backup_dir / "_BACKUPS_"
                        shutil.copytree(backups_dir, target_backups_dir, dirs_exist_ok=True)
                        backed_up.extend([str(f) for f in backups_dir.rglob("*.yaml")])
                
                logger.info(f"âœ… YAML íŒŒì¼ ë°±ì—… ì™„ë£Œ: {len(backed_up)}ê°œ")
                
            except Exception as e:
                failed.append(f"YAML ë°±ì—… ì‹¤íŒ¨: {str(e)}")
                logger.error(f"âŒ YAML ë°±ì—… ì‹¤íŒ¨: {e}")
        
        return backed_up, failed
    
    def verify_backup_integrity(self, backup_dir: Path, original_files: List[str]) -> Tuple[bool, List[str]]:
        """ë°±ì—… ë¬´ê²°ì„± ê²€ì¦"""
        issues = []
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸
        if not backup_dir.exists():
            issues.append("ë°±ì—… ë””ë ‰í† ë¦¬ ì—†ìŒ")
            return False, issues
        
        # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
        metadata_backup = backup_dir / "checkpoint_info.json"
        if not metadata_backup.exists():
            issues.append("ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ì—†ìŒ")
        
        # DB íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
        db_backup_dir = backup_dir / "databases"
        if db_backup_dir.exists():
            for db_name, original_db in self.target_dbs.items():
                if original_db.exists():
                    backup_db = db_backup_dir / original_db.name
                    if backup_db.exists():
                        # í¬ê¸° ë¹„êµ
                        original_size = original_db.stat().st_size
                        backup_size = backup_db.stat().st_size
                        
                        if original_size != backup_size:
                            issues.append(f"DB í¬ê¸° ë¶ˆì¼ì¹˜: {db_name} (ì›ë³¸:{original_size}, ë°±ì—…:{backup_size})")
                    else:
                        issues.append(f"ë°±ì—… DB íŒŒì¼ ì—†ìŒ: {db_name}")
        
        # YAML íŒŒì¼ í™•ì¸
        yaml_backup_dir = backup_dir / "yaml_files"
        if yaml_backup_dir.exists():
            backup_yaml_count = len(list(yaml_backup_dir.glob("*.yaml")))
            if self.data_info_path.exists():
                original_yaml_count = len(list(self.data_info_path.glob("*.yaml")))
                if backup_yaml_count != original_yaml_count:
                    issues.append(f"YAML íŒŒì¼ ìˆ˜ ë¶ˆì¼ì¹˜ (ì›ë³¸:{original_yaml_count}, ë°±ì—…:{backup_yaml_count})")
        
        integrity_passed = len(issues) == 0
        return integrity_passed, issues
    
    def create_migration_checkpoint(self, checkpoint_id: str, description: str = "", 
                                   backup_type: str = "full") -> bool:
        """ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬í¬ì¸íŠ¸ ìƒì„±"""
        logger.info(f"ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹œì‘: {checkpoint_id}")
        
        # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        checkpoints = self.load_checkpoint_metadata()
        
        # ì¤‘ë³µ ID í™•ì¸
        if checkpoint_id in checkpoints:
            logger.error(f"âŒ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ ID: {checkpoint_id}")
            return False
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir = self.backup_base / f"{checkpoint_id}_{timestamp}"
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # 1. DB íŒŒì¼ ë°±ì—…
            print(f"ğŸ“¦ DB íŒŒì¼ ë°±ì—… ì¤‘...")
            db_backed_up, db_failed = self.backup_database_files(backup_dir, backup_type)
            
            # 2. YAML íŒŒì¼ ë°±ì—…
            print(f"ğŸ“„ YAML íŒŒì¼ ë°±ì—… ì¤‘...")
            yaml_backed_up, yaml_failed = self.backup_yaml_files(backup_dir, backup_type)
            
            # 3. ì²´í¬í¬ì¸íŠ¸ ì •ë³´ ì €ì¥
            checkpoint_info = {
                'checkpoint_id': checkpoint_id,
                'created_at': datetime.now().isoformat(),
                'description': description,
                'backup_type': backup_type,
                'backup_location': str(backup_dir),
                'tool_versions': self.get_tool_versions()
            }
            
            info_file = backup_dir / "checkpoint_info.json"
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(checkpoint_info, f, indent=2, ensure_ascii=False)
            
            # 4. ë°±ì—… ë¬´ê²°ì„± ê²€ì¦
            print(f"ğŸ” ë°±ì—… ë¬´ê²°ì„± ê²€ì¦ ì¤‘...")
            integrity_passed, integrity_issues = self.verify_backup_integrity(backup_dir, db_backed_up + yaml_backed_up)
            
            # 5. ë©”íƒ€ë°ì´í„° ìƒì„±
            backup_size = self.calculate_backup_size(backup_dir)
            
            verification_status = "verified" if integrity_passed else "partial" if len(integrity_issues) < 3 else "failed"
            
            metadata = CheckpointMetadata(
                checkpoint_id=checkpoint_id,
                created_at=datetime.now().isoformat(),
                description=description,
                backup_type=backup_type,
                db_files=db_backed_up,
                yaml_files=yaml_backed_up,
                backup_size_mb=backup_size,
                verification_status=verification_status,
                tool_versions=self.get_tool_versions()
            )
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            checkpoints[checkpoint_id] = metadata
            self.save_checkpoint_metadata(checkpoints)
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì™„ë£Œ: {checkpoint_id}")
            print(f"ğŸ“Š ë°±ì—… í†µê³„:")
            print(f"   ğŸ’¾ DB íŒŒì¼: {len(db_backed_up)}ê°œ ë°±ì—…, {len(db_failed)}ê°œ ì‹¤íŒ¨")
            print(f"   ğŸ“„ YAML íŒŒì¼: {len(yaml_backed_up)}ê°œ ë°±ì—…, {len(yaml_failed)}ê°œ ì‹¤íŒ¨")
            print(f"   ğŸ“¦ ë°±ì—… í¬ê¸°: {backup_size:.1f}MB")
            print(f"   ğŸ” ë¬´ê²°ì„±: {verification_status}")
            
            if integrity_issues:
                print(f"âš ï¸ ë¬´ê²°ì„± ì´ìŠˆ:")
                for issue in integrity_issues[:3]:
                    print(f"   â€¢ {issue}")
            
            return verification_status != "failed"
            
        except Exception as e:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            
            # ì‹¤íŒ¨ ì‹œ ë¶€ë¶„ ë°±ì—… ì •ë¦¬
            if backup_dir.exists():
                try:
                    shutil.rmtree(backup_dir)
                except Exception:
                    pass
            
            return False
    
    def rollback_to_checkpoint(self, checkpoint_id: str, verify: bool = True) -> RollbackResult:
        """íŠ¹ì • ì²´í¬í¬ì¸íŠ¸ë¡œ ë¡¤ë°±"""
        logger.info(f"ğŸ”„ ë¡¤ë°± ì‹œì‘: {checkpoint_id}")
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        checkpoints = self.load_checkpoint_metadata()
        
        if checkpoint_id not in checkpoints:
            logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì—†ìŒ: {checkpoint_id}")
            return RollbackResult(
                checkpoint_id=checkpoint_id,
                rollback_time=datetime.now().isoformat(),
                success=False,
                restored_files=[],
                failed_files=[],
                verification_passed=False,
                issues=["ì²´í¬í¬ì¸íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ"],
                recommendations=["ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ í™•ì¸"]
            )
        
        metadata = checkpoints[checkpoint_id]
        
        # ë°±ì—… ë””ë ‰í† ë¦¬ ì°¾ê¸°
        backup_dirs = list(self.backup_base.glob(f"{checkpoint_id}_*"))
        if not backup_dirs:
            return RollbackResult(
                checkpoint_id=checkpoint_id,
                rollback_time=datetime.now().isoformat(),
                success=False,
                restored_files=[],
                failed_files=[],
                verification_passed=False,
                issues=["ë°±ì—… ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"],
                recommendations=["ì²´í¬í¬ì¸íŠ¸ ì¬ìƒì„± í•„ìš”"]
            )
        
        backup_dir = backup_dirs[0]  # ê°€ì¥ ìµœê·¼ ë°±ì—… ì‚¬ìš©
        
        restored_files = []
        failed_files = []
        issues = []
        
        try:
            print(f"ğŸ”„ ë¡¤ë°± ì§„í–‰ ì¤‘: {checkpoint_id}")
            
            # 1. í˜„ì¬ ìƒíƒœ ì„ì‹œ ë°±ì—… ìƒì„±
            temp_backup_id = f"temp_before_rollback_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            print(f"ğŸ’¾ í˜„ì¬ ìƒíƒœ ì„ì‹œ ë°±ì—… ìƒì„±: {temp_backup_id}")
            temp_backup_success = self.create_migration_checkpoint(temp_backup_id, "ë¡¤ë°± ì „ ì„ì‹œ ë°±ì—…", "incremental")
            
            if not temp_backup_success:
                issues.append("ì„ì‹œ ë°±ì—… ìƒì„± ì‹¤íŒ¨")
            
            # 2. DB íŒŒì¼ ë³µì›
            db_backup_dir = backup_dir / "databases"
            if db_backup_dir.exists():
                print(f"ğŸ“Š DB íŒŒì¼ ë³µì› ì¤‘...")
                for db_name, db_file in self.target_dbs.items():
                    backup_db = db_backup_dir / db_file.name
                    if backup_db.exists():
                        try:
                            # ê¸°ì¡´ íŒŒì¼ ë°±ì—… í›„ êµì²´
                            if db_file.exists():
                                temp_file = db_file.with_suffix('.temp_rollback')
                                shutil.move(db_file, temp_file)
                            
                            shutil.copy2(backup_db, db_file)
                            restored_files.append(str(db_file))
                            
                            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
                            temp_file = db_file.with_suffix('.temp_rollback')
                            if temp_file.exists():
                                temp_file.unlink()
                            
                            logger.info(f"âœ… DB ë³µì› ì™„ë£Œ: {db_name}")
                            
                        except Exception as e:
                            failed_files.append(f"{db_name}: {str(e)}")
                            logger.error(f"âŒ DB ë³µì› ì‹¤íŒ¨ ({db_name}): {e}")
                            
                            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë³µêµ¬ ì‹œë„
                            temp_file = db_file.with_suffix('.temp_rollback')
                            if temp_file.exists():
                                try:
                                    shutil.move(temp_file, db_file)
                                except Exception:
                                    pass
            
            # 3. YAML íŒŒì¼ ë³µì›
            yaml_backup_dir = backup_dir / "yaml_files"
            if yaml_backup_dir.exists():
                print(f"ğŸ“„ YAML íŒŒì¼ ë³µì› ì¤‘...")
                
                # ê¸°ì¡´ YAML íŒŒì¼ë“¤ ë°±ì—…
                if self.data_info_path.exists():
                    for yaml_file in self.data_info_path.glob("*.yaml"):
                        try:
                            temp_file = yaml_file.with_suffix('.temp_rollback')
                            shutil.move(yaml_file, temp_file)
                        except Exception:
                            pass
                
                # ë°±ì—…ëœ YAML íŒŒì¼ë“¤ ë³µì›
                for yaml_file in yaml_backup_dir.glob("*.yaml"):
                    try:
                        target_path = self.data_info_path / yaml_file.name
                        shutil.copy2(yaml_file, target_path)
                        restored_files.append(str(target_path))
                    except Exception as e:
                        failed_files.append(f"{yaml_file.name}: {str(e)}")
                
                # _MERGED_ ë””ë ‰í† ë¦¬ ë³µì›
                merged_backup = yaml_backup_dir / "_MERGED_"
                if merged_backup.exists():
                    merged_target = self.data_info_path / "_MERGED_"
                    try:
                        if merged_target.exists():
                            shutil.rmtree(merged_target)
                        shutil.copytree(merged_backup, merged_target)
                        restored_files.append(str(merged_target))
                    except Exception as e:
                        failed_files.append(f"_MERGED_: {str(e)}")
            
            # 4. ë¡¤ë°± ê²€ì¦ (ì˜µì…˜)
            verification_passed = True
            if verify:
                print(f"ğŸ” ë¡¤ë°± ê²€ì¦ ì¤‘...")
                
                # ê°„ë‹¨í•œ DB ì—°ê²° í…ŒìŠ¤íŠ¸
                for db_name, db_file in self.target_dbs.items():
                    if db_file.exists():
                        try:
                            import sqlite3
                            conn = sqlite3.connect(db_file)
                            cursor = conn.cursor()
                            cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='table'")
                            table_count = cursor.fetchone()[0]
                            conn.close()
                            
                            if table_count == 0:
                                issues.append(f"ë³µì›ëœ DBê°€ ë¹„ì–´ìˆìŒ: {db_name}")
                                verification_passed = False
                                
                        except Exception as e:
                            issues.append(f"DB ê²€ì¦ ì‹¤íŒ¨: {db_name} - {str(e)}")
                            verification_passed = False
            
            success = len(failed_files) == 0
            
            # ê²°ê³¼ ì¶œë ¥
            if success:
                print(f"âœ… ë¡¤ë°± ì™„ë£Œ: {checkpoint_id}")
            else:
                print(f"âš ï¸ ë¡¤ë°± ë¶€ë¶„ ì™„ë£Œ: {checkpoint_id}")
            
            print(f"ğŸ“Š ë¡¤ë°± í†µê³„:")
            print(f"   âœ… ë³µì› ì„±ê³µ: {len(restored_files)}ê°œ íŒŒì¼")
            print(f"   âŒ ë³µì› ì‹¤íŒ¨: {len(failed_files)}ê°œ íŒŒì¼")
            
            if failed_files:
                print(f"ì‹¤íŒ¨ ëª©ë¡:")
                for failed in failed_files[:3]:
                    print(f"   â€¢ {failed}")
            
            recommendations = []
            if not success:
                recommendations.extend([
                    "ì‹¤íŒ¨í•œ íŒŒì¼ë“¤ì„ ìˆ˜ë™ìœ¼ë¡œ ë³µì›",
                    "ì„ì‹œ ë°±ì—…ì—ì„œ ì¶”ê°€ ë³µêµ¬ ì‹œë„",
                    "super_db_health_monitor.pyë¡œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"
                ])
            
            return RollbackResult(
                checkpoint_id=checkpoint_id,
                rollback_time=datetime.now().isoformat(),
                success=success,
                restored_files=restored_files,
                failed_files=failed_files,
                verification_passed=verification_passed,
                issues=issues,
                recommendations=recommendations
            )
            
        except Exception as e:
            logger.error(f"âŒ ë¡¤ë°± ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return RollbackResult(
                checkpoint_id=checkpoint_id,
                rollback_time=datetime.now().isoformat(),
                success=False,
                restored_files=restored_files,
                failed_files=failed_files,
                verification_passed=False,
                issues=[f"ë¡¤ë°± ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"],
                recommendations=["ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜"]
            )
    
    def list_available_checkpoints(self) -> List[CheckpointMetadata]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ ì¡°íšŒ"""
        checkpoints = self.load_checkpoint_metadata()
        return list(checkpoints.values())
    
    def cleanup_old_checkpoints(self, keep_count: int = 10) -> int:
        """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
        checkpoints = self.load_checkpoint_metadata()
        
        if len(checkpoints) <= keep_count:
            return 0
        
        # ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬
        sorted_checkpoints = sorted(
            checkpoints.items(),
            key=lambda x: x[1].created_at,
            reverse=True
        )
        
        # ë³´ê´€í•  ì²´í¬í¬ì¸íŠ¸ì™€ ì‚­ì œí•  ì²´í¬í¬ì¸íŠ¸ ë¶„ë¦¬
        to_keep = sorted_checkpoints[:keep_count]
        to_delete = sorted_checkpoints[keep_count:]
        
        deleted_count = 0
        
        for checkpoint_id, metadata in to_delete:
            try:
                # ë°±ì—… ë””ë ‰í† ë¦¬ ì‚­ì œ
                backup_dirs = list(self.backup_base.glob(f"{checkpoint_id}_*"))
                for backup_dir in backup_dirs:
                    if backup_dir.exists():
                        shutil.rmtree(backup_dir)
                
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì œê±°
                del checkpoints[checkpoint_id]
                deleted_count += 1
                
                logger.info(f"ğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì™„ë£Œ: {checkpoint_id}")
                
            except Exception as e:
                logger.error(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨ ({checkpoint_id}): {e}")
        
        # ì—…ë°ì´íŠ¸ëœ ë©”íƒ€ë°ì´í„° ì €ì¥
        self.save_checkpoint_metadata(checkpoints)
        
        return deleted_count


def main():
    """
    ğŸ¤– LLM ì‚¬ìš© ê°€ì´ë“œ: ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    
    ëª…ë ¹í–‰ ì¸ìˆ˜ì— ë”°ë¼ ë‹¤ë¥¸ ë¡¤ë°± ê´€ë¦¬ ê¸°ëŠ¥ ì‹¤í–‰:
    - --create-checkpoint: ìƒˆ ì²´í¬í¬ì¸íŠ¸ ìƒì„±
    - --rollback: ì§€ì •ëœ ì²´í¬í¬ì¸íŠ¸ë¡œ ë¡¤ë°±
    - --list-checkpoints: ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
    - --cleanup: ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬
    
    ğŸ¯ LLMì´ ìì£¼ ì‚¬ìš©í•  íŒ¨í„´:
    1. python super_db_rollback_manager.py --create-checkpoint "pre_migration" --description "ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ ë°±ì—…"
    2. python super_db_rollback_manager.py --rollback "pre_migration" --verify
    3. python super_db_rollback_manager.py --list-checkpoints
    """
    parser = argparse.ArgumentParser(
        description='ğŸ”„ Super DB Rollback Manager - ì•ˆì „í•œ ë¡¤ë°± ë° ë³µêµ¬ ê´€ë¦¬ ë„êµ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ì²´í¬í¬ì¸íŠ¸ ìƒì„±
  python super_db_rollback_manager.py --create-checkpoint "pre_migration_phase1" --description "1ë‹¨ê³„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì „"
  
  # ë¡¤ë°± ì‹¤í–‰
  python super_db_rollback_manager.py --rollback "pre_migration_phase1" --verify
  
  # ì²´í¬í¬ì¸íŠ¸ ëª©ë¡
  python super_db_rollback_manager.py --list-checkpoints
  
  # ì •ë¦¬
  python super_db_rollback_manager.py --cleanup --keep 5
        """
    )
    
    parser.add_argument('--create-checkpoint', 
                       help='ìƒˆ ì²´í¬í¬ì¸íŠ¸ ìƒì„± (ì²´í¬í¬ì¸íŠ¸ ID)')
    
    parser.add_argument('--description', default='',
                       help='ì²´í¬í¬ì¸íŠ¸ ì„¤ëª…')
    
    parser.add_argument('--backup-type', default='full',
                       choices=['full', 'incremental', 'structure'],
                       help='ë°±ì—… ìœ í˜•')
    
    parser.add_argument('--rollback',
                       help='ë¡¤ë°±í•  ì²´í¬í¬ì¸íŠ¸ ID')
    
    parser.add_argument('--verify', action='store_true',
                       help='ë¡¤ë°± í›„ ê²€ì¦ ì‹¤í–‰')
    
    parser.add_argument('--list-checkpoints', action='store_true',
                       help='ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ ëª©ë¡ í‘œì‹œ')
    
    parser.add_argument('--cleanup', action='store_true',
                       help='ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬')
    
    parser.add_argument('--keep', type=int, default=10,
                       help='ë³´ê´€í•  ì²´í¬í¬ì¸íŠ¸ ìˆ˜ (ê¸°ë³¸ê°’: 10)')
    
    args = parser.parse_args()
    
    manager = SuperDBRollbackManager()
    
    try:
        if args.create_checkpoint:
            success = manager.create_migration_checkpoint(
                args.create_checkpoint, 
                args.description, 
                args.backup_type
            )
            exit(0 if success else 1)
            
        elif args.rollback:
            result = manager.rollback_to_checkpoint(args.rollback, args.verify)
            
            if not result.success:
                print(f"âŒ ë¡¤ë°± ì‹¤íŒ¨:")
                for issue in result.issues:
                    print(f"   â€¢ {issue}")
                
                if result.recommendations:
                    print(f"ê¶Œì¥ì‚¬í•­:")
                    for rec in result.recommendations:
                        print(f"   â€¢ {rec}")
            
            exit(0 if result.success else 1)
            
        elif args.list_checkpoints:
            checkpoints = manager.list_available_checkpoints()
            
            if not checkpoints:
                print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                exit(0)
            
            print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:")
            print("=" * 80)
            
            # ìµœì‹ ìˆœìœ¼ë¡œ ì •ë ¬
            sorted_checkpoints = sorted(checkpoints, key=lambda x: x.created_at, reverse=True)
            
            for checkpoint in sorted_checkpoints:
                status_emoji = {
                    'verified': 'ğŸŸ¢',
                    'partial': 'ğŸŸ¡',
                    'failed': 'ğŸ”´'
                }.get(checkpoint.verification_status, 'âšª')
                
                created_date = datetime.fromisoformat(checkpoint.created_at).strftime('%Y-%m-%d %H:%M')
                
                print(f"{status_emoji} {checkpoint.checkpoint_id}")
                print(f"   ğŸ“… ìƒì„±ì¼: {created_date}")
                print(f"   ğŸ“¦ í¬ê¸°: {checkpoint.backup_size_mb:.1f}MB")
                print(f"   ğŸ“‹ ì„¤ëª…: {checkpoint.description or 'ì„¤ëª… ì—†ìŒ'}")
                print(f"   ğŸ”§ ë°±ì—… ìœ í˜•: {checkpoint.backup_type}")
                print(f"   ğŸ’¾ DB íŒŒì¼: {len(checkpoint.db_files)}ê°œ")
                print(f"   ğŸ“„ YAML íŒŒì¼: {len(checkpoint.yaml_files)}ê°œ")
                print()
            
            exit(0)
            
        elif args.cleanup:
            deleted_count = manager.cleanup_old_checkpoints(args.keep)
            print(f"ğŸ—‘ï¸ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ ì‚­ì œ")
            print(f"ğŸ“¦ ë³´ê´€ ì¤‘ì¸ ì²´í¬í¬ì¸íŠ¸: {args.keep}ê°œ")
            exit(0)
            
        else:
            print("âŒ ì‘ì—…ì„ ì§€ì •í•´ì£¼ì„¸ìš”. --helpë¡œ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")
            exit(1)
            
    except Exception as e:
        logger.error(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        exit(1)


if __name__ == "__main__":
    main()
