#!/usr/bin/env python3
"""
Super íŒŒì¼ ì˜ì¡´ì„± ìƒì„¸ ë¶„ì„ê¸°
íŠ¹ì • íŒŒì¼ì˜ ì˜ì¡´ì„±ì„ ê¹Šì´ ìˆê²Œ ë¶„ì„í•˜ì—¬ ì‹¤ì œ ì‚¬ìš© ì—¬ë¶€ë¥¼ ì •í™•íˆ íŒë‹¨

ì‚¬ìš©ë²•:
python tools/super_file_dependency_analyzer.py <íŒŒì¼ê²½ë¡œ>
python tools/super_file_dependency_analyzer.py upbit_auto_trading/application/queries/query_container.py
"""

import ast
import re
import json
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass, asdict
import sys

@dataclass
class DependencyInfo:
    """ì˜ì¡´ì„± ì •ë³´"""
    file_path: str
    import_type: str  # direct, dynamic, string_reference, reflection
    context: str      # ë°œê²¬ëœ ì»¨í…ìŠ¤íŠ¸
    line_number: int
    confidence: float # ì‹¤ì œ ì‚¬ìš©ë  ê°€ëŠ¥ì„± (0.0-1.0)

@dataclass
class FileAnalysisResult:
    """íŒŒì¼ ë¶„ì„ ê²°ê³¼"""
    target_file: str
    file_size: int
    total_lines: int

    # ì°¸ì¡° ë¶„ì„
    imported_by: List[DependencyInfo]
    imports_from: List[str]

    # ì½”ë“œ ë¶„ì„
    class_definitions: List[str]
    function_definitions: List[str]
    constants: List[str]

    # DDD ë¶„ì„
    ddd_patterns: List[str]
    architecture_role: str

    # ì‚¬ìš© ê°€ëŠ¥ì„± í‰ê°€
    usage_probability: float
    risk_assessment: str
    recommendation: str

    # ìƒì„¸ ì¦ê±°
    evidence_for_usage: List[str]
    evidence_against_usage: List[str]

class SuperFileDependencyAnalyzer:
    """ê³ ë„í™”ëœ íŒŒì¼ ì˜ì¡´ì„± ë¶„ì„ê¸°"""

    def __init__(self, root_path: str = "upbit_auto_trading"):
        self.root_path = Path(root_path)
        self.all_python_files = list(self.root_path.rglob("*.py"))

        # DDD íŒ¨í„´ ì •ì˜ (Raw stringìœ¼ë¡œ ìˆ˜ì •)
        self.ddd_patterns = {
            "entity": [r"class.*Entity", r"@dataclass", r"class.*\(.*Entity\)"],
            "value_object": [r"class.*ValueObject", r"@dataclass.*frozen=True"],
            "repository": [r"class.*Repository", r"def.*save\(", r"def.*find"],
            "service": [r"class.*Service", r"def.*execute\(", r"def.*handle\("],
            "use_case": [r"class.*UseCase", r"def.*execute\("],
            "dto": [r"class.*DTO", r"class.*Request", r"class.*Response"],
            "query": [r"class.*Query", r"def.*query\(", r"SELECT.*FROM"],
            "container": [r"class.*Container", r"def.*register\(", r"@inject"],
            "interface": [r"class.*Interface", r"from.*abc.*import", r"@abstractmethod"]
        }

        # ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´
        self.di_patterns = [
            r"@inject",
            r"container\.resolve",
            r"container\.get",
            r"\.register\(",
            r"dependency_injection",
            r"DI\.",
            r"Inject\[",
            r"get_container\(\)"
        ]

        # ë™ì  ë¡œë”© íŒ¨í„´
        self.dynamic_patterns = [
            r"importlib\.import_module",
            r"__import__\(",
            r"exec\(",
            r"eval\(",
            r"getattr\(",
            r"hasattr\(",
            r"\.load_module",
            r"ModuleType"
        ]

    def analyze_file(self, target_file: str) -> FileAnalysisResult:
        """íŒŒì¼ ìƒì„¸ ë¶„ì„"""
        target_path = Path(target_file)
        if not target_path.exists():
            target_path = self.root_path / target_file

        if not target_path.exists():
            raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_file}")

        print(f"ğŸ” íŒŒì¼ ë¶„ì„ ì‹œì‘: {target_path}")

        # ê¸°ë³¸ ì •ë³´
        file_size = target_path.stat().st_size
        with open(target_path, 'r', encoding='utf-8') as f:
            content = f.read()
            lines = content.split('\n')
            total_lines = len(lines)

        # AST ë¶„ì„
        try:
            tree = ast.parse(content)
            classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            imports = self._extract_imports(tree)
        except SyntaxError:
            classes, functions, imports = [], [], []

        # ìƒìˆ˜ ì¶”ì¶œ
        constants = self._extract_constants(content)

        # ì°¸ì¡° ë¶„ì„
        imported_by = self._find_who_imports(target_path)

        # DDD íŒ¨í„´ ë¶„ì„
        ddd_patterns = self._analyze_ddd_patterns(content)
        architecture_role = self._determine_architecture_role(target_path, content, classes, functions)

        # ì‚¬ìš© ê°€ëŠ¥ì„± í‰ê°€
        evidence_for, evidence_against = self._collect_evidence(target_path, content, imported_by)
        usage_probability = self._calculate_usage_probability(evidence_for, evidence_against, imported_by)
        risk_assessment = self._assess_risk(usage_probability, architecture_role, ddd_patterns)
        recommendation = self._generate_recommendation(usage_probability, risk_assessment, architecture_role)

        return FileAnalysisResult(
            target_file=str(target_path),
            file_size=file_size,
            total_lines=total_lines,
            imported_by=imported_by,
            imports_from=imports,
            class_definitions=classes,
            function_definitions=functions,
            constants=constants,
            ddd_patterns=ddd_patterns,
            architecture_role=architecture_role,
            usage_probability=usage_probability,
            risk_assessment=risk_assessment,
            recommendation=recommendation,
            evidence_for_usage=evidence_for,
            evidence_against_usage=evidence_against
        )

    def _extract_imports(self, tree: ast.AST) -> List[str]:
        """ASTì—ì„œ import ì¶”ì¶œ"""
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    imports.append(node.module)
        return imports

    def _extract_constants(self, content: str) -> List[str]:
        """ìƒìˆ˜ ì •ì˜ ì¶”ì¶œ"""
        constants = []
        for line in content.split('\n'):
            line = line.strip()
            if re.match(r'^[A-Z_][A-Z0-9_]*\s*=', line):
                constants.append(line.split('=')[0].strip())
        return constants

    def _find_who_imports(self, target_path: Path) -> List[DependencyInfo]:
        """ëˆ„ê°€ ì´ íŒŒì¼ì„ importí•˜ëŠ”ì§€ ì°¾ê¸°"""
        imported_by = []

        try:
            rel_target = target_path.relative_to(Path.cwd())
            target_module = str(rel_target).replace('/', '.').replace('\\', '.').replace('.py', '')
        except ValueError:
            target_module = str(target_path).replace('.py', '')

        target_name = target_path.stem

        for py_file in self.all_python_files:
            if py_file == target_path:
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    lines = content.split('\n')

                # ë‹¤ì–‘í•œ import íŒ¨í„´ ê²€ìƒ‰
                patterns_to_check = [
                    (f"from.*{target_module}.*import", "direct_import"),
                    (f"import.*{target_module}", "direct_import"),
                    (f"from.*{target_name}.*import", "partial_import"),
                    (f'"{target_module}"', "string_reference"),
                    (f"'{target_module}'", "string_reference"),
                    (f"{target_name}", "name_reference")
                ]

                for i, line in enumerate(lines, 1):
                    for pattern, import_type in patterns_to_check:
                        if re.search(pattern, line, re.IGNORECASE):
                            confidence = self._calculate_import_confidence(line, import_type)

                            imported_by.append(DependencyInfo(
                                file_path=str(py_file),
                                import_type=import_type,
                                context=line.strip(),
                                line_number=i,
                                confidence=confidence
                            ))
            except (UnicodeDecodeError, PermissionError):
                continue

        return imported_by

    def _calculate_import_confidence(self, line: str, import_type: str) -> float:
        """import ì‹ ë¢°ë„ ê³„ì‚°"""
        if import_type == "direct_import":
            return 0.9
        elif import_type == "partial_import":
            return 0.8
        elif import_type == "string_reference":
            if "import" in line or "module" in line:
                return 0.7
            return 0.3
        elif import_type == "name_reference":
            if line.strip().startswith("#"):
                return 0.1
            return 0.4
        return 0.5

    def _analyze_ddd_patterns(self, content: str) -> List[str]:
        """DDD íŒ¨í„´ ë¶„ì„"""
        found_patterns = []

        for pattern_name, patterns in self.ddd_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content, re.IGNORECASE | re.MULTILINE):
                    found_patterns.append(pattern_name)
                    break

        return found_patterns

    def _determine_architecture_role(self, file_path: Path, content: str, classes: List[str], functions: List[str]) -> str:
        """ì•„í‚¤í…ì²˜ì—ì„œì˜ ì—­í•  íŒë‹¨"""
        path_str = str(file_path)

        # ê²½ë¡œ ê¸°ë°˜ íŒë‹¨
        if "domain" in path_str:
            if "entities" in path_str:
                return "Domain Entity"
            elif "services" in path_str:
                return "Domain Service"
            elif "repositories" in path_str:
                return "Domain Repository"
            elif "value_objects" in path_str:
                return "Value Object"
            return "Domain Layer"
        elif "application" in path_str:
            if "use_cases" in path_str:
                return "Use Case"
            elif "services" in path_str:
                return "Application Service"
            elif "queries" in path_str:
                return "Query Handler"
            return "Application Layer"
        elif "infrastructure" in path_str:
            return "Infrastructure Layer"
        elif "ui" in path_str or "presentation" in path_str:
            return "Presentation Layer"
        elif "business_logic" in path_str:
            return "Business Logic"

        # ë‚´ìš© ê¸°ë°˜ íŒë‹¨
        if any("Repository" in cls for cls in classes):
            return "Repository Pattern"
        elif any("Service" in cls for cls in classes):
            return "Service Pattern"
        elif any("UseCase" in cls for cls in classes):
            return "Use Case Pattern"
        elif any("Container" in cls for cls in classes):
            return "Dependency Container"
        elif any("Query" in cls for cls in classes):
            return "Query Pattern"

        return "General Implementation"

    def _collect_evidence(self, target_path: Path, content: str, imported_by: List[DependencyInfo]) -> Tuple[List[str], List[str]]:
        """ì‚¬ìš©/ë¯¸ì‚¬ìš© ì¦ê±° ìˆ˜ì§‘"""
        evidence_for = []
        evidence_against = []

        # ì‚¬ìš© ì¦ê±°
        high_confidence_imports = [dep for dep in imported_by if dep.confidence >= 0.7]
        if high_confidence_imports:
            evidence_for.append(f"ê³ ì‹ ë¢°ë„ import {len(high_confidence_imports)}ê°œ ë°œê²¬")

        # DI íŒ¨í„´ ê²€ìƒ‰
        di_matches = sum(1 for pattern in self.di_patterns if re.search(pattern, content))
        if di_matches > 0:
            evidence_for.append(f"ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ {di_matches}ê°œ ë°œê²¬")

        # ë™ì  ë¡œë”© íŒ¨í„´
        dynamic_matches = sum(1 for pattern in self.dynamic_patterns if re.search(pattern, content))
        if dynamic_matches > 0:
            evidence_for.append(f"ë™ì  ë¡œë”© íŒ¨í„´ {dynamic_matches}ê°œ ë°œê²¬")

        # í´ë˜ìŠ¤/í•¨ìˆ˜ ì¡´ì¬
        if re.search(r"class\s+\w+", content):
            evidence_for.append("í´ë˜ìŠ¤ ì •ì˜ ì¡´ì¬")
        if re.search(r"def\s+\w+", content):
            evidence_for.append("í•¨ìˆ˜ ì •ì˜ ì¡´ì¬")

        # ë¯¸ì‚¬ìš© ì¦ê±°
        if len(imported_by) == 0:
            evidence_against.append("ì–´ë–¤ íŒŒì¼ì—ì„œë„ ì§ì ‘ importë˜ì§€ ì•ŠìŒ")

        low_confidence_only = all(dep.confidence < 0.5 for dep in imported_by)
        if low_confidence_only and imported_by:
            evidence_against.append("ëª¨ë“  ì°¸ì¡°ê°€ ë‚®ì€ ì‹ ë¢°ë„")

        if target_path.stat().st_size == 0:
            evidence_against.append("ë¹ˆ íŒŒì¼")

        # TODO ì½”ë©˜íŠ¸ë‚˜ ë¯¸ì™„ì„± ì½”ë“œ
        if re.search(r"#\s*TODO", content, re.IGNORECASE):
            evidence_against.append("TODO ì£¼ì„ í¬í•¨ (ë¯¸ì™„ì„± ì½”ë“œ)")

        if re.search(r"pass\s*$", content, re.MULTILINE):
            evidence_against.append("pass ë¬¸ë§Œ ìˆëŠ” í•¨ìˆ˜/í´ë˜ìŠ¤ ì¡´ì¬")

        return evidence_for, evidence_against

    def _calculate_usage_probability(self, evidence_for: List[str], evidence_against: List[str], imported_by: List[DependencyInfo]) -> float:
        """ì‚¬ìš© ê°€ëŠ¥ì„± í™•ë¥  ê³„ì‚°"""
        score = 0.5  # ê¸°ë³¸ ì ìˆ˜

        # ì¦ê±° ê¸°ë°˜ ì ìˆ˜ ì¡°ì •
        score += len(evidence_for) * 0.1
        score -= len(evidence_against) * 0.15

        # import ì‹ ë¢°ë„ ê¸°ë°˜ ì¡°ì •
        if imported_by:
            avg_confidence = sum(dep.confidence for dep in imported_by) / len(imported_by)
            score += (avg_confidence - 0.5) * 0.4

        # ë²”ìœ„ ì œí•œ
        return max(0.0, min(1.0, score))

    def _assess_risk(self, usage_probability: float, architecture_role: str, ddd_patterns: List[str]) -> str:
        """ìœ„í—˜ë„ í‰ê°€"""
        if usage_probability >= 0.8:
            return "ğŸ”´ HIGH RISK - ì‚­ì œ ìœ„í—˜"
        elif usage_probability >= 0.6:
            return "ğŸŸ¡ MEDIUM RISK - ì‹ ì¤‘í•œ ê²€í†  í•„ìš”"
        elif usage_probability >= 0.4:
            return "ğŸŸ  LOW-MEDIUM RISK - ì¶”ê°€ ì¡°ì‚¬ ê¶Œì¥"
        elif usage_probability >= 0.2:
            return "ğŸŸ¢ LOW RISK - ì‚­ì œ í›„ë³´"
        else:
            return "âœ… VERY LOW RISK - ì•ˆì „í•œ ì‚­ì œ í›„ë³´"

    def _generate_recommendation(self, usage_probability: float, risk_assessment: str, architecture_role: str) -> str:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        if usage_probability >= 0.8:
            return f"âŒ ì‚­ì œ ê¸ˆì§€ - {architecture_role}ìœ¼ë¡œ ì¤‘ìš”í•œ ì—­í•  ìˆ˜í–‰ ê°€ëŠ¥ì„± ë†’ìŒ"
        elif usage_probability >= 0.6:
            return f"âš ï¸ ì‹ ì¤‘í•œ ê²€í†  í•„ìš” - ì‹¤ì œ ëŸ°íƒ€ì„ í…ŒìŠ¤íŠ¸ ê¶Œì¥"
        elif usage_probability >= 0.4:
            return f"ğŸ” ì¶”ê°€ ì¡°ì‚¬ í•„ìš” - DIë‚˜ ë™ì  ë¡œë”©ìœ¼ë¡œ ì‚¬ìš©ë  ê°€ëŠ¥ì„± ìˆìŒ"
        elif usage_probability >= 0.2:
            return f"ğŸ“‹ ì‚­ì œ ê³ ë ¤ ê°€ëŠ¥ - ë°±ì—… í›„ ì œê±° í…ŒìŠ¤íŠ¸"
        else:
            return f"âœ… ì•ˆì „í•œ ì‚­ì œ - ë¯¸ì‚¬ìš© í™•ë¥  ë†’ìŒ"

    def print_analysis_result(self, result: FileAnalysisResult):
        """ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ íŒŒì¼ ìƒì„¸ ë¶„ì„ ê²°ê³¼")
        print(f"{'='*80}")

        print(f"ğŸ“ íŒŒì¼: {result.target_file}")
        print(f"ğŸ“ í¬ê¸°: {result.file_size:,}B ({result.total_lines}ì¤„)")
        print(f"ğŸ—ï¸ ì•„í‚¤í…ì²˜ ì—­í• : {result.architecture_role}")
        print(f"ğŸ¯ DDD íŒ¨í„´: {', '.join(result.ddd_patterns) if result.ddd_patterns else 'ì—†ìŒ'}")

        print(f"\nğŸ“Š ì‚¬ìš© ê°€ëŠ¥ì„± í‰ê°€:")
        print(f"  í™•ë¥ : {result.usage_probability:.1%}")
        print(f"  ìœ„í—˜ë„: {result.risk_assessment}")
        print(f"  ê¶Œì¥ì‚¬í•­: {result.recommendation}")

        if result.imported_by:
            print(f"\nğŸ”— ì°¸ì¡°í•˜ëŠ” íŒŒì¼ë“¤ ({len(result.imported_by)}ê°œ):")
            for dep in sorted(result.imported_by, key=lambda x: x.confidence, reverse=True)[:10]:
                print(f"  â€¢ {dep.file_path}:{dep.line_number}")
                print(f"    íƒ€ì…: {dep.import_type}, ì‹ ë¢°ë„: {dep.confidence:.1%}")
                print(f"    ì»¨í…ìŠ¤íŠ¸: {dep.context}")
        else:
            print(f"\nğŸ”— ì°¸ì¡°í•˜ëŠ” íŒŒì¼: ì—†ìŒ")

        if result.imports_from:
            print(f"\nğŸ“¦ importí•˜ëŠ” ëª¨ë“ˆë“¤:")
            for imp in result.imports_from[:10]:
                print(f"  â€¢ {imp}")

        if result.class_definitions:
            print(f"\nğŸ›ï¸ í´ë˜ìŠ¤ ì •ì˜ ({len(result.class_definitions)}ê°œ):")
            for cls in result.class_definitions[:5]:
                print(f"  â€¢ {cls}")

        if result.function_definitions:
            print(f"\nâš™ï¸ í•¨ìˆ˜ ì •ì˜ ({len(result.function_definitions)}ê°œ):")
            for func in result.function_definitions[:5]:
                print(f"  â€¢ {func}")

        if result.evidence_for_usage:
            print(f"\nâœ… ì‚¬ìš© ì¦ê±°:")
            for evidence in result.evidence_for_usage:
                print(f"  â€¢ {evidence}")

        if result.evidence_against_usage:
            print(f"\nâŒ ë¯¸ì‚¬ìš© ì¦ê±°:")
            for evidence in result.evidence_against_usage:
                print(f"  â€¢ {evidence}")

        print(f"\n{'='*80}")

def main():
    if len(sys.argv) != 2:
        print("ì‚¬ìš©ë²•: python tools/super_file_dependency_analyzer.py <íŒŒì¼ê²½ë¡œ>")
        print("ì˜ˆì‹œ: python tools/super_file_dependency_analyzer.py upbit_auto_trading/application/queries/query_container.py")
        sys.exit(1)

    target_file = sys.argv[1]
    analyzer = SuperFileDependencyAnalyzer()

    try:
        result = analyzer.analyze_file(target_file)
        analyzer.print_analysis_result(result)

        # JSON ë³´ê³ ì„œ ì €ì¥
        json_output = f"super_file_analysis_{Path(target_file).stem}.json"
        with open(json_output, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {json_output}")

    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
