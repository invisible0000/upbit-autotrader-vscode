# âš¡ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

> **ëª©ì **: Clean Architectureì—ì„œ ì„±ëŠ¥ ë³‘ëª©ì  í•´ê²° ë° ìµœì í™”  
> **ëŒ€ìƒ**: ê°œë°œì, ì„±ëŠ¥ ì—”ì§€ë‹ˆì–´  
> **ì˜ˆìƒ ì½ê¸° ì‹œê°„**: 16ë¶„

## ğŸ¯ ì„±ëŠ¥ ìµœì í™” ì „ëµ

### ğŸ“Š ê³„ì¸µë³„ ì„±ëŠ¥ í¬ì¸íŠ¸
```
Domain Layer      â† ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìµœì í™” (ì•Œê³ ë¦¬ì¦˜, ë©”ëª¨ë¦¬)
Application Layer â† ìœ ìŠ¤ì¼€ì´ìŠ¤ íë¦„ ìµœì í™” (ìºì‹±, ë°°ì¹˜)
Infrastructure    â† I/O ìµœì í™” (DB, API, íŒŒì¼)
Presentation      â† UI ì‘ë‹µì„± ìµœì í™” (ë¹„ë™ê¸°, ë Œë”ë§)
```

## ğŸ’ Domain Layer ìµœì í™”

### 1. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ìµœì í™”
```python
# domain/entities/trading_condition.py
class TradingCondition:
    """ìµœì í™”ëœ ê±°ë˜ ì¡°ê±´"""
    
    def __init__(self):
        # âœ… ê³„ì‚° ê²°ê³¼ ìºì‹±
        self._evaluation_cache = {}
        self._cache_timestamp = None
        self._cache_ttl = 60  # 60ì´ˆ TTL
    
    def evaluate(self, market_data: MarketData) -> bool:
        """ì¡°ê±´ í‰ê°€ - ìºì‹± ì ìš©"""
        
        # ìºì‹œ í‚¤ ìƒì„± (ë°ì´í„° í•´ì‹œ ê¸°ë°˜)
        cache_key = self._generate_cache_key(market_data)
        current_time = datetime.utcnow()
        
        # ìºì‹œ í™•ì¸
        if (cache_key in self._evaluation_cache and 
            self._cache_timestamp and
            (current_time - self._cache_timestamp).seconds < self._cache_ttl):
            return self._evaluation_cache[cache_key]
        
        # ì‹¤ì œ ê³„ì‚° ìˆ˜í–‰
        result = self._perform_evaluation(market_data)
        
        # ìºì‹œ ì €ì¥
        self._evaluation_cache[cache_key] = result
        self._cache_timestamp = current_time
        
        return result
    
    def _generate_cache_key(self, market_data: MarketData) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì¤‘ìš”í•œ ë°ì´í„°ë§Œ í•´ì‹±í•˜ì—¬ í‚¤ ìƒì„±
        key_data = f"{market_data.symbol}_{market_data.timestamp}_{market_data.close_price}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def _perform_evaluation(self, market_data: MarketData) -> bool:
        """ì‹¤ì œ ì¡°ê±´ í‰ê°€ ë¡œì§"""
        # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„
        pass

# domain/services/indicator_calculation_service.py
class IndicatorCalculationService:
    """ì§€í‘œ ê³„ì‚° ì„œë¹„ìŠ¤ - ë©”ëª¨ë¦¬ ìµœì í™”"""
    
    def __init__(self):
        # âœ… ìˆœí™˜ ë²„í¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        self._price_buffer = collections.deque(maxlen=1000)
        self._indicator_cache = LRUCache(maxsize=100)
    
    def calculate_sma(self, prices: List[float], period: int) -> float:
        """SMA ê³„ì‚° - ìµœì í™” ë²„ì „"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"sma_{period}_{hash(tuple(prices[-period:]))}"
        if cache_key in self._indicator_cache:
            return self._indicator_cache[cache_key]
        
        # íš¨ìœ¨ì ì¸ ê³„ì‚° (numpy í™œìš©)
        if len(prices) < period:
            return None
        
        # âœ… numpyë¡œ ë²¡í„°í™” ì—°ì‚°
        import numpy as np
        result = np.mean(prices[-period:])
        
        # ìºì‹œ ì €ì¥
        self._indicator_cache[cache_key] = result
        return result
    
    def calculate_rsi(self, prices: List[float], period: int = 14) -> float:
        """RSI ê³„ì‚° - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „"""
        if len(prices) < period + 1:
            return None
        
        # âœ… ì¦ë¶„ ê³„ì‚°ìœ¼ë¡œ ì „ì²´ ì¬ê³„ì‚° ë°©ì§€
        gains = []
        losses = []
        
        for i in range(1, len(prices)):
            change = prices[i] - prices[i-1]
            if change > 0:
                gains.append(change)
                losses.append(0)
            else:
                gains.append(0)
                losses.append(abs(change))
        
        if len(gains) < period:
            return None
        
        avg_gain = sum(gains[-period:]) / period
        avg_loss = sum(losses[-period:]) / period
        
        if avg_loss == 0:
            return 100
        
        rs = avg_gain / avg_loss
        return 100 - (100 / (1 + rs))
```

### 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”
```python
# domain/common/memory_manager.py
class DomainMemoryManager:
    """ë„ë©”ì¸ ê°ì²´ ë©”ëª¨ë¦¬ ê´€ë¦¬"""
    
    def __init__(self):
        self._object_pools = {}
        self._weak_references = weakref.WeakSet()
    
    def get_pooled_object(self, object_type: type, *args, **kwargs):
        """ê°ì²´ í’€ì—ì„œ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì²´ ê°€ì ¸ì˜¤ê¸°"""
        pool_key = object_type.__name__
        
        if pool_key not in self._object_pools:
            self._object_pools[pool_key] = []
        
        pool = self._object_pools[pool_key]
        
        # í’€ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ê°ì²´ ì°¾ê¸°
        for obj in pool:
            if obj.is_reusable():
                obj.reset(*args, **kwargs)
                return obj
        
        # ìƒˆ ê°ì²´ ìƒì„±
        new_obj = object_type(*args, **kwargs)
        pool.append(new_obj)
        self._weak_references.add(new_obj)
        
        return new_obj
    
    def cleanup_unused_objects(self):
        """ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê°ì²´ ì •ë¦¬"""
        for pool in self._object_pools.values():
            pool[:] = [obj for obj in pool if obj.is_in_use()]
```

## âš™ï¸ Application Layer ìµœì í™”

### 1. ì„œë¹„ìŠ¤ ë ˆë²¨ ìºì‹±
```python
# application/services/condition_query_service.py
class ConditionQueryService:
    """ì¡°ê±´ ì¡°íšŒ ì„œë¹„ìŠ¤ - ìºì‹± ìµœì í™”"""
    
    def __init__(self, condition_repo, cache_manager):
        self.condition_repo = condition_repo
        self.cache = cache_manager
    
    @cached(ttl=300)  # 5ë¶„ ìºì‹œ
    def get_active_conditions(self) -> List[ConditionDto]:
        """í™œì„± ì¡°ê±´ ëª©ë¡ ì¡°íšŒ - ìºì‹± ì ìš©"""
        conditions = self.condition_repo.find_all_active()
        return [ConditionDto.from_domain(c) for c in conditions]
    
    @cached(ttl=60, key_generator=lambda self, variable_id: f"compat_{variable_id}")
    def get_compatible_variables(self, variable_id: str) -> List[VariableDto]:
        """í˜¸í™˜ ë³€ìˆ˜ ëª©ë¡ - ë³€ìˆ˜ë³„ ìºì‹±"""
        variables = self.condition_repo.find_compatible_variables(variable_id)
        return [VariableDto.from_domain(v) for v in variables]
    
    def invalidate_condition_cache(self, condition_id: str = None):
        """ì¡°ê±´ ìºì‹œ ë¬´íš¨í™”"""
        if condition_id:
            self.cache.delete(f"condition_{condition_id}")
        else:
            self.cache.delete_pattern("condition_*")

# application/common/cache_manager.py
class CacheManager:
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìºì‹œ ê´€ë¦¬"""
    
    def __init__(self):
        self._cache = {}
        self._ttl_data = {}
        self._cleanup_interval = 300  # 5ë¶„ë§ˆë‹¤ ì •ë¦¬
        self._last_cleanup = datetime.utcnow()
    
    def get(self, key: str):
        """ìºì‹œì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°"""
        self._cleanup_if_needed()
        
        if key in self._cache:
            ttl_info = self._ttl_data.get(key)
            if ttl_info and datetime.utcnow() > ttl_info['expires_at']:
                self.delete(key)
                return None
            return self._cache[key]
        return None
    
    def set(self, key: str, value, ttl: int = None):
        """ìºì‹œì— ê°’ ì €ì¥"""
        self._cache[key] = value
        
        if ttl:
            self._ttl_data[key] = {
                'expires_at': datetime.utcnow() + timedelta(seconds=ttl)
            }
    
    def delete(self, key: str):
        """ìºì‹œì—ì„œ ê°’ ì‚­ì œ"""
        self._cache.pop(key, None)
        self._ttl_data.pop(key, None)
    
    def delete_pattern(self, pattern: str):
        """íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìºì‹œ ì‚­ì œ"""
        import re
        regex = re.compile(pattern.replace('*', '.*'))
        
        keys_to_delete = [
            key for key in self._cache.keys() 
            if regex.match(key)
        ]
        
        for key in keys_to_delete:
            self.delete(key)
    
    def _cleanup_if_needed(self):
        """ë§Œë£Œëœ ìºì‹œ í•­ëª© ì •ë¦¬"""
        now = datetime.utcnow()
        if (now - self._last_cleanup).seconds > self._cleanup_interval:
            self._cleanup_expired_items()
            self._last_cleanup = now
    
    def _cleanup_expired_items(self):
        """ë§Œë£Œëœ í•­ëª© ì •ë¦¬"""
        now = datetime.utcnow()
        expired_keys = [
            key for key, ttl_info in self._ttl_data.items()
            if now > ttl_info['expires_at']
        ]
        
        for key in expired_keys:
            self.delete(key)
```

### 2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
# application/services/batch_condition_service.py
class BatchConditionService:
    """ë°°ì¹˜ ì¡°ê±´ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, condition_repo, event_publisher):
        self.condition_repo = condition_repo
        self.event_publisher = event_publisher
        self.batch_size = 100
    
    def process_conditions_batch(self, market_data: MarketData):
        """ì¡°ê±´ë“¤ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
        
        # âœ… í˜ì´ì§•ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        offset = 0
        total_processed = 0
        
        while True:
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¡°ê±´ ì¡°íšŒ
            conditions = self.condition_repo.find_active_conditions_paginated(
                offset=offset, 
                limit=self.batch_size
            )
            
            if not conditions:
                break
            
            # âœ… ë³‘ë ¬ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
            evaluation_results = self._evaluate_conditions_parallel(
                conditions, market_data
            )
            
            # âœ… ë°°ì¹˜ë¡œ ì´ë²¤íŠ¸ ë°œí–‰
            events = []
            for condition, result in evaluation_results:
                if result:
                    events.append(ConditionActivatedEvent(condition.id))
            
            if events:
                self.event_publisher.publish_batch(events)
            
            total_processed += len(conditions)
            offset += self.batch_size
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del conditions, evaluation_results, events
            gc.collect()
        
        return total_processed
    
    def _evaluate_conditions_parallel(self, conditions, market_data):
        """ì¡°ê±´ë“¤ì„ ë³‘ë ¬ë¡œ í‰ê°€"""
        import concurrent.futures
        
        def evaluate_single(condition):
            try:
                result = condition.evaluate(market_data)
                return (condition, result)
            except Exception as e:
                logger.error(f"ì¡°ê±´ í‰ê°€ ì‹¤íŒ¨: {condition.id}, ì˜¤ë¥˜: {str(e)}")
                return (condition, False)
        
        # âœ… ThreadPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(evaluate_single, condition) for condition in conditions]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        return results
```

## ğŸ”Œ Infrastructure Layer ìµœì í™”

### 1. ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
```python
# infrastructure/repositories/optimized_condition_repository.py
class OptimizedConditionRepository:
    """ìµœì í™”ëœ ì¡°ê±´ Repository"""
    
    def __init__(self, db_connection):
        self.db = db_connection
        self._setup_connection_optimizations()
    
    def _setup_connection_optimizations(self):
        """DB ì—°ê²° ìµœì í™” ì„¤ì •"""
        # âœ… WAL ëª¨ë“œë¡œ ì½ê¸° ì„±ëŠ¥ í–¥ìƒ
        self.db.execute("PRAGMA journal_mode=WAL")
        
        # âœ… ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì¦ê°€
        self.db.execute("PRAGMA cache_size=10000")  # 10MB
        
        # âœ… ë™ê¸°í™” ëª¨ë“œ ìµœì í™” (ì•ˆì „ì„± vs ì„±ëŠ¥)
        self.db.execute("PRAGMA synchronous=NORMAL")
        
        # âœ… ì„ì‹œ ì €ì¥ì†Œë¥¼ ë©”ëª¨ë¦¬ë¡œ
        self.db.execute("PRAGMA temp_store=MEMORY")
    
    def find_active_conditions_batch(self, symbol: str, limit: int = 1000):
        """í™œì„± ì¡°ê±´ ë°°ì¹˜ ì¡°íšŒ - ìµœì í™”ëœ ì¿¼ë¦¬"""
        
        # âœ… ì¸ë±ìŠ¤ í™œìš© ë° í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¡°íšŒ
        query = """
        SELECT c.id, c.variable_id, c.operator, c.target_value, c.parameters,
               v.name as variable_name, v.comparison_group
        FROM trading_conditions c
        INNER JOIN trading_variables v ON c.variable_id = v.id
        WHERE c.is_active = 1 AND c.symbol = ?
        ORDER BY c.priority DESC, c.created_at ASC
        LIMIT ?
        """
        
        return self.db.fetchall(query, (symbol, limit))
    
    def bulk_insert_conditions(self, conditions: List[TradingCondition]):
        """ì¡°ê±´ ì¼ê´„ ì‚½ì… - íŠ¸ëœì­ì…˜ ìµœì í™”"""
        
        # âœ… ë‹¨ì¼ íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë°°ì¹˜ ì‚½ì…
        query = """
        INSERT INTO trading_conditions 
        (id, variable_id, operator, target_value, parameters, created_at, is_active)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """
        
        condition_data = [
            (
                condition.id.value,
                condition.variable.id.value,
                condition.operator,
                str(condition.target_value),
                json.dumps(condition.parameters.to_dict()),
                condition.created_at.isoformat(),
                True
            )
            for condition in conditions
        ]
        
        with self.db.transaction():
            self.db.executemany(query, condition_data)
    
    def find_conditions_with_prepared_statement(self, variable_ids: List[str]):
        """ì¤€ë¹„ëœ ë¬¸ì¥ìœ¼ë¡œ ì¡°ê±´ ì¡°íšŒ"""
        
        # âœ… IN ì ˆ ìµœì í™”
        placeholders = ','.join(['?' for _ in variable_ids])
        query = f"""
        SELECT * FROM trading_conditions 
        WHERE variable_id IN ({placeholders}) AND is_active = 1
        """
        
        return self.db.fetchall(query, variable_ids)

# infrastructure/database/connection_pool.py
class SQLiteConnectionPool:
    """SQLite ì—°ê²° í’€"""
    
    def __init__(self, db_path: str, pool_size: int = 5):
        self.db_path = db_path
        self.pool_size = pool_size
        self._pool = queue.Queue(maxsize=pool_size)
        self._create_connections()
    
    def _create_connections(self):
        """ì—°ê²° í’€ ìƒì„±"""
        for _ in range(self.pool_size):
            conn = sqlite3.connect(
                self.db_path,
                check_same_thread=False,
                timeout=30.0
            )
            conn.row_factory = sqlite3.Row
            self._pool.put(conn)
    
    @contextmanager
    def get_connection(self):
        """ì—°ê²° ê°€ì ¸ì˜¤ê¸°"""
        conn = self._pool.get(timeout=10)
        try:
            yield conn
        finally:
            self._pool.put(conn)
```

### 2. API í´ë¼ì´ì–¸íŠ¸ ìµœì í™”
```python
# infrastructure/api_clients/optimized_upbit_client.py
class OptimizedUpbitClient:
    """ìµœì í™”ëœ ì—…ë¹„íŠ¸ API í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        # âœ… ì—°ê²° í’€ë§ìœ¼ë¡œ ì—°ê²° ì¬ì‚¬ìš©
        self.session = requests.Session()
        adapter = requests.adapters.HTTPAdapter(
            pool_connections=10,
            pool_maxsize=20,
            max_retries=3
        )
        self.session.mount('https://', adapter)
        
        # âœ… ì‘ë‹µ ìºì‹œ
        self._response_cache = TTLCache(maxsize=1000, ttl=60)
        
        # âœ… Rate limiting
        self._rate_limiter = RateLimiter(max_calls=10, period=1)
    
    @cached_property
    def _session_with_retries(self):
        """ì¬ì‹œë„ ë¡œì§ì´ ìˆëŠ” ì„¸ì…˜"""
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("https://", adapter)
        return self.session
    
    async def get_candle_data_async(self, symbol: str, timeframe: str):
        """ë¹„ë™ê¸° ìº”ë“¤ ë°ì´í„° ì¡°íšŒ"""
        
        # ìºì‹œ í™•ì¸
        cache_key = f"{symbol}_{timeframe}"
        if cache_key in self._response_cache:
            return self._response_cache[cache_key]
        
        # Rate limiting ì ìš©
        with self._rate_limiter:
            async with aiohttp.ClientSession() as session:
                url = f"https://api.upbit.com/v1/candles/{timeframe}"
                params = {"market": symbol, "count": 200}
                
                async with session.get(url, params=params) as response:
                    data = await response.json()
                    
                    # ìºì‹œ ì €ì¥
                    self._response_cache[cache_key] = data
                    return data
    
    def get_multiple_symbols_data(self, symbols: List[str], timeframe: str):
        """ì—¬ëŸ¬ ì‹¬ë³¼ ë°ì´í„° ë™ì‹œ ì¡°íšŒ"""
        
        async def fetch_all():
            tasks = [
                self.get_candle_data_async(symbol, timeframe)
                for symbol in symbols
            ]
            return await asyncio.gather(*tasks)
        
        return asyncio.run(fetch_all())
```

## ğŸ¨ Presentation Layer ìµœì í™”

### 1. UI ì‘ë‹µì„± ìµœì í™”
```python
# presentation/views/optimized_condition_list_view.py
class OptimizedConditionListView(QWidget):
    """ìµœì í™”ëœ ì¡°ê±´ ëª©ë¡ View"""
    
    def __init__(self):
        super().__init__()
        self.condition_model = ConditionTableModel()
        self.proxy_model = QSortFilterProxyModel()
        
        # âœ… ê°€ìƒí™”ëœ í…Œì´ë¸”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
        self.table_view = QTableView()
        self.table_view.setModel(self.proxy_model)
        self.proxy_model.setSourceModel(self.condition_model)
        
        # âœ… ì§€ì—° ë¡œë”© ì„¤ì •
        self.table_view.setVerticalScrollMode(QAbstractItemView.ScrollMode.ScrollPerPixel)
        self.table_view.verticalScrollBar().valueChanged.connect(self._on_scroll)
        
        self._current_page = 0
        self._page_size = 50
        self._loading = False
    
    def _on_scroll(self, value):
        """ìŠ¤í¬ë¡¤ ì‹œ ì§€ì—° ë¡œë”©"""
        scrollbar = self.table_view.verticalScrollBar()
        
        # ìŠ¤í¬ë¡¤ì´ í•˜ë‹¨ 90%ì— ë„ë‹¬í•˜ë©´ ì¶”ê°€ ë¡œë”©
        if (value >= scrollbar.maximum() * 0.9 and 
            not self._loading and 
            self.condition_model.can_fetch_more()):
            
            self._loading = True
            self._load_more_conditions()
    
    def _load_more_conditions(self):
        """ì¶”ê°€ ì¡°ê±´ ë¡œë”© - ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ"""
        
        def load_conditions():
            """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¡°ê±´ ë¡œë”©"""
            try:
                offset = self._current_page * self._page_size
                conditions = self.presenter.load_conditions_paginated(
                    offset=offset, 
                    limit=self._page_size
                )
                return conditions
            except Exception as e:
                logger.error(f"ì¡°ê±´ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
                return []
        
        # âœ… QThreadë¡œ ë°±ê·¸ë¼ìš´ë“œ ë¡œë”©
        self.loader_thread = ConditionLoaderThread(load_conditions)
        self.loader_thread.conditions_loaded.connect(self._on_conditions_loaded)
        self.loader_thread.start()
    
    def _on_conditions_loaded(self, conditions):
        """ì¡°ê±´ ë¡œë”© ì™„ë£Œ ì²˜ë¦¬"""
        self.condition_model.append_conditions(conditions)
        self._current_page += 1
        self._loading = False

class ConditionLoaderThread(QThread):
    """ì¡°ê±´ ë¡œë”© ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ"""
    conditions_loaded = pyqtSignal(list)
    
    def __init__(self, load_function):
        super().__init__()
        self.load_function = load_function
    
    def run(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰"""
        conditions = self.load_function()
        self.conditions_loaded.emit(conditions)
```

### 2. ì°¨íŠ¸ ë Œë”ë§ ìµœì í™”
```python
# presentation/widgets/optimized_chart_widget.py
class OptimizedChartWidget(QWidget):
    """ìµœì í™”ëœ ì°¨íŠ¸ ìœ„ì ¯"""
    
    def __init__(self):
        super().__init__()
        # âœ… OpenGL ê°€ì† í™œì„±í™”
        self.setOpenGLEnabled(True)
        
        # âœ… ë°ì´í„° ë‹¤ìš´ìƒ˜í”Œë§
        self.max_data_points = 1000
        self._last_render_time = 0
        self._render_interval = 16  # 60 FPS
        
        # âœ… ë Œë”ë§ ìºì‹œ
        self._render_cache = {}
        self._cache_key = None
    
    def update_chart_data(self, data: List[dict]):
        """ì°¨íŠ¸ ë°ì´í„° ì—…ë°ì´íŠ¸ - ìµœì í™”"""
        
        # âœ… ë‹¤ìš´ìƒ˜í”Œë§ìœ¼ë¡œ ë°ì´í„° í¬ì¸íŠ¸ ì œí•œ
        if len(data) > self.max_data_points:
            step = len(data) // self.max_data_points
            data = data[::step]
        
        # âœ… ìºì‹œ í‚¤ ìƒì„±
        new_cache_key = self._generate_cache_key(data)
        if new_cache_key == self._cache_key:
            return  # ë°ì´í„° ë³€ê²½ ì—†ìŒ
        
        self._cache_key = new_cache_key
        
        # âœ… í”„ë ˆì„ ë ˆì´íŠ¸ ì œí•œ
        current_time = time.time() * 1000
        if current_time - self._last_render_time < self._render_interval:
            return
        
        self._render_chart(data)
        self._last_render_time = current_time
    
    def _render_chart(self, data):
        """ì‹¤ì œ ì°¨íŠ¸ ë Œë”ë§"""
        # ì°¨íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ ìµœì í™”ëœ ë Œë”ë§ ë¡œì§
        pass
    
    def _generate_cache_key(self, data):
        """ìºì‹œ í‚¤ ìƒì„±"""
        if not data:
            return None
        
        # ì²«ë²ˆì§¸, ë§ˆì§€ë§‰, ì¤‘ê°„ê°’ìœ¼ë¡œ ê°„ë‹¨í•œ í•´ì‹œ ìƒì„±
        key_points = [
            data[0]['timestamp'],
            data[-1]['timestamp'],
            len(data)
        ]
        return hash(tuple(key_points))
```

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# shared/monitoring/performance_monitor.py
class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self.metrics = {}
        self.start_times = {}
    
    @contextmanager
    def measure(self, operation_name: str):
        """ì‘ì—… ì„±ëŠ¥ ì¸¡ì •"""
        start_time = time.perf_counter()
        memory_before = psutil.Process().memory_info().rss
        
        try:
            yield
        finally:
            end_time = time.perf_counter()
            memory_after = psutil.Process().memory_info().rss
            
            duration = end_time - start_time
            memory_used = memory_after - memory_before
            
            self._record_metric(operation_name, duration, memory_used)
    
    def _record_metric(self, operation: str, duration: float, memory: int):
        """ë©”íŠ¸ë¦­ ê¸°ë¡"""
        if operation not in self.metrics:
            self.metrics[operation] = {
                'count': 0,
                'total_duration': 0,
                'max_duration': 0,
                'total_memory': 0,
                'max_memory': 0
            }
        
        metric = self.metrics[operation]
        metric['count'] += 1
        metric['total_duration'] += duration
        metric['max_duration'] = max(metric['max_duration'], duration)
        metric['total_memory'] += memory
        metric['max_memory'] = max(metric['max_memory'], memory)
    
    def get_performance_report(self) -> dict:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = {}
        for operation, metric in self.metrics.items():
            if metric['count'] > 0:
                report[operation] = {
                    'avg_duration': metric['total_duration'] / metric['count'],
                    'max_duration': metric['max_duration'],
                    'avg_memory_mb': metric['total_memory'] / metric['count'] / 1024 / 1024,
                    'max_memory_mb': metric['max_memory'] / 1024 / 1024,
                    'call_count': metric['count']
                }
        return report

# ì‚¬ìš© ì˜ˆì‹œ
performance_monitor = PerformanceMonitor()

with performance_monitor.measure("condition_evaluation"):
    result = condition.evaluate(market_data)
```

## ğŸ” ë‹¤ìŒ ë‹¨ê³„

- **[ì—ëŸ¬ ì²˜ë¦¬](11_ERROR_HANDLING.md)**: ì„±ëŠ¥ ìµœì í™”ì™€ ì•ˆì •ì„± ê· í˜•
- **[ë””ë²„ê¹… ê°€ì´ë“œ](15_DEBUGGING_GUIDE.md)**: ì„±ëŠ¥ ì´ìŠˆ ì§„ë‹¨ ë°©ë²•
- **[í…ŒìŠ¤íŒ… ì „ëµ](16_TESTING_STRATEGY.md)**: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ êµ¬í˜„

---
**ğŸ’¡ í•µì‹¬**: "Clean Architectureì—ì„œëŠ” ê° ê³„ì¸µë³„ë¡œ ì ì ˆí•œ ìµœì í™” ì „ëµì„ ì ìš©í•˜ì—¬ ì „ì²´ ì‹œìŠ¤í…œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤!"
