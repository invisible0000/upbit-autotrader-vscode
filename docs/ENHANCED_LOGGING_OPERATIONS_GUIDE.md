# LLM ì—ì´ì „íŠ¸ ë¡œê¹… ì‹œìŠ¤í…œ v4.0 ìš´ì˜ ê°€ì´ë“œ

## ğŸƒâ€â™‚ï¸ í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬

### 1. ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **Python**: 3.8 ì´ìƒ
- **ë©”ëª¨ë¦¬**: ìµœì†Œ 2GB RAM
- **ë””ìŠ¤í¬**: ìµœì†Œ 1GB ì—¬ìœ  ê³µê°„
- **CPU**: ë“€ì–¼ ì½”ì–´ ì´ìƒ

#### ê¶Œì¥ ì‚¬ì–‘ (ê³ ë¶€í•˜ í™˜ê²½)
- **Python**: 3.11 ì´ìƒ
- **ë©”ëª¨ë¦¬**: 8GB RAM ì´ìƒ
- **ë””ìŠ¤í¬**: SSD 5GB ì´ìƒ
- **CPU**: ì¿¼ë“œ ì½”ì–´ ì´ìƒ

#### ì˜ì¡´ì„± íŒ¨í‚¤ì§€
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€
pip install psutil
pip install aiofiles
pip install pyyaml

# ì„ íƒì  íŒ¨í‚¤ì§€ (ì„±ëŠ¥ í–¥ìƒ)
pip install uvloop      # ë” ë¹ ë¥¸ ì´ë²¤íŠ¸ ë£¨í”„ (Linux/macOS)
pip install orjson      # ë” ë¹ ë¥¸ JSON ì²˜ë¦¬
```

### 2. í”„ë¡œë•ì…˜ ì„¤ì •

#### config/enhanced_logging.production.yaml
```yaml
core:
  enabled: true
  log_level: "INFO"
  max_log_file_size_mb: 100
  backup_count: 10

briefing:
  enabled: true
  update_interval: 300        # 5ë¶„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
  auto_generate: true
  include_performance_metrics: true

dashboard:
  enabled: true
  realtime_updates: true
  update_interval: 60         # 1ë¶„ë§ˆë‹¤ ì—…ë°ì´íŠ¸
  max_dashboard_history: 1000

performance:
  async_processing: true
  queue_size: 50000          # ëŒ€ìš©ëŸ‰ í
  worker_count: 6            # CPU ì½”ì–´ ìˆ˜ì— ë§ì¶° ì¡°ì •
  batch_size: 500            # ëŒ€ìš©ëŸ‰ ë°°ì¹˜

  memory_optimization: true
  memory_threshold_mb: 2000   # 2GB ì„ê³„ê°’
  gc_threshold_factor: 2.0
  monitoring_interval: 60

  caching_enabled: true
  default_cache_size: 10000
  default_cache_ttl: 3600    # 1ì‹œê°„

  performance_monitoring: true
  metric_collection_interval: 30
  history_retention_hours: 24
```

### 3. ì„œë¹„ìŠ¤ ë“±ë¡ ë° ê´€ë¦¬

#### systemd ì„œë¹„ìŠ¤ íŒŒì¼ (Linux)
```ini
# /etc/systemd/system/upbit-logging.service
[Unit]
Description=Upbit Auto Trading Logging Service
After=network.target

[Service]
Type=forking
User=upbit
Group=upbit
WorkingDirectory=/opt/upbit-autotrader
Environment=PYTHONPATH=/opt/upbit-autotrader
ExecStart=/opt/upbit-autotrader/venv/bin/python -m upbit_auto_trading.infrastructure.logging.service
ExecReload=/bin/kill -HUP $MAINPID
KillMode=mixed
Restart=always
RestartSec=5

# ë¦¬ì†ŒìŠ¤ ì œí•œ
MemoryLimit=4G
CPUQuota=200%

[Install]
WantedBy=multi-user.target
```

```bash
# ì„œë¹„ìŠ¤ ë“±ë¡ ë° ì‹œì‘
sudo systemctl daemon-reload
sudo systemctl enable upbit-logging
sudo systemctl start upbit-logging

# ìƒíƒœ í™•ì¸
sudo systemctl status upbit-logging

# ë¡œê·¸ í™•ì¸
sudo journalctl -u upbit-logging -f
```

#### Windows ì„œë¹„ìŠ¤ (NSSM ì‚¬ìš©)
```cmd
# NSSMìœ¼ë¡œ ì„œë¹„ìŠ¤ ìƒì„±
nssm install "Upbit Logging Service" "C:\Python311\python.exe"
nssm set "Upbit Logging Service" Arguments "-m upbit_auto_trading.infrastructure.logging.service"
nssm set "Upbit Logging Service" AppDirectory "C:\upbit-autotrader"
nssm set "Upbit Logging Service" Start SERVICE_AUTO_START

# ì„œë¹„ìŠ¤ ì‹œì‘
net start "Upbit Logging Service"
```

### 4. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

#### Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# upbit_auto_trading/infrastructure/logging/monitoring/prometheus_exporter.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time

class PrometheusExporter:
    def __init__(self, port=8000):
        self.port = port

        # ë©”íŠ¸ë¦­ ì •ì˜
        self.log_entries_total = Counter('logging_entries_total',
                                       'Total log entries processed',
                                       ['level', 'component'])

        self.log_processing_duration = Histogram('logging_processing_duration_seconds',
                                               'Log processing duration')

        self.system_health_score = Gauge('system_health_score',
                                        'Current system health score (0-100)')

        self.memory_usage_mb = Gauge('memory_usage_mb',
                                   'Current memory usage in MB')

        self.cache_hit_rate = Gauge('cache_hit_rate',
                                  'Cache hit rate percentage',
                                  ['cache_name'])

    def start_server(self):
        start_http_server(self.port)
        print(f"Prometheus metrics server started on port {self.port}")

    def record_log_entry(self, level, component):
        self.log_entries_total.labels(level=level, component=component).inc()

    def record_processing_time(self, duration_seconds):
        self.log_processing_duration.observe(duration_seconds)

    def update_system_health(self, score):
        self.system_health_score.set(score)

    def update_memory_usage(self, memory_mb):
        self.memory_usage_mb.set(memory_mb)

    def update_cache_hit_rate(self, cache_name, hit_rate):
        self.cache_hit_rate.labels(cache_name=cache_name).set(hit_rate)

# ì‚¬ìš© ì˜ˆì œ
exporter = PrometheusExporter(port=8000)
exporter.start_server()
```

#### Grafana ëŒ€ì‹œë³´ë“œ ì„¤ì •
```json
{
  "dashboard": {
    "title": "Upbit Logging System Monitoring",
    "panels": [
      {
        "title": "Log Entries per Second",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(logging_entries_total[5m])",
            "legendFormat": "{{level}} - {{component}}"
          }
        ]
      },
      {
        "title": "System Health Score",
        "type": "singlestat",
        "targets": [
          {
            "expr": "system_health_score",
            "legendFormat": "Health Score"
          }
        ],
        "thresholds": "50,80"
      },
      {
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "memory_usage_mb",
            "legendFormat": "Memory (MB)"
          }
        ]
      },
      {
        "title": "Cache Hit Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "cache_hit_rate",
            "legendFormat": "{{cache_name}}"
          }
        ]
      }
    ]
  }
}
```

### 5. ë°±ì—… ë° ë³µêµ¬

#### ìë™ ë°±ì—… ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# backup_logging_data.sh

BACKUP_DIR="/backup/upbit-logging"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_PATH="$BACKUP_DIR/backup_$DATE"

# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$BACKUP_PATH"

# ì„¤ì • íŒŒì¼ ë°±ì—…
cp -r /opt/upbit-autotrader/config "$BACKUP_PATH/"

# ë¡œê·¸ íŒŒì¼ ë°±ì—…
cp -r /opt/upbit-autotrader/logs "$BACKUP_PATH/"

# ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…
cp -r /opt/upbit-autotrader/data "$BACKUP_PATH/"

# ì••ì¶•
tar -czf "$BACKUP_PATH.tar.gz" -C "$BACKUP_DIR" "backup_$DATE"
rm -rf "$BACKUP_PATH"

# ì˜¤ë˜ëœ ë°±ì—… ì •ë¦¬ (30ì¼ ì´ìƒ)
find "$BACKUP_DIR" -name "backup_*.tar.gz" -mtime +30 -delete

echo "Backup completed: $BACKUP_PATH.tar.gz"
```

#### crontab ì„¤ì •
```bash
# ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ë°±ì—… ì‹¤í–‰
0 2 * * * /opt/upbit-autotrader/scripts/backup_logging_data.sh

# ë§¤ì‹œê°„ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
0 * * * * /opt/upbit-autotrader/scripts/health_check.sh
```

#### ë³µêµ¬ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# restore_logging_data.sh

BACKUP_FILE="$1"
RESTORE_DIR="/opt/upbit-autotrader"

if [ -z "$BACKUP_FILE" ]; then
    echo "Usage: $0 <backup_file.tar.gz>"
    exit 1
fi

# ì„œë¹„ìŠ¤ ì¤‘ì§€
sudo systemctl stop upbit-logging

# í˜„ì¬ ë°ì´í„° ë°±ì—…
mv "$RESTORE_DIR/config" "$RESTORE_DIR/config.old"
mv "$RESTORE_DIR/logs" "$RESTORE_DIR/logs.old"
mv "$RESTORE_DIR/data" "$RESTORE_DIR/data.old"

# ë°±ì—… íŒŒì¼ ë³µì›
tar -xzf "$BACKUP_FILE" -C /tmp/
BACKUP_DIR=$(basename "$BACKUP_FILE" .tar.gz)

cp -r "/tmp/$BACKUP_DIR/config" "$RESTORE_DIR/"
cp -r "/tmp/$BACKUP_DIR/logs" "$RESTORE_DIR/"
cp -r "/tmp/$BACKUP_DIR/data" "$RESTORE_DIR/"

# ê¶Œí•œ ì„¤ì •
chown -R upbit:upbit "$RESTORE_DIR"

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start upbit-logging

echo "Restore completed from $BACKUP_FILE"
```

### 6. ë¡œê·¸ ìˆœí™˜ ë° ì •ë¦¬

#### logrotate ì„¤ì •
```
# /etc/logrotate.d/upbit-logging
/opt/upbit-autotrader/logs/*.log {
    daily
    rotate 30
    compress
    delaycompress
    missingok
    notifempty
    create 644 upbit upbit
    postrotate
        /bin/kill -HUP $(cat /var/run/upbit-logging.pid 2>/dev/null) 2>/dev/null || true
    endscript
}
```

#### ìë™ ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
```python
# cleanup_old_data.py
import os
import time
from datetime import datetime, timedelta
from pathlib import Path

class DataCleanupManager:
    def __init__(self, base_path="/opt/upbit-autotrader"):
        self.base_path = Path(base_path)

    def cleanup_old_logs(self, days_to_keep=30):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        logs_dir = self.base_path / "logs"
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)

        for log_file in logs_dir.glob("*.log.*"):
            if log_file.stat().st_mtime < cutoff_date.timestamp():
                log_file.unlink()
                print(f"Deleted old log: {log_file}")

    def cleanup_dashboard_data(self, hours_to_keep=168):  # 7ì¼
        """ì˜¤ë˜ëœ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì •ë¦¬"""
        dashboard_dir = self.base_path / "data" / "dashboard"
        cutoff_date = datetime.now() - timedelta(hours=hours_to_keep)

        for data_file in dashboard_dir.glob("dashboard_*.json"):
            if data_file.stat().st_mtime < cutoff_date.timestamp():
                data_file.unlink()
                print(f"Deleted old dashboard data: {data_file}")

    def cleanup_performance_metrics(self, days_to_keep=7):
        """ì˜¤ë˜ëœ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì •ë¦¬"""
        metrics_dir = self.base_path / "data" / "metrics"
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)

        for metrics_file in metrics_dir.glob("metrics_*.json"):
            if metrics_file.stat().st_mtime < cutoff_date.timestamp():
                metrics_file.unlink()
                print(f"Deleted old metrics: {metrics_file}")

if __name__ == "__main__":
    cleanup_manager = DataCleanupManager()
    cleanup_manager.cleanup_old_logs(30)
    cleanup_manager.cleanup_dashboard_data(168)
    cleanup_manager.cleanup_performance_metrics(7)
```

### 7. ì„±ëŠ¥ íŠœë‹

#### ê³ ë¶€í•˜ í™˜ê²½ ìµœì í™”
```python
# ê³ ì„±ëŠ¥ ì„¤ì • ì˜ˆì œ
async_processor_config = {
    "queue_size": 100000,      # ëŒ€ìš©ëŸ‰ í
    "worker_count": 8,         # CPU ì½”ì–´ ìˆ˜ Ã— 2
    "batch_size": 1000,        # ëŒ€ìš©ëŸ‰ ë°°ì¹˜
    "priority_queue_enabled": True,
    "compression_enabled": True  # ë©”ëª¨ë¦¬ ì ˆì•½
}

memory_optimizer_config = {
    "memory_threshold_mb": 4000,    # 4GB ì„ê³„ê°’
    "gc_threshold_factor": 1.5,     # ì ê·¹ì  GC
    "monitoring_interval": 30,      # ìì£¼ ëª¨ë‹ˆí„°ë§
    "aggressive_cleanup": True      # ì ê·¹ì  ì •ë¦¬
}

cache_manager_config = {
    "l1_cache_size": 1000,         # ë¹ ë¥¸ L1 ìºì‹œ
    "l2_cache_size": 50000,        # ëŒ€ìš©ëŸ‰ L2 ìºì‹œ
    "l3_cache_size": 500000,       # ì´ˆëŒ€ìš©ëŸ‰ L3 ìºì‹œ
    "intelligent_eviction": True,   # ì§€ëŠ¥í˜• ì œê±°
    "compression_enabled": True     # ì••ì¶• ì €ì¥
}
```

#### ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”
```sql
-- ì¸ë±ìŠ¤ ìµœì í™”
CREATE INDEX idx_logs_timestamp ON logs(timestamp);
CREATE INDEX idx_logs_level ON logs(level);
CREATE INDEX idx_logs_component ON logs(component);
CREATE UNIQUE INDEX idx_logs_composite ON logs(timestamp, component, level);

-- íŒŒí‹°ì…”ë‹ (PostgreSQL ì˜ˆì œ)
CREATE TABLE logs_2024_01 PARTITION OF logs
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

-- í†µê³„ ì—…ë°ì´íŠ¸
ANALYZE logs;
```

### 8. ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

#### ë¡œê·¸ ë°ì´í„° ì•”í˜¸í™”
```python
from cryptography.fernet import Fernet
import base64

class SecureLoggingService:
    def __init__(self, encryption_key=None):
        if encryption_key is None:
            encryption_key = Fernet.generate_key()
        self.cipher_suite = Fernet(encryption_key)

    def encrypt_sensitive_data(self, data):
        """ë¯¼ê°í•œ ë°ì´í„° ì•”í˜¸í™”"""
        if isinstance(data, str):
            data = data.encode()
        return self.cipher_suite.encrypt(data)

    def decrypt_sensitive_data(self, encrypted_data):
        """ë¯¼ê°í•œ ë°ì´í„° ë³µí˜¸í™”"""
        return self.cipher_suite.decrypt(encrypted_data).decode()

    def secure_log(self, level, component, message, sensitive_data=None):
        """ë³´ì•ˆ ë¡œê·¸ ê¸°ë¡"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "level": level,
            "component": component,
            "message": message
        }

        if sensitive_data:
            log_entry["sensitive_data"] = base64.b64encode(
                self.encrypt_sensitive_data(str(sensitive_data))
            ).decode()

        return log_entry
```

#### ì ‘ê·¼ ì œì–´ ë° ê°ì‚¬
```python
class AccessControlManager:
    def __init__(self):
        self.authorized_users = set()
        self.access_log = []

    def authorize_user(self, user_id, role):
        """ì‚¬ìš©ì ê¶Œí•œ ë¶€ì—¬"""
        self.authorized_users.add((user_id, role))
        self._log_access("AUTHORIZE", user_id, role)

    def check_permission(self, user_id, action):
        """ê¶Œí•œ í™•ì¸"""
        user_roles = [role for uid, role in self.authorized_users if uid == user_id]

        permission_map = {
            "READ_LOGS": ["admin", "operator", "viewer"],
            "MODIFY_CONFIG": ["admin"],
            "DELETE_DATA": ["admin"],
            "EXPORT_DATA": ["admin", "operator"]
        }

        allowed_roles = permission_map.get(action, [])
        has_permission = any(role in allowed_roles for role in user_roles)

        self._log_access("CHECK_PERMISSION", user_id, action, has_permission)
        return has_permission

    def _log_access(self, action, user_id, target, result=None):
        """ì ‘ê·¼ ë¡œê·¸ ê¸°ë¡"""
        access_record = {
            "timestamp": datetime.now().isoformat(),
            "action": action,
            "user_id": user_id,
            "target": target,
            "result": result,
            "ip_address": self._get_client_ip()
        }
        self.access_log.append(access_record)
```

### 9. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

#### ì¼ë°˜ì ì¸ ë¬¸ì œ ë° í•´ê²°ì±…

**1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜**
```bash
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
free -h
top -p $(pgrep -f upbit)

# í•´ê²°ì±…
# 1. memory_threshold_mb ê°’ ë‚®ì¶”ê¸°
# 2. GC ì„¤ì • ì¡°ì •
# 3. ìºì‹œ í¬ê¸° ì¤„ì´ê¸°
```

**2. ë¡œê·¸ ì²˜ë¦¬ ì§€ì—°**
```python
# ì„±ëŠ¥ ì§„ë‹¨
performance_monitor = PerformanceMonitor()
stats = performance_monitor.get_current_performance_summary()

if stats["queue_size"] > 10000:
    print("í í¬ê¸°ê°€ ë„ˆë¬´ í¼ - worker_count ì¦ê°€ í•„ìš”")

if stats["processing_time_avg"] > 100:
    print("ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ê¹€ - ë°°ì¹˜ í¬ê¸° ì¡°ì • í•„ìš”")
```

**3. ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±**
```bash
# ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ í™•ì¸
df -h

# ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬
find /opt/upbit-autotrader/logs -name "*.log" -mtime +30 -delete
find /opt/upbit-autotrader/data -name "*.json" -mtime +7 -delete
```

**4. ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨**
```bash
# ë¡œê·¸ í™•ì¸
sudo journalctl -u upbit-logging -n 50

# ì„¤ì • íŒŒì¼ ê²€ì¦
python -c "
import yaml
with open('/opt/upbit-autotrader/config/enhanced_logging.yaml') as f:
    yaml.safe_load(f)
print('Config file is valid')
"

# ê¶Œí•œ í™•ì¸
ls -la /opt/upbit-autotrader/
```

### 10. ì—…ê·¸ë ˆì´ë“œ ê°€ì´ë“œ

#### ë²„ì „ ì—…ê·¸ë ˆì´ë“œ ì ˆì°¨
```bash
#!/bin/bash
# upgrade_logging_system.sh

VERSION="$1"
if [ -z "$VERSION" ]; then
    echo "Usage: $0 <version>"
    exit 1
fi

# 1. ë°±ì—… ìƒì„±
./backup_logging_data.sh

# 2. ì„œë¹„ìŠ¤ ì¤‘ì§€
sudo systemctl stop upbit-logging

# 3. ì½”ë“œ ì—…ë°ì´íŠ¸
cd /opt/upbit-autotrader
git fetch origin
git checkout "v$VERSION"

# 4. ì˜ì¡´ì„± ì—…ë°ì´íŠ¸
./venv/bin/pip install -r requirements.txt

# 5. ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
./venv/bin/python -m upbit_auto_trading.infrastructure.logging.migration

# 6. ì„¤ì • ê²€ì¦
./venv/bin/python -c "
from upbit_auto_trading.infrastructure.logging.config import EnhancedLoggingConfig
config = EnhancedLoggingConfig.from_file('config/enhanced_logging.yaml')
print('Configuration is valid')
"

# 7. ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start upbit-logging

# 8. ìƒíƒœ í™•ì¸
sleep 5
sudo systemctl status upbit-logging

echo "Upgrade to version $VERSION completed"
```

---

ì´ ìš´ì˜ ê°€ì´ë“œë¥¼ í†µí•´ LLM ì—ì´ì „íŠ¸ ë¡œê¹… ì‹œìŠ¤í…œ v4.0ì„ ì•ˆì •ì ìœ¼ë¡œ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ìš´ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
