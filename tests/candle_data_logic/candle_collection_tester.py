"""
CandleCollectionTester v2.0 - CandleDataProvider v4.1 ì „ìš© ì„±ëŠ¥ ì¸¡ì • ë„êµ¬

CandleDataProviderì˜ ì‹¤ì œ API(plan_collection, start_collection, get_next_chunk ë“±)ì— ë§ì¶°
ì„±ëŠ¥ ì¸¡ì •ê³¼ í†µê³„ ìˆ˜ì§‘ì— íŠ¹í™”ëœ í…ŒìŠ¤í„°

ì£¼ìš” ê¸°ëŠ¥:
- ì‹¤ì œ CandleDataProvider API ì‚¬ìš©
- ì²­í¬ ë‹¨ìœ„ ì§„í–‰ë¥  ì¶”ì 
- API í˜¸ì¶œ ì„±ëŠ¥ ì¸¡ì •
- DB ìƒíƒœ ë³€í™” ì¶”ì 
- ìƒì„¸ ì„±ëŠ¥ í†µê³„ ì œê³µ
"""

import sqlite3
import os
import time
from datetime import datetime, timezone
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

from upbit_auto_trading.infrastructure.logging import create_component_logger

logger = create_component_logger("CandleCollectionTester")

from upbit_auto_trading.infrastructure.market_data.candle.candle_data_provider import CandleDataProvider
from upbit_auto_trading.infrastructure.repositories.sqlite_candle_repository import SqliteCandleRepository
from upbit_auto_trading.infrastructure.external_apis.upbit.upbit_public_client import UpbitPublicClient
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import OverlapAnalyzer
from upbit_auto_trading.infrastructure.database.database_manager import DatabaseManager
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils


@dataclass
class PerformanceStats:
    """ì„±ëŠ¥ í†µê³„ ì •ë³´"""
    # ìš”ì²­ ì •ë³´
    symbol: str
    timeframe: str
    count: Optional[int]
    to: Optional[datetime]
    end: Optional[datetime]

    # ê³„íš ì •ë³´
    total_count: int
    estimated_chunks: int
    estimated_duration_seconds: float

    # ì‹¤í–‰ í†µê³„
    actual_chunks: int
    actual_duration_ms: float
    chunks_per_second: float
    candles_per_second: float

    # DB í†µê³„
    db_records_before: int
    db_records_after: int
    db_records_added: int

    # ì²­í¬ë³„ ìƒì„¸ í†µê³„
    chunk_times_ms: List[float]
    total_api_calls: int
    avg_chunk_time_ms: float
    min_chunk_time_ms: float
    max_chunk_time_ms: float

    # ì™„ë£Œ ìƒíƒœ
    success: bool
    error_message: Optional[str] = None


class CandleCollectionTesterV2:
    """
    CandleDataProvider v4.1 ì „ìš© ì„±ëŠ¥ ì¸¡ì • ë„êµ¬

    ì‹¤ì œ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì²­í¬ ë‹¨ìœ„ë¡œ ì§„í–‰ ì¶”ì í•˜ë©° ìƒì„¸í•œ ì„±ëŠ¥ í†µê³„ ì œê³µ
    """

    def __init__(self, db_path: str = "data/market_data.sqlite3", chunk_size: int = 200):
        self.db_path = db_path
        self.chunk_size = min(chunk_size, 200)  # ì—…ë¹„íŠ¸ ì œí•œ ì¤€ìˆ˜ (ìµœëŒ€ 200)
        self.provider: Optional[CandleDataProvider] = None
        self.table_name = "candles_KRW_BTC_1m"

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        # CandleDataProvider ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì…)
        db_paths = {'market_data': self.db_path}
        db_manager = DatabaseManager(db_paths)
        repository = SqliteCandleRepository(db_manager)

        # Zero-429 ì •ì±…: í†µí•© Rate Limiter ì‚¬ìš© (ìë™ìœ¼ë¡œ Zero-429 ì •ì±… ì ìš©ë¨)
        upbit_client = UpbitPublicClient()
        time_utils = TimeUtils()
        overlap_analyzer = OverlapAnalyzer(repository, time_utils)

        self.provider = CandleDataProvider(
            repository=repository,
            upbit_client=upbit_client,
            overlap_analyzer=overlap_analyzer,
            chunk_size=self.chunk_size
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        try:
            # CandleDataProvider HTTP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
            if hasattr(self.provider, 'upbit_client') and self.provider.upbit_client:
                if hasattr(self.provider.upbit_client, 'close'):
                    await self.provider.upbit_client.close()
                    logger.debug("âœ… HTTP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"HTTP í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        try:
            # SqliteCandleRepositoryì˜ DatabaseManager ì •ë¦¬
            if hasattr(self.provider, 'repository') and self.provider.repository:
                if hasattr(self.provider.repository, 'db_manager'):
                    self.provider.repository.db_manager.close_all()
                    logger.debug("âœ… Repository DB ë§¤ë‹ˆì € ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"Repository DB ì—°ê²° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        try:
            # í…ŒìŠ¤í„° ìì²´ì˜ DB ë§¤ë‹ˆì € ì •ë¦¬ (ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª… ì‚¬ìš©)
            if hasattr(self, 'db_manager') and self.db_manager:
                self.db_manager.close_all()
                logger.debug("âœ… í…ŒìŠ¤í„° DB ì—°ê²° ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"í…ŒìŠ¤í„° DB ì—°ê²° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        try:
            # ì „ì—­ DatabaseConnectionProvider ì •ë¦¬
            from upbit_auto_trading.infrastructure.database.database_manager import DatabaseConnectionProvider
            provider_instance = DatabaseConnectionProvider()
            if hasattr(provider_instance, '_db_manager') and provider_instance._db_manager:
                provider_instance._db_manager.close_all()
                logger.debug("âœ… ì „ì—­ DB ì—°ê²° ì œê³µì ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.warning(f"ì „ì—­ DB ì—°ê²° ì œê³µì ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    async def test_collection_performance(
        self,
        symbol: str,
        timeframe: str,
        count: Optional[int] = None,
        to: Optional[datetime] = None,
        end: Optional[datetime] = None
    ) -> PerformanceStats:
        """
        ìº”ë“¤ ìˆ˜ì§‘ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            timeframe: íƒ€ì„í”„ë ˆì„ (ì˜ˆ: '1m')
            count: ìš”ì²­ ê°œìˆ˜
            to: ì‹œì‘ ì‹œê°„
            end: ì¢…ë£Œ ì‹œê°„

        Returns:
            PerformanceStats: ìƒì„¸ ì„±ëŠ¥ í†µê³„
        """
        if not self.provider:
            raise RuntimeError("Providerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. async with ì‚¬ìš©í•˜ì„¸ìš”.")

        print(f"ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘: {symbol} {timeframe}")
        if count:
            print(f"   ğŸ“Š ìš”ì²­: {count}ê°œ")
        if to:
            print(f"   ğŸ“… ì‹œì‘ ì‹œê°„: {to}")
        if end:
            print(f"   ğŸ“… ì¢…ë£Œ ì‹œê°„: {end}")

        # 1. ìˆ˜ì§‘ ì „ DB ìƒíƒœ
        db_before = self._get_db_record_count()

        try:
            # 2. ê³„íš ìˆ˜ë¦½
            plan_start = time.perf_counter()
            plan = self.provider.plan_collection(symbol, timeframe, count, to, end)
            plan_time_ms = (time.perf_counter() - plan_start) * 1000

            print(f"ğŸ“‹ ê³„íš ìˆ˜ë¦½ ì™„ë£Œ: {plan_time_ms:.1f}ms")
            print(f"   ğŸ“Š ì˜ˆìƒ: {plan.total_count}ê°œ ìº”ë“¤, {plan.estimated_chunks}ì²­í¬")
            print(f"   â±ï¸ ì˜ˆìƒ ì†Œìš”ì‹œê°„: {plan.estimated_duration_seconds:.1f}ì´ˆ")

            # 3. ìˆ˜ì§‘ ì‹œì‘
            collection_start = time.perf_counter()
            request_id = self.provider.start_collection(symbol, timeframe, count, to, end)

            print(f"ğŸ¯ ìˆ˜ì§‘ ì‹œì‘: ID={request_id}")

            # 4. ì²­í¬ë³„ ìˆ˜ì§‘ ì‹¤í–‰
            chunk_times = []
            chunk_count = 0

            while True:
                # ë‹¤ìŒ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunk_info = self.provider.get_next_chunk(request_id)
                if chunk_info is None:
                    break

                chunk_count += 1
                chunk_start = time.perf_counter()

                print(f"ğŸ“¦ ì²­í¬ {chunk_count}/{plan.estimated_chunks} ì²˜ë¦¬ ì¤‘...")
                print(f"   ğŸ†” ì²­í¬ ID: {chunk_info.chunk_id}")
                print(f"   ğŸ“Š ê°œìˆ˜: {chunk_info.count}")
                if chunk_info.to:
                    print(f"   ğŸ“… to: {chunk_info.to}")
                if chunk_info.end:
                    print(f"   ğŸ“… end: {chunk_info.end}")

                # ì²­í¬ ì™„ë£Œ ì²˜ë¦¬
                await self.provider.mark_chunk_completed(request_id)

                chunk_time_ms = (time.perf_counter() - chunk_start) * 1000
                chunk_times.append(chunk_time_ms)

                print(f"   âœ… ì™„ë£Œ: {chunk_time_ms:.1f}ms")

            # 5. ìˆ˜ì§‘ ì™„ë£Œ
            total_duration_ms = (time.perf_counter() - collection_start) * 1000

            # 6. ìˆ˜ì§‘ í›„ DB ìƒíƒœ
            db_after = self._get_db_record_count()

            # 7. í†µê³„ ê³„ì‚°
            chunk_times_ms = chunk_times if chunk_times else [0.0]

            stats = PerformanceStats(
                # ìš”ì²­ ì •ë³´
                symbol=symbol,
                timeframe=timeframe,
                count=count,
                to=to,
                end=end,

                # ê³„íš ì •ë³´
                total_count=plan.total_count,
                estimated_chunks=plan.estimated_chunks,
                estimated_duration_seconds=plan.estimated_duration_seconds,

                # ì‹¤í–‰ í†µê³„
                actual_chunks=chunk_count,
                actual_duration_ms=total_duration_ms,
                chunks_per_second=chunk_count / (total_duration_ms / 1000) if total_duration_ms > 0 else 0,
                candles_per_second=plan.total_count / (total_duration_ms / 1000) if total_duration_ms > 0 else 0,

                # DB í†µê³„
                db_records_before=db_before,
                db_records_after=db_after,
                db_records_added=db_after - db_before,

                # ì²­í¬ë³„ ìƒì„¸ í†µê³„
                chunk_times_ms=chunk_times_ms,
                total_api_calls=chunk_count,  # ì²­í¬ ìˆ˜ = API í˜¸ì¶œ ìˆ˜
                avg_chunk_time_ms=sum(chunk_times_ms) / len(chunk_times_ms),
                min_chunk_time_ms=min(chunk_times_ms),
                max_chunk_time_ms=max(chunk_times_ms),

                # ì™„ë£Œ ìƒíƒœ
                success=True
            )

            print(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {total_duration_ms:.1f}ms")
            return stats

        except Exception as e:
            error_stats = PerformanceStats(
                symbol=symbol, timeframe=timeframe, count=count, to=to, end=end,
                total_count=0, estimated_chunks=0, estimated_duration_seconds=0,
                actual_chunks=0, actual_duration_ms=0, chunks_per_second=0, candles_per_second=0,
                db_records_before=db_before, db_records_after=self._get_db_record_count(), db_records_added=0,
                chunk_times_ms=[], total_api_calls=0, avg_chunk_time_ms=0, min_chunk_time_ms=0, max_chunk_time_ms=0,
                success=False, error_message=str(e)
            )
            return error_stats

    def _get_db_record_count(self) -> int:
        """DB ë ˆì½”ë“œ ìˆ˜ ì¡°íšŒ"""
        if not os.path.exists(self.db_path):
            return 0

        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute(f"SELECT COUNT(*) FROM {self.table_name}")
                return cursor.fetchone()[0]
        except sqlite3.OperationalError:
            # í…Œì´ë¸”ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°
            return 0
        except Exception:
            return 0

    def print_performance_summary(self, stats: PerformanceStats) -> None:
        """ì„±ëŠ¥ í†µê³„ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š === ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ===")

        if not stats.success:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {stats.error_message}")
            return

        print(f"ğŸ¯ ìš”ì²­: {stats.symbol} {stats.timeframe}")
        if stats.count:
            print(f"   ğŸ“ ê°œìˆ˜: {stats.count}ê°œ")
        if stats.to:
            print(f"   ğŸ“… ì‹œì‘: {stats.to}")
        if stats.end:
            print(f"   ğŸ“… ì¢…ë£Œ: {stats.end}")

        print(f"\nğŸ“‹ ê³„íš vs ì‹¤ì œ:")
        print(f"   ğŸ“Š ìº”ë“¤: ì˜ˆìƒ {stats.total_count}ê°œ")
        print(f"   ğŸ“¦ ì²­í¬: ì˜ˆìƒ {stats.estimated_chunks}ê°œ â†’ ì‹¤ì œ {stats.actual_chunks}ê°œ")
        print(f"   â±ï¸ ì†Œìš”ì‹œê°„: ì˜ˆìƒ {stats.estimated_duration_seconds:.1f}ì´ˆ â†’ ì‹¤ì œ {stats.actual_duration_ms / 1000:.1f}ì´ˆ")

        print("\nğŸš€ ì„±ëŠ¥ ì§€í‘œ:")
        print(f"   ğŸ“¦ ì²­í¬/ì´ˆ: {stats.chunks_per_second:.2f}")
        print(f"   ğŸ“Š ìº”ë“¤/ì´ˆ: {stats.candles_per_second:.1f}")
        print(f"   ğŸŒ API í˜¸ì¶œ: {stats.total_api_calls}íšŒ")

        print("\nğŸ’¾ DB ìƒíƒœ:")
        print(f"   ğŸ“‹ ì´ì „: {stats.db_records_before:,}ê°œ")
        print(f"   ğŸ“‹ ì´í›„: {stats.db_records_after:,}ê°œ")
        print(f"   ğŸ“ˆ ì¦ê°€: +{stats.db_records_added:,}ê°œ")

        print("\nâ±ï¸ ì²­í¬ ì²˜ë¦¬ ì‹œê°„:")
        print(f"   í‰ê· : {stats.avg_chunk_time_ms:.1f}ms")
        print(f"   ìµœì†Œ: {stats.min_chunk_time_ms:.1f}ms")
        print(f"   ìµœëŒ€: {stats.max_chunk_time_ms:.1f}ms")

    def print_detailed_analysis(self, stats: PerformanceStats) -> None:
        """ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ ì¶œë ¥"""
        self.print_performance_summary(stats)

        if not stats.success:
            return

        print("\nğŸ” === ìƒì„¸ ë¶„ì„ ===")

        # íš¨ìœ¨ì„± ë¶„ì„
        if stats.actual_duration_ms > 0 and stats.estimated_duration_seconds > 0:
            speed_ratio = (stats.estimated_duration_seconds * 1000) / stats.actual_duration_ms
            if speed_ratio > 1.1:
                print(f"ğŸš€ ì˜ˆìƒë³´ë‹¤ ë¹ ë¦„: {speed_ratio:.1f}ë°° ë¹ ë¥¸ ìˆ˜ì§‘")
            elif speed_ratio < 0.9:
                print(f"ğŸŒ ì˜ˆìƒë³´ë‹¤ ëŠë¦¼: {1 / speed_ratio:.1f}ë°° ëŠë¦° ìˆ˜ì§‘")
            else:
                print(f"ğŸ¯ ì˜ˆìƒê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥: {speed_ratio:.1f}ë°°")

        # ì²­í¬ ì²˜ë¦¬ ê· ì¼ì„± ë¶„ì„
        if len(stats.chunk_times_ms) > 1:
            import statistics
            std_dev = statistics.stdev(stats.chunk_times_ms)
            cv = std_dev / stats.avg_chunk_time_ms if stats.avg_chunk_time_ms > 0 else 0

            if cv < 0.2:
                print(f"ğŸ“Š ì•ˆì •ì ì¸ ì²­í¬ ì²˜ë¦¬: ë³€ë™ê³„ìˆ˜ {cv:.2f}")
            elif cv < 0.5:
                print(f"ğŸ“Š ë³´í†µ ì²­í¬ ì²˜ë¦¬: ë³€ë™ê³„ìˆ˜ {cv:.2f}")
            else:
                print(f"ğŸ“Š ë¶ˆê· ë“±í•œ ì²­í¬ ì²˜ë¦¬: ë³€ë™ê³„ìˆ˜ {cv:.2f}")

        # DB ì €ì¥ íš¨ìœ¨ì„±
        if stats.db_records_added > 0 and stats.total_count > 0:
            save_ratio = stats.db_records_added / stats.total_count
            if save_ratio >= 0.95:
                print(f"ğŸ’¾ ìš°ìˆ˜í•œ DB ì €ì¥: {save_ratio * 100:.1f}% ì €ì¥ë¨")
            elif save_ratio >= 0.8:
                print(f"ğŸ’¾ ì–‘í˜¸í•œ DB ì €ì¥: {save_ratio * 100:.1f}% ì €ì¥ë¨")
            else:
                print(f"ğŸ’¾ ì €ì¡°í•œ DB ì €ì¥: {save_ratio * 100:.1f}% ì €ì¥ë¨ (ì¤‘ë³µ ë˜ëŠ” ì˜¤ë¥˜?)")

        # ì²­í¬ë³„ ì‹œê°„ ë¶„í¬
        if len(stats.chunk_times_ms) > 3:
            print("\nğŸ“ˆ ì²­í¬ë³„ ì²˜ë¦¬ ì‹œê°„ (ì²˜ìŒ 3ê°œ):")
            for i, chunk_time in enumerate(stats.chunk_times_ms[:3]):
                print(f"   ì²­í¬ {i + 1}: {chunk_time:.1f}ms")


# í¸ì˜ í•¨ìˆ˜ë“¤
async def test_count_collection(
    symbol: str = "KRW-BTC",
    timeframe: str = "1m",
    count: int = 100
) -> PerformanceStats:
    """ê°œìˆ˜ ì§€ì • ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (í¸ì˜ í•¨ìˆ˜)"""
    async with CandleCollectionTesterV2() as tester:
        return await tester.test_collection_performance(
            symbol=symbol,
            timeframe=timeframe,
            count=count
        )


async def test_time_range_collection(
    symbol: str = "KRW-BTC",
    timeframe: str = "1m",
    to: Optional[datetime] = None,
    end: Optional[datetime] = None
) -> PerformanceStats:
    """ì‹œê°„ ë²”ìœ„ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (í¸ì˜ í•¨ìˆ˜)"""
    async with CandleCollectionTesterV2() as tester:
        return await tester.test_collection_performance(
            symbol=symbol,
            timeframe=timeframe,
            to=to,
            end=end
        )


if __name__ == "__main__":
    # ì˜ˆì œ ì‹¤í–‰
    import asyncio

    async def example_test():
        print("ğŸ§ª CandleDataProvider v4.1 ì„±ëŠ¥ í…ŒìŠ¤í„°")

        # 100ê°œ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
        stats = await test_count_collection(count=100)

        # ê²°ê³¼ ì¶œë ¥
        async with CandleCollectionTesterV2() as tester:
            tester.print_detailed_analysis(stats)

    asyncio.run(example_test())
