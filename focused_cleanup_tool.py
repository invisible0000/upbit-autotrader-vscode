#!/usr/bin/env python3
"""
ì§‘ì¤‘ì  ì‹œìŠ¤í…œ ì •ë¦¬ ë„êµ¬ - .venv ì œì™¸, í•µì‹¬ ì˜ì—­ë§Œ

ë²”ìœ„ ì œí•œ:
- upbit_auto_trading/ (ì†ŒìŠ¤ì½”ë“œ)
- data_info/ (ì •ì˜ íŒŒì¼ë“¤)
- data/ (ë°ì´í„°ë² ì´ìŠ¤)
- config/ (ì„¤ì • íŒŒì¼ë“¤)
- .venv, __pycache__, .git ë“± ì œì™¸
"""

import sqlite3
from pathlib import Path
import shutil
import json
from datetime import datetime


class FocusedSystemCleaner:
    """ì§‘ì¤‘ì  ì‹œìŠ¤í…œ ì •ë¦¬ê¸° - í•µì‹¬ ì˜ì—­ë§Œ"""

    def __init__(self):
        self.project_root = Path(".")

        # ë¶„ì„ ëŒ€ìƒ í´ë” (ì œí•œì )
        self.target_folders = [
            "upbit_auto_trading",
            "data_info",
            "data",
            "config",
            "tests",
            "tools"
        ]

        # ì œì™¸í•  í´ë”ë“¤
        self.exclude_patterns = [
            ".venv",
            "__pycache__",
            ".git",
            "*.egg-info",
            "logs",
            "temp",
            "backups"
        ]

    def analyze_legacy_folders(self):
        """ë ˆê±°ì‹œ í´ë” ì •í™•í•œ í˜„í™©"""
        print("ğŸ“ ë ˆê±°ì‹œ í´ë” í˜„í™© ë¶„ì„")

        legacy_targets = {
            "data_info/indicators": "SMAë§Œ ë‚¨ì€ êµ¬ ì§€í‘œ í´ë”",
            "data_info/tv_variable_help_guides": "êµ¬ ë„ì›€ë§ ê°€ì´ë“œ",
            "data_info/_archived": "ì´ë¯¸ ì•„ì¹´ì´ë¸Œëœ íŒŒì¼ë“¤",
            "legacy": "ë ˆê±°ì‹œ ì½”ë“œë“¤"
        }

        results = {}
        for folder_path, description in legacy_targets.items():
            path = Path(folder_path)
            if path.exists():
                files = list(path.rglob("*"))
                file_count = len([f for f in files if f.is_file()])
                folder_count = len([f for f in files if f.is_dir()])

                results[folder_path] = {
                    "exists": True,
                    "description": description,
                    "file_count": file_count,
                    "folder_count": folder_count,
                    "total_size_kb": sum(f.stat().st_size for f in files if f.is_file()) // 1024
                }
                print(f"  ğŸ“‚ {folder_path}: {file_count}ê°œ íŒŒì¼, {folder_count}ê°œ í´ë” ({results[folder_path]['total_size_kb']}KB)")
                print(f"     ğŸ’¡ {description}")
            else:
                results[folder_path] = {"exists": False, "description": description}
                print(f"  âŒ {folder_path}: ì—†ìŒ")

        return results

    def analyze_database_tables(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì •í™•í•œ ë¶„ì„"""
        print("\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ë¶„ì„")

        # íŠ¸ë¦¬ê±° ë¹Œë” ê°œë°œì— í•„ìˆ˜ì¸ í…Œì´ë¸”ë“¤
        essential_tables = {
            "settings.sqlite3": [
                "tv_trading_variables",
                "tv_variable_parameters",
                "tv_variable_help_documents",
                "tv_help_texts",
                "tv_placeholder_texts",
                "api_keys",  # ì•”í˜¸í™”ëœ API í‚¤
                "user_settings"  # ì‚¬ìš©ì ì„¤ì •
            ]
        }

        db_analysis = {}

        for db_file in ["data/settings.sqlite3", "data/strategies.sqlite3", "data/market_data.sqlite3"]:
            db_path = Path(db_file)
            db_name = db_path.name

            if not db_path.exists():
                print(f"  âŒ {db_file}: íŒŒì¼ ì—†ìŒ")
                continue

            try:
                with sqlite3.connect(str(db_path)) as conn:
                    # ëª¨ë“  í…Œì´ë¸” ëª©ë¡
                    cursor = conn.execute(
                        "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'"
                    )
                    all_tables = [row[0] for row in cursor.fetchall()]

                    # ê° í…Œì´ë¸”ì˜ ë ˆì½”ë“œ ìˆ˜
                    table_info = {}
                    for table in all_tables:
                        try:
                            count_cursor = conn.execute(f"SELECT COUNT(*) FROM {table}")
                            record_count = count_cursor.fetchone()[0]
                            table_info[table] = record_count
                        except Exception as e:
                            table_info[table] = f"ì˜¤ë¥˜: {e}"

                    # í•„ìˆ˜ vs ì„ íƒì  ë¶„ë¥˜
                    essential_for_this_db = essential_tables.get(db_name, [])
                    essential_found = [t for t in all_tables if t in essential_for_this_db]
                    optional_tables = [t for t in all_tables if t not in essential_for_this_db]
                    missing_essential = [t for t in essential_for_this_db if t not in all_tables]

                    db_analysis[db_file] = {
                        "total_tables": len(all_tables),
                        "table_info": table_info,
                        "essential_found": essential_found,
                        "optional_tables": optional_tables,
                        "missing_essential": missing_essential
                    }

                    print(f"\n  ğŸ“Š {db_file}:")
                    print(f"    ğŸ“‹ ì „ì²´ í…Œì´ë¸”: {len(all_tables)}ê°œ")

                    if essential_found:
                        print(f"    âœ… í•„ìˆ˜ í…Œì´ë¸”: {len(essential_found)}ê°œ")
                        for table in essential_found:
                            count = table_info.get(table, 0)
                            print(f"      - {table}: {count}ê°œ ë ˆì½”ë“œ")

                    if optional_tables:
                        print(f"    ğŸ¤” ì„ íƒì  í…Œì´ë¸”: {len(optional_tables)}ê°œ")
                        for table in optional_tables:
                            count = table_info.get(table, 0)
                            print(f"      - {table}: {count}ê°œ ë ˆì½”ë“œ")

                    if missing_essential:
                        print(f"    âŒ ëˆ„ë½ëœ í•„ìˆ˜: {len(missing_essential)}ê°œ")
                        for table in missing_essential:
                            print(f"      - {table}")

            except Exception as e:
                print(f"  âŒ {db_file} ë¶„ì„ ì˜¤ë¥˜: {e}")
                db_analysis[db_file] = {"error": str(e)}

        return db_analysis

    def check_import_dependencies(self):
        """import ì˜ì¡´ì„± ì²´í¬ - í•µì‹¬ í´ë”ë§Œ"""
        print("\nğŸ”— Import ì˜ì¡´ì„± ë¶„ì„ (í•µì‹¬ í´ë”ë§Œ)")

        old_patterns = [
            "from data_info.indicators",
            "import data_info.indicators",
            "from data_info.tv_variable_help_guides",
            "import data_info.tv_variable_help_guides"
        ]

        found_imports = {}

        for folder in self.target_folders:
            folder_path = Path(folder)
            if not folder_path.exists():
                continue

            for pattern in old_patterns:
                found_files = []

                # Python íŒŒì¼ì—ì„œ ê²€ìƒ‰
                for py_file in folder_path.rglob("*.py"):
                    try:
                        with open(py_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                            if pattern in content:
                                found_files.append(str(py_file))
                    except Exception:
                        continue

                if found_files:
                    found_imports[pattern] = found_files

        if found_imports:
            print("  âš ï¸  ë ˆê±°ì‹œ import ë°œê²¬:")
            for pattern, files in found_imports.items():
                print(f"    ğŸ” '{pattern}': {len(files)}ê°œ íŒŒì¼")
                for file_path in files[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                    print(f"      - {file_path}")
                if len(files) > 3:
                    print(f"      ... +{len(files)-3}ê°œ ë”")
        else:
            print("  âœ… ë ˆê±°ì‹œ import ì—†ìŒ")

        return found_imports

    def generate_cleanup_plan(self, folder_analysis, db_analysis, import_analysis):
        """ì •ë¦¬ ê³„íš ìƒì„±"""
        print("\nğŸ“‹ ì •ë¦¬ ê³„íš ìƒì„±")

        plan = {
            "step1_fix_imports": {
                "description": "ë ˆê±°ì‹œ import êµ¬ë¬¸ ìˆ˜ì •",
                "files_to_fix": sum(len(files) for files in import_analysis.values()),
                "required": len(import_analysis) > 0
            },
            "step2_backup_legacy_folders": {
                "description": "ë ˆê±°ì‹œ í´ë” ë°±ì—…",
                "folders": [name for name, info in folder_analysis.items() if info.get("exists")],
                "total_files": sum(info.get("file_count", 0) for info in folder_analysis.values() if info.get("exists"))
            },
            "step3_cleanup_database": {
                "description": "ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒì  í…Œì´ë¸” ì •ë¦¬",
                "databases": []
            },
            "step4_remove_legacy_folders": {
                "description": "ë°±ì—… ì™„ë£Œ í›„ ë ˆê±°ì‹œ í´ë” ì‚­ì œ",
                "safe_to_remove": []
            }
        }

        # DB ì •ë¦¬ ê³„íš
        for db_path, info in db_analysis.items():
            if "optional_tables" in info and info["optional_tables"]:
                plan["step3_cleanup_database"]["databases"].append({
                    "db": db_path,
                    "optional_tables": info["optional_tables"],
                    "keep_essential": info.get("essential_found", [])
                })

        # ì•ˆì „í•˜ê²Œ ì œê±° ê°€ëŠ¥í•œ í´ë”
        safe_folders = [
            "data_info/indicators",  # trading_variablesë¡œ ì™„ì „ ì´ê´€ë¨
            "data_info/tv_variable_help_guides"  # ìƒˆ êµ¬ì¡°ë¡œ ì´ê´€ë¨
        ]

        for folder in safe_folders:
            if folder in folder_analysis and folder_analysis[folder].get("exists"):
                plan["step4_remove_legacy_folders"]["safe_to_remove"].append(folder)

        print("  ğŸ“ ê³„íš ìš”ì•½:")
        print(f"    1. Import ìˆ˜ì •: {plan['step1_fix_imports']['files_to_fix']}ê°œ íŒŒì¼")
        print(f"    2. í´ë” ë°±ì—…: {len(plan['step2_backup_legacy_folders']['folders'])}ê°œ í´ë”")
        print(f"    3. DB ì •ë¦¬: {len(plan['step3_cleanup_database']['databases'])}ê°œ ë°ì´í„°ë² ì´ìŠ¤")
        print(f"    4. í´ë” ì‚­ì œ: {len(plan['step4_remove_legacy_folders']['safe_to_remove'])}ê°œ í´ë”")

        return plan

    def run_focused_analysis(self):
        """ì§‘ì¤‘ ë¶„ì„ ì‹¤í–‰"""
        print("ğŸ¯ ì§‘ì¤‘ì  ì‹œìŠ¤í…œ ë¶„ì„ ì‹œì‘")
        print("="*50)
        print("ë²”ìœ„: í•µì‹¬ í´ë”ë§Œ (.venv ì œì™¸)")
        print("ëª©í‘œ: íŠ¸ë¦¬ê±° ë¹Œë” ê°œë°œ í™˜ê²½ ìµœì í™”")
        print("="*50)

        # ê° ë¶„ì„ ë‹¨ê³„
        folder_analysis = self.analyze_legacy_folders()
        db_analysis = self.analyze_database_tables()
        import_analysis = self.check_import_dependencies()
        cleanup_plan = self.generate_cleanup_plan(folder_analysis, db_analysis, import_analysis)

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "="*50)
        print("ğŸ“Š ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*50)

        total_legacy_files = sum(info.get("file_count", 0) for info in folder_analysis.values() if info.get("exists"))
        total_legacy_size = sum(info.get("total_size_kb", 0) for info in folder_analysis.values() if info.get("exists"))

        print(f"ğŸ’¾ ë ˆê±°ì‹œ íŒŒì¼: {total_legacy_files}ê°œ ({total_legacy_size}KB)")
        print(f"ğŸ”— ìˆ˜ì • í•„ìš”í•œ import: {len(import_analysis)}ê°œ íŒ¨í„´")

        for db_path, info in db_analysis.items():
            if "optional_tables" in info:
                print(f"ğŸ—„ï¸  {Path(db_path).name}: {len(info['optional_tables'])}ê°œ ì„ íƒì  í…Œì´ë¸”")

        print(f"\nâœ… ì•ˆì „ ì‚­ì œ ê°€ëŠ¥: {len(cleanup_plan['step4_remove_legacy_folders']['safe_to_remove'])}ê°œ í´ë”")

        # ê²°ê³¼ ì €ì¥
        result = {
            "timestamp": datetime.now().isoformat(),
            "scope": "focused_analysis_excluding_venv",
            "folder_analysis": folder_analysis,
            "db_analysis": db_analysis,
            "import_analysis": import_analysis,
            "cleanup_plan": cleanup_plan
        }

        with open("_focused_cleanup_result.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼: _focused_cleanup_result.json")

        return result


def main():
    print("ğŸ”§ ì§‘ì¤‘ì  ì‹œìŠ¤í…œ ì •ë¦¬ ë„êµ¬")
    print("ë²”ìœ„: í•µì‹¬ ê°œë°œ í´ë”ë§Œ (.venv ì œì™¸)")

    cleaner = FocusedSystemCleaner()
    result = cleaner.run_focused_analysis()

    print("\nğŸ‰ ì§‘ì¤‘ ë¶„ì„ ì™„ë£Œ!")
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("1. _focused_cleanup_result.json ê²€í† ")
    print("2. í•„ìš”ì‹œ import êµ¬ë¬¸ ìˆ˜ì •")
    print("3. ë ˆê±°ì‹œ í´ë” ë°±ì—… í›„ ì‚­ì œ")
    print("4. íŠ¸ë¦¬ê±° ë¹Œë” ê°œë°œ ì§„í–‰!")


if __name__ == "__main__":
    main()
