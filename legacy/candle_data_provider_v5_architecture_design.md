# ğŸ—ï¸ CandleDataProvider v5.0 - Architecture Design & Interface Specification
> ê¸°ë°˜ ì»´í¬ë„ŒíŠ¸ í†µí•©ì„ ìœ„í•œ ìƒì„¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

## ğŸ¯ Overall Architecture

### Component Integration Map
```
CandleDataProvider v5.0
â”œâ”€â”€ Core Integration Layer
â”‚   â”œâ”€â”€ OverlapAnalyzer v5.0      âœ… (5-state classification)
â”‚   â”œâ”€â”€ TimeUtils                 âœ… (timedelta-based, 27 timeframes)
â”‚   â”œâ”€â”€ CandleModels             âœ… (CandleData + Response models)
â”‚   â”œâ”€â”€ SqliteCandleRepository   âœ… (10 optimized methods)
â”‚   â””â”€â”€ CandleRepositoryInterface âœ… (DDD compliance)
â”‚
â”œâ”€â”€ Service Layer (NEW)
â”‚   â”œâ”€â”€ Request Validation & Standardization
â”‚   â”œâ”€â”€ Chunk Management (200 candles max)
â”‚   â”œâ”€â”€ Cache Layer (60s TTL)
â”‚   â””â”€â”€ Response Assembly
â”‚
â””â”€â”€ Client Interface
    â””â”€â”€ get_candles() - Single Entry Point
```

### Data Flow Architecture
```
â”Œâ”€ Application Layer Request â”€â”
â”‚  get_candles(symbol, timeframe, count?, start_time?, end_time?, inclusive_start?)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    CandleDataProvider v5.0   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Request Validation   â”‚  â”‚ â—„â”€â”€ TimeUtils (ì‹œê°„ ê³„ì‚°)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. Cache Check (60s)    â”‚  â”‚ â—„â”€â”€ CandleCache (ë©”ëª¨ë¦¬)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. Chunk Split (200ê°œ)  â”‚  â”‚ â—„â”€â”€ TimeUtils (ë²”ìœ„ ê³„ì‚°)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 4. Sequential Collection â”‚  â”‚
â”‚  â”‚   â”œâ”€ OverlapAnalyzer    â”‚  â”‚ â—„â”€â”€ 5-state classification
â”‚  â”‚   â”œâ”€ DB Query           â”‚  â”‚ â—„â”€â”€ SqliteCandleRepository
â”‚  â”‚   â”œâ”€ API Request        â”‚  â”‚ â—„â”€â”€ UpbitPublicClient
â”‚  â”‚   â””â”€ Data Storage       â”‚  â”‚ â—„â”€â”€ Repository.save_chunk()
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 5. Response Assembly    â”‚  â”‚ â—„â”€â”€ CandleDataResponse
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     CandleDataResponse        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ success: bool           â”‚  â”‚
â”‚  â”‚ candles: List[CandleDataâ”‚  â”‚
â”‚  â”‚ total_count: int        â”‚  â”‚
â”‚  â”‚ data_source: str        â”‚  â”‚
â”‚  â”‚ response_time_ms: float â”‚  â”‚
â”‚  â”‚ error_message?: str     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Detailed Component Integration

### 1. OverlapAnalyzer v5.0 Integration

#### Integration Point
```python
from upbit_auto_trading.infrastructure.market_data.candle.overlap_analyzer import OverlapAnalyzer
from upbit_auto_trading.infrastructure.market_data.candle.candle_models import OverlapRequest, OverlapResult, OverlapStatus

class CandleDataProvider:
    def __init__(self, ...):
        self.overlap_analyzer = OverlapAnalyzer(
            repository=self.repository,
            time_utils=self.time_utils,
            enable_validation=True  # ê°œë°œ ì´ˆê¸°ëŠ” True, ì•ˆì •í™” í›„ False
        )
```

#### Usage Pattern
```python
async def _analyze_chunk_overlap(self, chunk: CandleChunk) -> OverlapResult:
    """ì²­í¬ë³„ ê²¹ì¹¨ ë¶„ì„ - OverlapAnalyzer v5.0 í™œìš©"""
    request = OverlapRequest(
        symbol=chunk.symbol,
        timeframe=chunk.timeframe,
        target_start=chunk.start_time,
        target_end=self._calculate_chunk_end_time(chunk),
        target_count=chunk.count
    )

    result = await self.overlap_analyzer.analyze_overlap(request)

    # 5-state ê²°ê³¼ì— ë”°ë¥¸ ì²˜ë¦¬ ë¶„ê¸°
    if result.status == OverlapStatus.NO_OVERLAP:
        return await self._collect_full_api_chunk(chunk)
    elif result.status == OverlapStatus.COMPLETE_OVERLAP:
        return await self._collect_full_db_chunk(chunk)
    elif result.status == OverlapStatus.PARTIAL_START:
        return await self._collect_mixed_from_start(chunk, result.partial_end)
    elif result.status in [OverlapStatus.PARTIAL_MIDDLE_FRAGMENT, OverlapStatus.PARTIAL_MIDDLE_CONTINUOUS]:
        return await self._collect_mixed_middle(chunk, result)
    else:
        # Fallback: ì „ì²´ API ìš”ì²­
        return await self._collect_full_api_chunk(chunk)
```

### 2. TimeUtils Integration

#### Integration Point
```python
from upbit_auto_trading.infrastructure.market_data.candle.time_utils import TimeUtils

class CandleDataProvider:
    def __init__(self, ...):
        self.time_utils = TimeUtils
```

#### Usage Patterns
```python
def _validate_and_standardize_request(self, ...) -> Tuple[datetime, datetime, int]:
    """ìš”ì²­ ê²€ì¦ ë° í‘œì¤€í™” - TimeUtils í™œìš©"""

    # 1. íƒ€ì„í”„ë ˆì„ ê²€ì¦
    try:
        timeframe_seconds = self.time_utils.get_timeframe_seconds(timeframe)
    except ValueError as e:
        raise ValidationError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: {timeframe}")

    # 2. ì‹œê°„ ë²”ìœ„ ê³„ì‚°
    if start_time and end_time:
        expected_count = self.time_utils.calculate_expected_count(start_time, end_time, timeframe)
        return start_time, end_time, expected_count
    elif start_time and count:
        delta = self.time_utils.get_timeframe_delta(timeframe)
        end_time = start_time + (delta * (count - 1))
        return start_time, end_time, count
    elif count:
        # í˜„ì¬ ì‹œê°„ë¶€í„° ì—­ìˆœ
        now = datetime.now(timezone.utc)
        aligned_now = self.time_utils._align_to_candle_boundary(now, timeframe)
        start_time = self.time_utils.get_aligned_time_by_ticks(aligned_now, timeframe, -(count-1))
        return start_time, aligned_now, count
    else:
        raise ValidationError("count, start_time+count, ë˜ëŠ” start_time+end_time ì¤‘ í•˜ë‚˜ëŠ” í•„ìˆ˜")

def _split_into_chunks(self, symbol: str, timeframe: str, count: int,
                      start_time: datetime, end_time: datetime) -> List[CandleChunk]:
    """200ê°œ ì²­í¬ ë¶„í•  - TimeUtils í™œìš©"""
    chunks = []
    chunk_size = 200

    # TimeUtilsë¡œ ì‹œê°„ ì‹œí€€ìŠ¤ ìƒì„±
    time_sequence = self.time_utils.get_time_range(start_time, end_time, timeframe)

    for i in range(0, len(time_sequence), chunk_size):
        chunk_start = time_sequence[i]
        chunk_times = time_sequence[i:i+chunk_size]
        chunk_count = len(chunk_times)

        chunk = CandleChunk(
            symbol=symbol,
            timeframe=timeframe,
            start_time=chunk_start,
            count=chunk_count,
            chunk_index=len(chunks)
        )
        chunks.append(chunk)

    return chunks
```

### 3. CandleModels Integration

#### Import Strategy
```python
from upbit_auto_trading.infrastructure.market_data.candle.candle_models import (
    CandleData, CandleDataResponse, CandleChunk, CollectionResult,
    OverlapRequest, OverlapResult, OverlapStatus,
    create_success_response, create_error_response
)
```

#### Model Usage Patterns
```python
async def _collect_api_chunk(self, chunk: CandleChunk) -> CollectionResult:
    """APIì—ì„œ ì²­í¬ ìˆ˜ì§‘ - CandleModels í™œìš©"""
    start_time = time.perf_counter()

    try:
        # API í˜¸ì¶œ
        api_data = await self.upbit_client.get_candles(
            market=chunk.symbol,
            timeframe=chunk.timeframe,
            count=chunk.count,
            to=chunk.start_time.isoformat()
        )

        # CandleData ëª¨ë¸ë¡œ ë³€í™˜
        candles = [
            CandleData.from_upbit_api(data, chunk.timeframe)
            for data in api_data
        ]

        collection_time = (time.perf_counter() - start_time) * 1000

        return CollectionResult(
            chunk=chunk,
            collected_candles=candles,
            data_source="api",
            api_requests_made=1,
            collection_time_ms=collection_time
        )

    except Exception as e:
        logger.error(f"API ì²­í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return CollectionResult(
            chunk=chunk,
            collected_candles=[],
            data_source="error",
            api_requests_made=0,
            collection_time_ms=(time.perf_counter() - start_time) * 1000
        )

async def _assemble_final_response(self, results: List[CollectionResult],
                                 request_start_time: float) -> CandleDataResponse:
    """ìµœì¢… ì‘ë‹µ ì¡°í•© - CandleDataResponse í™œìš©"""

    # ëª¨ë“  ê²°ê³¼ ì¡°í•©
    all_candles = []
    data_sources = []
    total_api_requests = 0

    for result in results:
        all_candles.extend(result.collected_candles)
        data_sources.append(result.data_source)
        total_api_requests += result.api_requests_made

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬ (CandleData ëª¨ë¸ì˜ ì •ë ¬ í‚¤ í™œìš©)
    unique_candles = {
        f"{c.symbol}_{c.timeframe}_{c.candle_date_time_utc}": c
        for c in all_candles
    }
    sorted_candles = sorted(
        unique_candles.values(),
        key=lambda c: c.candle_date_time_utc
    )

    # ì‘ë‹µ ì‹œê°„ ê³„ì‚°
    response_time_ms = (time.perf_counter() - request_start_time) * 1000

    # ë°ì´í„° ì†ŒìŠ¤ ê²°ì •
    unique_sources = set(data_sources)
    if len(unique_sources) > 1:
        data_source = "mixed"
    elif "db" in unique_sources:
        data_source = "db"
    elif "api" in unique_sources:
        data_source = "api"
    else:
        data_source = "cache"

    # CandleDataResponse ìƒì„±
    return create_success_response(
        candles=sorted_candles,
        data_source=data_source,
        response_time_ms=response_time_ms
    )
```

### 4. SqliteCandleRepository Integration

#### Repository Pattern Usage
```python
from upbit_auto_trading.infrastructure.repositories.sqlite_candle_repository import SqliteCandleRepository

class CandleDataProvider:
    def __init__(self, db_manager: DatabaseManager, ...):
        self.repository = SqliteCandleRepository(db_manager)

async def _collect_db_chunk(self, chunk: CandleChunk) -> CollectionResult:
    """DBì—ì„œ ì²­í¬ ìˆ˜ì§‘ - Repository 10ê°œ ë©”ì„œë“œ í™œìš©"""
    start_time = time.perf_counter()

    try:
        # í…Œì´ë¸” ì¡´ì¬ í™•ì¸
        if not await self.repository.table_exists(chunk.symbol, chunk.timeframe):
            logger.debug(f"í…Œì´ë¸” ì—†ìŒ: {chunk.symbol}_{chunk.timeframe}")
            return CollectionResult(
                chunk=chunk,
                collected_candles=[],
                data_source="db",
                api_requests_made=0,
                collection_time_ms=(time.perf_counter() - start_time) * 1000
            )

        # ë²”ìœ„ ë°ì´í„° ì¡°íšŒ (ìƒˆë¡œìš´ CandleData ëª¨ë¸ ë°˜í™˜)
        chunk_end_time = self._calculate_chunk_end_time(chunk)
        candles = await self.repository.get_candles_by_range(
            chunk.symbol, chunk.timeframe, chunk.start_time, chunk_end_time
        )

        collection_time = (time.perf_counter() - start_time) * 1000

        return CollectionResult(
            chunk=chunk,
            collected_candles=candles,
            data_source="db",
            api_requests_made=0,
            collection_time_ms=collection_time
        )

    except Exception as e:
        logger.error(f"DB ì²­í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return CollectionResult(
            chunk=chunk,
            collected_candles=[],
            data_source="error",
            api_requests_made=0,
            collection_time_ms=(time.perf_counter() - start_time) * 1000
        )

async def _save_collected_candles(self, symbol: str, timeframe: str,
                                candles: List[CandleData]) -> int:
    """ìˆ˜ì§‘ëœ ìº”ë“¤ ì €ì¥ - Repository save_chunk í™œìš©"""
    if not candles:
        return 0

    try:
        # í…Œì´ë¸” ìƒì„± (í•„ìš”ì‹œ)
        await self.repository.ensure_table_exists(symbol, timeframe)

        # ë°°ì¹˜ ì €ì¥ (INSERT OR IGNORE ë°©ì‹)
        saved_count = await self.repository.save_candle_chunk(symbol, timeframe, candles)

        logger.debug(f"ìº”ë“¤ ì €ì¥ ì™„ë£Œ: {saved_count}/{len(candles)}ê°œ (ì¤‘ë³µ ì œì™¸)")
        return saved_count

    except Exception as e:
        logger.error(f"ìº”ë“¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        return 0
```

### 5. CandleRepositoryInterface Compliance

#### Interface Compliance Check
```python
from upbit_auto_trading.domain.repositories.candle_repository_interface import CandleRepositoryInterface

# SqliteCandleRepositoryê°€ ëª¨ë“  abstract methodë¥¼ êµ¬í˜„í–ˆëŠ”ì§€ í™•ì¸
class CandleDataProvider:
    def __init__(self, db_manager: DatabaseManager, ...):
        # íƒ€ì… ì²´í¬ë¡œ interface ì¤€ìˆ˜ í™•ì¸
        self.repository: CandleRepositoryInterface = SqliteCandleRepository(db_manager)

        # ëŸ°íƒ€ì„ì—ì„œ 10ê°œ ë©”ì„œë“œ ì¡´ì¬ í™•ì¸ (ê°œë°œ ë‹¨ê³„ì—ì„œë§Œ)
        required_methods = [
            'get_data_ranges', 'has_any_data_in_range', 'is_range_complete',
            'find_last_continuous_time', 'is_continue_till_end', 'has_data_at_time',
            'find_data_start_in_range', 'ensure_table_exists', 'save_candle_chunk',
            'get_candles_by_range'
        ]

        for method_name in required_methods:
            if not hasattr(self.repository, method_name):
                raise RuntimeError(f"Repositoryì—ì„œ {method_name} ë©”ì„œë“œ ëˆ„ë½")
```

---

## ğŸ›ï¸ Public Interface Specification

### Main Entry Point
```python
async def get_candles(
    self,
    symbol: str,                    # ê±°ë˜ ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
    timeframe: str,                 # íƒ€ì„í”„ë ˆì„ ('1s', '1m', '5m', ..., '1y')
    count: Optional[int] = None,    # ìº”ë“¤ ê°œìˆ˜ (1~10000)
    start_time: Optional[datetime] = None,  # ì‹œì‘ ì‹œê°„ (UTC)
    end_time: Optional[datetime] = None,    # ì¢…ë£Œ ì‹œê°„ (UTC)
    inclusive_start: bool = True    # start_time í¬í•¨ ì—¬ë¶€
) -> CandleDataResponse:
    """
    ìº”ë“¤ ë°ì´í„° ì¡°íšŒ - 5ê°€ì§€ íŒŒë¼ë¯¸í„° ì¡°í•© ì§€ì›

    Parameter Combinations:
    1. countë§Œ: ìµœì‹  ë°ì´í„°ë¶€í„° ì—­ìˆœìœ¼ë¡œ countê°œ
    2. start_time + count: start_timeë¶€í„° countê°œ (inclusive_start ì ìš©)
    3. start_time + end_time: êµ¬ê°„ ì§€ì • (inclusive_start ì ìš©)
    4. count + end_time: end_timeê¹Œì§€ ì—­ìˆœìœ¼ë¡œ countê°œ
    5. ëª¨ë“  íŒŒë¼ë¯¸í„°: start_time~end_time ë²”ìœ„ì—ì„œ ìµœëŒ€ countê°œ

    Performance:
    - 100ê°œ ìº”ë“¤: í‰ê·  50ms ì´í•˜
    - 1000ê°œ ìº”ë“¤: í‰ê·  500ms ì´í•˜
    - ìºì‹œ íˆíŠ¸ì‹œ: í‰ê·  10ms ì´í•˜

    Error Handling:
    - ë¯¸ë˜ ì‹œê°„ ìš”ì²­: ValidationError
    - ì§€ì›í•˜ì§€ ì•ŠëŠ” íƒ€ì„í”„ë ˆì„: ValidationError
    - Rate limit ì´ˆê³¼: ìë™ ë°±ì˜¤í”„ ì¬ì‹œë„
    - ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: ìµœëŒ€ 3íšŒ ì¬ì‹œë„

    Returns:
        CandleDataResponse:
            - success: ì„±ê³µ ì—¬ë¶€
            - candles: CandleData ë¦¬ìŠ¤íŠ¸ (ì‹œê°„ìˆœ ì •ë ¬)
            - total_count: ì‹¤ì œ ë°˜í™˜ëœ ìº”ë“¤ ìˆ˜
            - data_source: "cache"/"db"/"api"/"mixed"
            - response_time_ms: ì‘ë‹µ ì‹œê°„
            - error_message: ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ì‹œ)
    """
```

### Convenience Methods
```python
async def get_latest_candles(
    self,
    symbol: str,
    timeframe: str,
    count: int = 200
) -> CandleDataResponse:
    """ìµœì‹  ìº”ë“¤ ì¡°íšŒ (get_candlesì˜ ê°„í¸ ë²„ì „)"""
    return await self.get_candles(symbol, timeframe, count=count)

def get_stats(self) -> dict:
    """ì„œë¹„ìŠ¤ í†µê³„ ì¡°íšŒ

    Returns:
        {
            'total_requests': int,
            'cache_hits': int,
            'cache_misses': int,
            'api_requests': int,
            'average_response_time_ms': float,
            'supported_timeframes': List[str],
            'active_cache_entries': int
        }
    """

def get_supported_timeframes(self) -> List[str]:
    """ì§€ì›í•˜ëŠ” íƒ€ì„í”„ë ˆì„ ëª©ë¡ ë°˜í™˜"""
    return ['1s', '1m', '3m', '5m', '10m', '15m', '30m', '60m', '240m',
            '1h', '4h', '1d', '1w', '1M', '1y']

def get_cache_stats(self) -> dict:
    """ìºì‹œ í†µê³„ ì¡°íšŒ

    Returns:
        {
            'total_entries': int,
            'memory_usage_mb': float,
            'hit_rate': float,
            'average_ttl_remaining': float
        }
    """

# Factory Functions
def create_candle_data_provider(
    db_manager: Optional[DatabaseManager] = None,
    upbit_client: Optional[UpbitPublicClient] = None
) -> CandleDataProvider:
    """CandleDataProvider íŒ©í† ë¦¬ í•¨ìˆ˜ (ë™ê¸° ë²„ì „)"""

async def create_candle_data_provider_async(
    db_manager: Optional[DatabaseManager] = None,
    upbit_client: Optional[UpbitPublicClient] = None
) -> CandleDataProvider:
    """CandleDataProvider íŒ©í† ë¦¬ í•¨ìˆ˜ (ë¹„ë™ê¸° ë²„ì „, ì´ˆê¸°í™” í¬í•¨)"""
```

---

## âš¡ Performance Optimization Strategy

### 1. Caching Layer
```python
class CandleCache:
    """60ì´ˆ TTL ë©”ëª¨ë¦¬ ìºì‹œ"""
    def __init__(self, max_memory_mb: int = 100):
        self.cache: Dict[str, CacheEntry] = {}
        self.max_memory_bytes = max_memory_mb * 1024 * 1024

    async def get(self, key: CacheKey) -> Optional[List[CandleData]]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""

    async def set(self, key: CacheKey, candles: List[CandleData], ttl: int = 60):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""

    def cleanup_expired(self):
        """ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
```

### 2. Chunk Processing Optimization
```python
async def _collect_chunks_sequentially(
    self, chunks: List[CandleChunk], target_end_time: datetime
) -> Tuple[List[CandleData], List[str]]:
    """ì²­í¬ ìˆœì°¨ ìˆ˜ì§‘ - ì¡°ê¸° ì¤‘ë‹¨ ìµœì í™”"""

    all_candles = []
    data_sources = []

    for chunk in chunks:
        # target_end_time ë„ë‹¬ì‹œ ì¡°ê¸° ì¤‘ë‹¨
        if self._is_collection_complete(chunk, target_end_time):
            logger.debug(f"target_end_time ë„ë‹¬, ìˆ˜ì§‘ ì¤‘ë‹¨: {chunk.chunk_index+1}/{len(chunks)}")
            break

        # ì²­í¬ë³„ ìµœì í™”ëœ ìˆ˜ì§‘
        result = await self._collect_chunk_optimized(chunk)
        all_candles.extend(result.collected_candles)
        data_sources.append(result.data_source)

        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_chunk_stats(result)

    return all_candles, data_sources
```

### 3. Memory Management
```python
def _monitor_memory_usage(self):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ ë° ì •ë¦¬"""
    if self.cache.get_memory_usage_mb() > 80:  # 80MB ì´ˆê³¼ì‹œ
        self.cache.cleanup_expired()
        self.cache.evict_lru()  # LRU ì •ì±…ìœ¼ë¡œ ì¶”ê°€ ì •ë¦¬
```

---

## ğŸ” Error Handling & Recovery

### Error Hierarchy
```python
class CandleDataProviderError(Exception):
    """ê¸°ë³¸ ì—ëŸ¬ í´ë˜ìŠ¤"""

class ValidationError(CandleDataProviderError):
    """íŒŒë¼ë¯¸í„° ê²€ì¦ ì—ëŸ¬"""

class RateLimitError(CandleDataProviderError):
    """API Rate Limit ì—ëŸ¬"""

class NetworkError(CandleDataProviderError):
    """ë„¤íŠ¸ì›Œí¬ ì—°ê²° ì—ëŸ¬"""

class DataIntegrityError(CandleDataProviderError):
    """ë°ì´í„° ë¬´ê²°ì„± ì—ëŸ¬"""
```

### Recovery Strategies
```python
async def _handle_api_error(self, error: Exception, chunk: CandleChunk,
                           retry_count: int = 0) -> CollectionResult:
    """API ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬"""

    if isinstance(error, RateLimitError) and retry_count < 3:
        # ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
        wait_time = 2 ** retry_count
        await asyncio.sleep(wait_time)
        return await self._collect_api_chunk(chunk)

    elif isinstance(error, NetworkError) and retry_count < 3:
        # ì„ í˜• ë°±ì˜¤í”„ ì¬ì‹œë„
        await asyncio.sleep(1 * (retry_count + 1))
        return await self._collect_api_chunk(chunk)

    else:
        # ë³µêµ¬ ë¶ˆê°€ëŠ¥ - DB í´ë°± ì‹œë„
        logger.warning(f"API ë³µêµ¬ ì‹¤íŒ¨, DB í´ë°± ì‹œë„: {error}")
        return await self._collect_db_chunk(chunk)
```

---

## ğŸ“Š Monitoring & Observability

### Metrics Collection
```python
def _update_metrics(self, operation: str, duration_ms: float, success: bool):
    """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    self.stats[f'{operation}_count'] += 1
    self.stats[f'{operation}_total_time_ms'] += duration_ms

    if success:
        self.stats[f'{operation}_success_count'] += 1
    else:
        self.stats[f'{operation}_error_count'] += 1

def get_health_status(self) -> dict:
    """í—¬ìŠ¤ ì²´í¬ ì •ë³´"""
    return {
        'status': 'healthy' if self._is_healthy() else 'unhealthy',
        'cache_status': self.cache.get_health_status(),
        'repository_status': self.repository.get_health_status(),
        'api_client_status': self.upbit_client.get_health_status(),
        'last_successful_request': self.stats.get('last_successful_request'),
        'error_rate_last_hour': self._calculate_error_rate()
    }
```

ì´ì œ ì´ ì•„í‚¤í…ì²˜ ì„¤ê³„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ êµ¬í˜„ì„ ì§„í–‰í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìŠ¹ì¸í•´ì£¼ì‹œë©´ ë‹¤ìŒ ë‹¨ê³„ì¸ API ëª…ì„¸ì„œ ì‘ì„±ìœ¼ë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤.
