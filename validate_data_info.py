#!/usr/bin/env python3
"""
data_info í´ë” ì „ì²´ ê²€í†  ë° ì •ë¦¬ ìŠ¤í¬ë¦½íŠ¸
- SQL ìŠ¤í‚¤ë§ˆ ê²€ì¦
- YAML íŒŒì¼ ê²€ì¦ ë° ì¤‘ë³µ ì •ë¦¬
- ë°±ì—… íŒŒì¼ ì •ë¦¬
"""

import yaml
import sqlite3
import re
from pathlib import Path
from datetime import datetime
import shutil
import json

class DataInfoValidator:
    def __init__(self):
        self.data_info_path = Path("data_info")
        self.legacy_path = Path("legacy")
        self.report = {
            "sql_schemas": {},
            "yaml_files": {},
            "duplicates": [],
            "issues": [],
            "actions": []
        }

    def run_full_validation(self):
        """ì „ì²´ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("=== DATA_INFO í´ë” ì „ì²´ ê²€í†  ì‹œì‘ ===\n")

        # 1. SQL ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ì¦
        self.validate_sql_schemas()

        # 2. YAML íŒŒì¼ ê²€ì¦
        self.validate_yaml_files()

        # 3. ì¤‘ë³µ íŒŒì¼ ì‹ë³„
        self.identify_duplicates()

        # 4. ë°±ì—… íŒŒì¼ ì •ë¦¬
        self.organize_backups()

        # 5. ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report()

        # 6. ì •ë¦¬ ì‘ì—… ì‹¤í–‰
        self.execute_cleanup()

    def validate_sql_schemas(self):
        """SQL ìŠ¤í‚¤ë§ˆ íŒŒì¼ë“¤ ê²€ì¦"""
        print("1. SQL ìŠ¤í‚¤ë§ˆ íŒŒì¼ ê²€ì¦")

        sql_files = list(self.data_info_path.glob("*.sql"))
        expected_schemas = [
            "upbit_autotrading_schema_settings.sql",
            "upbit_autotrading_schema_strategies.sql",
            "upbit_autotrading_schema_market_data.sql"
        ]

        for schema_file in expected_schemas:
            file_path = self.data_info_path / schema_file
            print(f"  ê²€ì¦: {schema_file}")

            if not file_path.exists():
                issue = f"âŒ í•„ìˆ˜ ìŠ¤í‚¤ë§ˆ íŒŒì¼ ëˆ„ë½: {schema_file}"
                print(f"    {issue}")
                self.report["issues"].append(issue)
                continue

            # SQL íŒŒì¼ ë‚´ìš© ê²€ì¦
            content = file_path.read_text(encoding='utf-8')
            schema_info = self.analyze_sql_schema(content, schema_file)
            self.report["sql_schemas"][schema_file] = schema_info

            print(f"    âœ… í…Œì´ë¸” ìˆ˜: {len(schema_info['tables'])}")
            for table in schema_info['tables']:
                print(f"      - {table}")

    def analyze_sql_schema(self, content, filename):
        """SQL ìŠ¤í‚¤ë§ˆ ë‚´ìš© ë¶„ì„"""
        # CREATE TABLE ë¬¸ ì°¾ê¸°
        table_pattern = r'CREATE TABLE\s+(?:IF NOT EXISTS\s+)?([^\s(]+)'
        tables = re.findall(table_pattern, content, re.IGNORECASE)

        # ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ ì°¾ê¸°
        fk_pattern = r'FOREIGN KEY.*?REFERENCES\s+([^\s(]+)'
        foreign_keys = re.findall(fk_pattern, content, re.IGNORECASE)

        # ì¸ë±ìŠ¤ ì°¾ê¸°
        index_pattern = r'CREATE.*?INDEX.*?ON\s+([^\s(]+)'
        indexes = re.findall(index_pattern, content, re.IGNORECASE)

        return {
            "tables": tables,
            "foreign_keys": foreign_keys,
            "indexes": indexes,
            "line_count": len(content.split('\n')),
            "size_bytes": len(content.encode('utf-8'))
        }

    def validate_yaml_files(self):
        """YAML íŒŒì¼ë“¤ ê²€ì¦"""
        print("\n2. YAML íŒŒì¼ ê²€ì¦")

        yaml_files = list(self.data_info_path.glob("*.yaml"))

        for yaml_file in yaml_files:
            print(f"  ê²€ì¦: {yaml_file.name}")

            try:
                with open(yaml_file, 'r', encoding='utf-8') as f:
                    data = yaml.safe_load(f)

                yaml_info = self.analyze_yaml_content(data, yaml_file.name)
                self.report["yaml_files"][yaml_file.name] = yaml_info

                print(f"    âœ… ìœ íš¨í•œ YAML")
                if yaml_info.get("main_sections"):
                    print(f"    ğŸ“Š ì£¼ìš” ì„¹ì…˜: {len(yaml_info['main_sections'])}")
                    for section in yaml_info["main_sections"][:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                        print(f"      - {section}")

            except yaml.YAMLError as e:
                issue = f"âŒ YAML íŒŒì‹± ì˜¤ë¥˜: {yaml_file.name} - {e}"
                print(f"    {issue}")
                self.report["issues"].append(issue)
            except Exception as e:
                issue = f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {yaml_file.name} - {e}"
                print(f"    {issue}")
                self.report["issues"].append(issue)

    def analyze_yaml_content(self, data, filename):
        """YAML ë‚´ìš© ë¶„ì„"""
        info = {
            "valid": True,
            "main_sections": [],
            "record_count": 0,
            "has_backups_header": False
        }

        if isinstance(data, dict):
            info["main_sections"] = list(data.keys())

            # ë ˆì½”ë“œ ìˆ˜ ê³„ì‚°
            for key, value in data.items():
                if isinstance(value, dict):
                    info["record_count"] += len(value)

            # ë°±ì—… í—¤ë” í™•ì¸ (í¸ì§‘ ì„¸ì…˜ ì •ë³´ ë“±)
            content_str = str(data)
            if any(keyword in content_str for keyword in ["í¸ì§‘ ì„¸ì…˜", "ë°±ì—…", "EDIT_", "session_"]):
                info["has_backups_header"] = True

        return info

    def identify_duplicates(self):
        """ì¤‘ë³µ íŒŒì¼ ì‹ë³„"""
        print("\n3. ì¤‘ë³µ íŒŒì¼ ì‹ë³„")

        # íŒŒì¼ëª… íŒ¨í„´ ë¶„ì„
        base_files = {}
        backup_files = []

        for file_path in self.data_info_path.glob("*.yaml"):
            filename = file_path.name

            # ë°±ì—… íŒŒì¼ íŒ¨í„´ ì‹ë³„
            if any(pattern in filename for pattern in ["backup", "BACKUP", "EDIT_", "ORIGINAL_"]):
                backup_files.append(filename)
                continue

            # ë² ì´ìŠ¤ íŒŒì¼ëª… ì¶”ì¶œ
            base_name = filename
            if base_name not in base_files:
                base_files[base_name] = []
            base_files[base_name].append(filename)

        # ì¤‘ë³µ ì°¾ê¸°
        for base_name, files in base_files.items():
            if len(files) > 1:
                self.report["duplicates"].append({
                    "base_name": base_name,
                    "files": files
                })

        print(f"  ğŸ“ ë² ì´ìŠ¤ íŒŒì¼: {len(base_files)}")
        print(f"  ğŸ“„ ë°±ì—… íŒŒì¼: {len(backup_files)}")
        print(f"  ğŸ”„ ì¤‘ë³µ ê·¸ë£¹: {len(self.report['duplicates'])}")

        for backup in backup_files:
            print(f"    ë°±ì—…: {backup}")

    def organize_backups(self):
        """ë°±ì—… íŒŒì¼ ì •ë¦¬"""
        print("\n4. ë°±ì—… íŒŒì¼ ì •ë¦¬ ê³„íš")

        # legacy í´ë” ìƒì„±
        if not self.legacy_path.exists():
            self.legacy_path.mkdir()
            self.report["actions"].append("legacy í´ë” ìƒì„±")

        # ë°±ì—… íŒŒì¼ë“¤ ì´ë™ ê³„íš
        backup_patterns = ["backup", "BACKUP", "EDIT_", "ORIGINAL_"]

        for file_path in self.data_info_path.glob("*.yaml"):
            filename = file_path.name

            if any(pattern in filename for pattern in backup_patterns):
                target_path = self.legacy_path / filename
                self.report["actions"].append(f"ì´ë™: {filename} â†’ legacy/{filename}")
                print(f"  ğŸ“¦ ì´ë™ ì˜ˆì •: {filename}")

        # _BACKUPS_ í´ë”ë„ legacyë¡œ ì´ë™ ê³„íš
        backups_dir = self.data_info_path / "_BACKUPS_"
        if backups_dir.exists():
            target_dir = self.legacy_path / "data_info_BACKUPS"
            self.report["actions"].append(f"ì´ë™: _BACKUPS_ â†’ legacy/data_info_BACKUPS")
            print(f"  ğŸ“¦ ì´ë™ ì˜ˆì •: _BACKUPS_ í´ë”")

    def generate_report(self):
        """ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n5. ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"data_info_validation_report_{timestamp}.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.report, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥: {report_file}")

        # ìš”ì•½ ì¶œë ¥
        print("\n=== ê²€ì¦ ìš”ì•½ ===")
        print(f"SQL ìŠ¤í‚¤ë§ˆ íŒŒì¼: {len(self.report['sql_schemas'])}")
        print(f"YAML íŒŒì¼: {len(self.report['yaml_files'])}")
        print(f"ë°œê²¬ëœ ë¬¸ì œ: {len(self.report['issues'])}")
        print(f"ì •ë¦¬ ì‘ì—… í•­ëª©: {len(self.report['actions'])}")

        if self.report['issues']:
            print("\nâš ï¸  ë°œê²¬ëœ ë¬¸ì œë“¤:")
            for issue in self.report['issues']:
                print(f"  - {issue}")

    def execute_cleanup(self):
        """ì •ë¦¬ ì‘ì—… ì‹¤í–‰"""
        print("\n6. ì •ë¦¬ ì‘ì—… ì‹¤í–‰")

        response = input("ì •ë¦¬ ì‘ì—…ì„ ì‹¤í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ")
        if response.lower() != 'y':
            print("ì •ë¦¬ ì‘ì—…ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        try:
            # ë°±ì—… íŒŒì¼ë“¤ ì´ë™
            backup_patterns = ["backup", "BACKUP", "EDIT_", "ORIGINAL_"]

            for file_path in self.data_info_path.glob("*.yaml"):
                filename = file_path.name

                if any(pattern in filename for pattern in backup_patterns):
                    target_path = self.legacy_path / filename
                    shutil.move(str(file_path), str(target_path))
                    print(f"  âœ… ì´ë™ ì™„ë£Œ: {filename}")

            # _BACKUPS_ í´ë” ì´ë™
            backups_dir = self.data_info_path / "_BACKUPS_"
            if backups_dir.exists():
                target_dir = self.legacy_path / "data_info_BACKUPS"
                shutil.move(str(backups_dir), str(target_dir))
                print(f"  âœ… í´ë” ì´ë™ ì™„ë£Œ: _BACKUPS_")

            # _MERGED_ í´ë”ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì œê±°
            merged_dir = self.data_info_path / "_MERGED_"
            if merged_dir.exists() and not any(merged_dir.iterdir()):
                merged_dir.rmdir()
                print(f"  âœ… ë¹ˆ í´ë” ì œê±°: _MERGED_")

            print("\nâœ… ëª¨ë“  ì •ë¦¬ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

        except Exception as e:
            print(f"âŒ ì •ë¦¬ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    validator = DataInfoValidator()
    validator.run_full_validation()
